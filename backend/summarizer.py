import os
import openai
import logging
from typing import Optional

logger = logging.getLogger(__name__)

class Summarizer:
    """æ–‡æœ¬æ€»ç»“å™¨ï¼Œä½¿ç”¨OpenAI APIç”Ÿæˆå¤šè¯­è¨€æ‘˜è¦"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ€»ç»“å™¨"""
        # ä»ç¯å¢ƒå˜é‡è·å–OpenAI APIé…ç½®
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_BASE_URL")
        
        if not api_key:
            logger.warning("æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†æ— æ³•ä½¿ç”¨æ‘˜è¦åŠŸèƒ½")
        
        if api_key:
            if base_url:
                self.client = openai.OpenAI(api_key=api_key, base_url=base_url)
                logger.info(f"OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨è‡ªå®šä¹‰ç«¯ç‚¹: {base_url}")
            else:
                self.client = openai.OpenAI(api_key=api_key)
                logger.info("OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨é»˜è®¤ç«¯ç‚¹")
        else:
            self.client = None
        
        # æ”¯æŒçš„è¯­è¨€æ˜ å°„
        self.language_map = {
            "en": "English",
            "zh": "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰",
            "es": "EspaÃ±ol",
            "fr": "FranÃ§ais", 
            "de": "Deutsch",
            "it": "Italiano",
            "pt": "PortuguÃªs",
            "ru": "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
            "ja": "æ—¥æœ¬èª",
            "ko": "í•œêµ­ì–´",
            "ar": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
        }
    
    async def optimize_transcript(self, raw_transcript: str) -> str:
        """
        ä¼˜åŒ–è½¬å½•æ–‡æœ¬ï¼šä¿®æ­£é”™åˆ«å­—ï¼ŒæŒ‰å«ä¹‰åˆ†æ®µ
        æ”¯æŒé•¿æ–‡æœ¬è‡ªåŠ¨åˆ†å—å¤„ç†
        
        Args:
            raw_transcript: åŸå§‹è½¬å½•æ–‡æœ¬
            
        Returns:
            ä¼˜åŒ–åçš„è½¬å½•æ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        try:
            if not self.client:
                logger.warning("OpenAI APIä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹è½¬å½•")
                return raw_transcript

            # é¢„å¤„ç†ï¼šä»…ç§»é™¤æ—¶é—´æˆ³ä¸å…ƒä¿¡æ¯ï¼Œä¿ç•™å…¨éƒ¨å£è¯­/é‡å¤å†…å®¹
            preprocessed = self._remove_timestamps_and_meta(raw_transcript)
            # ä½¿ç”¨JSç­–ç•¥ï¼šæŒ‰å­—ç¬¦é•¿åº¦åˆ†å—ï¼ˆæ›´è´´è¿‘tokensä¸Šé™ï¼Œé¿å…ä¼°ç®—è¯¯å·®ï¼‰
            detected_lang_code = self._detect_transcript_language(preprocessed)
            max_chars_per_chunk = 4000  # å¯¹é½JSï¼šæ¯å—æœ€å¤§çº¦4000å­—ç¬¦

            if len(preprocessed) > max_chars_per_chunk:
                logger.info(f"æ–‡æœ¬è¾ƒé•¿({len(preprocessed)} chars)ï¼Œå¯ç”¨åˆ†å—ä¼˜åŒ–")
                return await self._format_long_transcript_in_chunks(preprocessed, detected_lang_code, max_chars_per_chunk)
            else:
                return await self._format_single_chunk(preprocessed, detected_lang_code)

        except Exception as e:
            logger.error(f"ä¼˜åŒ–è½¬å½•æ–‡æœ¬å¤±è´¥: {str(e)}")
            logger.info("è¿”å›åŸå§‹è½¬å½•æ–‡æœ¬")
            return raw_transcript

    def _estimate_tokens(self, text: str) -> int:
        """
        æ”¹è¿›çš„tokenæ•°é‡ä¼°ç®—ç®—æ³•
        æ›´ä¿å®ˆçš„ä¼°ç®—ï¼Œè€ƒè™‘ç³»ç»Ÿpromptå’Œæ ¼å¼åŒ–å¼€é”€
        """
        # æ›´ä¿å®ˆçš„ä¼°ç®—ï¼šè€ƒè™‘å®é™…ä½¿ç”¨ä¸­çš„tokenè†¨èƒ€
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_words = len([word for word in text.split() if word.isascii() and word.isalpha()])
        
        # è®¡ç®—åŸºç¡€tokens
        base_tokens = chinese_chars * 1.5 + english_words * 1.3
        
        # è€ƒè™‘markdownæ ¼å¼ã€æ—¶é—´æˆ³ç­‰å¼€é”€ï¼ˆçº¦30%é¢å¤–å¼€é”€ï¼‰
        format_overhead = len(text) * 0.15
        
        # è€ƒè™‘ç³»ç»Ÿpromptå¼€é”€ï¼ˆçº¦2000-3000 tokensï¼‰
        system_prompt_overhead = 2500
        
        total_estimated = int(base_tokens + format_overhead + system_prompt_overhead)
        
        return total_estimated

    async def _optimize_single_chunk(self, raw_transcript: str) -> str:
        """
        ä¼˜åŒ–å•ä¸ªæ–‡æœ¬å—
        """
        detected_lang = self._detect_transcript_language(raw_transcript)
        lang_instruction = self._get_language_instruction(detected_lang)
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬ç¼–è¾‘ä¸“å®¶ã€‚è¯·å¯¹æä¾›çš„è§†é¢‘è½¬å½•æ–‡æœ¬è¿›è¡Œä¼˜åŒ–å¤„ç†ã€‚

ç‰¹åˆ«æ³¨æ„ï¼šè¿™å¯èƒ½æ˜¯è®¿è°ˆã€å¯¹è¯æˆ–æ¼”è®²ï¼Œå¦‚æœåŒ…å«å¤šä¸ªè¯´è¯è€…ï¼Œå¿…é¡»ä¿æŒæ¯ä¸ªè¯´è¯è€…çš„åŸå§‹è§†è§’ã€‚

è¦æ±‚ï¼š
1. **ä¸¥æ ¼ä¿æŒåŸå§‹è¯­è¨€({lang_instruction})ï¼Œç»å¯¹ä¸è¦ç¿»è¯‘æˆå…¶ä»–è¯­è¨€**
2. **å®Œå…¨ç§»é™¤æ‰€æœ‰æ—¶é—´æˆ³æ ‡è®°ï¼ˆå¦‚ [00:00 - 00:05]ï¼‰**
3. **æ™ºèƒ½è¯†åˆ«å’Œé‡ç»„è¢«æ—¶é—´æˆ³æ‹†åˆ†çš„å®Œæ•´å¥å­**ï¼Œè¯­æ³•ä¸Šä¸å®Œæ•´çš„å¥å­ç‰‡æ®µéœ€è¦ä¸ä¸Šä¸‹æ–‡åˆå¹¶
4. ä¿®æ­£æ˜æ˜¾çš„é”™åˆ«å­—å’Œè¯­æ³•é”™è¯¯
5. å°†é‡ç»„åçš„å®Œæ•´å¥å­æŒ‰ç…§è¯­ä¹‰å’Œé€»è¾‘å«ä¹‰åˆ†æˆè‡ªç„¶çš„æ®µè½
6. æ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”
7. **ä¸¥æ ¼ä¿æŒåŸæ„ä¸å˜ï¼Œä¸è¦æ·»åŠ æˆ–åˆ é™¤å®é™…å†…å®¹**
8. **ç»å¯¹ä¸è¦æ”¹å˜äººç§°ä»£è¯ï¼ˆå¦‚I/æˆ‘ã€you/ä½ ã€he/ä»–ã€she/å¥¹ç­‰ï¼‰**
9. **ä¿æŒæ¯ä¸ªè¯´è¯è€…çš„åŸå§‹è§†è§’å’Œè¯­å¢ƒ**
10. **è¯†åˆ«å¯¹è¯ç»“æ„ï¼šè®¿è°ˆè€…ç”¨"you"ï¼Œè¢«è®¿è€…ç”¨"I/we"ï¼Œç»ä¸æ··æ·†**
11. ç¡®ä¿æ¯ä¸ªå¥å­è¯­æ³•å®Œæ•´ï¼Œè¯­è¨€æµç•…è‡ªç„¶

å¤„ç†ç­–ç•¥ï¼š
- ä¼˜å…ˆè¯†åˆ«ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µï¼ˆå¦‚ä»¥ä»‹è¯ã€è¿è¯ã€å½¢å®¹è¯ç»“å°¾ï¼‰
- æŸ¥çœ‹ç›¸é‚»çš„æ–‡æœ¬ç‰‡æ®µï¼Œåˆå¹¶å½¢æˆå®Œæ•´å¥å­
- é‡æ–°æ–­å¥ï¼Œç¡®ä¿æ¯å¥è¯è¯­æ³•å®Œæ•´
- æŒ‰ä¸»é¢˜å’Œé€»è¾‘é‡æ–°åˆ†æ®µ

åˆ†æ®µè¦æ±‚ï¼š
- æŒ‰ä¸»é¢˜å’Œé€»è¾‘å«ä¹‰åˆ†æ®µï¼Œæ¯æ®µåŒ…å«1-8ä¸ªç›¸å…³å¥å­
- å•æ®µé•¿åº¦ä¸è¶…è¿‡400å­—ç¬¦
- é¿å…è¿‡å¤šçš„çŸ­æ®µè½ï¼Œåˆå¹¶ç›¸å…³å†…å®¹
- å½“ä¸€ä¸ªå®Œæ•´æƒ³æ³•æˆ–è§‚ç‚¹è¡¨è¾¾ååˆ†æ®µ

è¾“å‡ºæ ¼å¼ï¼š
- çº¯æ–‡æœ¬æ®µè½ï¼Œæ— æ—¶é—´æˆ³æˆ–æ ¼å¼æ ‡è®°
- æ¯ä¸ªå¥å­ç»“æ„å®Œæ•´
- æ¯ä¸ªæ®µè½è®¨è®ºä¸€ä¸ªä¸»è¦è¯é¢˜
- æ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”

é‡è¦æé†’ï¼šè¿™æ˜¯{lang_instruction}å†…å®¹ï¼Œè¯·å®Œå…¨ç”¨{lang_instruction}è¿›è¡Œä¼˜åŒ–ï¼Œé‡ç‚¹è§£å†³å¥å­è¢«æ—¶é—´æˆ³æ‹†åˆ†å¯¼è‡´çš„ä¸è¿è´¯é—®é¢˜ï¼åŠ¡å¿…è¿›è¡Œåˆç†çš„åˆ†æ®µï¼Œé¿å…å‡ºç°è¶…é•¿æ®µè½ï¼

**å…³é”®è¦æ±‚ï¼šè¿™å¯èƒ½æ˜¯è®¿è°ˆå¯¹è¯ï¼Œç»å¯¹ä¸è¦æ”¹å˜ä»»ä½•äººç§°ä»£è¯æˆ–è¯´è¯è€…è§†è§’ï¼è®¿è°ˆè€…è¯´"you"ï¼Œè¢«è®¿è€…è¯´"I/we"ï¼Œå¿…é¡»ä¸¥æ ¼ä¿æŒï¼**"""

        user_prompt = f"""è¯·å°†ä»¥ä¸‹{lang_instruction}è§†é¢‘è½¬å½•æ–‡æœ¬ä¼˜åŒ–ä¸ºæµç•…çš„æ®µè½æ–‡æœ¬ï¼š

{raw_transcript}

é‡ç‚¹ä»»åŠ¡ï¼š
1. ç§»é™¤æ‰€æœ‰æ—¶é—´æˆ³æ ‡è®°
2. è¯†åˆ«å¹¶é‡ç»„è¢«æ‹†åˆ†çš„å®Œæ•´å¥å­
3. ç¡®ä¿æ¯ä¸ªå¥å­è¯­æ³•å®Œæ•´ã€æ„æ€è¿è´¯
4. æŒ‰å«ä¹‰é‡æ–°åˆ†æ®µï¼Œæ®µè½é—´ç©ºè¡Œåˆ†éš”
5. ä¿æŒ{lang_instruction}è¯­è¨€ä¸å˜

åˆ†æ®µæŒ‡å¯¼ï¼š
- æŒ‰ä¸»é¢˜å’Œé€»è¾‘å«ä¹‰åˆ†æ®µï¼Œæ¯æ®µåŒ…å«1-8ä¸ªç›¸å…³å¥å­
- å•æ®µé•¿åº¦ä¸è¶…è¿‡400å­—ç¬¦
- é¿å…è¿‡å¤šçš„çŸ­æ®µè½ï¼Œåˆå¹¶ç›¸å…³å†…å®¹
- ç¡®ä¿æ®µè½ä¹‹é—´æœ‰æ˜ç¡®çš„ç©ºè¡Œ

è¯·ç‰¹åˆ«æ³¨æ„ä¿®å¤å› æ—¶é—´æˆ³åˆ†å‰²å¯¼è‡´çš„å¥å­ä¸å®Œæ•´é—®é¢˜ï¼Œå¹¶è¿›è¡Œåˆç†çš„æ®µè½åˆ’åˆ†ï¼"""

        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=4000,  # å¯¹é½JSï¼šä¼˜åŒ–/æ ¼å¼åŒ–é˜¶æ®µæœ€å¤§tokensâ‰ˆ4000
            temperature=0.1
        )
        
        return response.choices[0].message.content

    async def _optimize_with_chunks(self, raw_transcript: str, max_tokens: int) -> str:
        """
        åˆ†å—ä¼˜åŒ–é•¿æ–‡æœ¬
        """
        detected_lang = self._detect_transcript_language(raw_transcript)
        lang_instruction = self._get_language_instruction(detected_lang)
        
        # æŒ‰æ®µè½åˆ†å‰²åŸå§‹è½¬å½•ï¼ˆä¿ç•™æ—¶é—´æˆ³ä½œä¸ºåˆ†å‰²å‚è€ƒï¼‰
        chunks = self._split_into_chunks(raw_transcript, max_tokens)
        logger.info(f"åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œå¤„ç†")
        
        optimized_chunks = []
        
        for i, chunk in enumerate(chunks):
            logger.info(f"æ­£åœ¨ä¼˜åŒ–ç¬¬ {i+1}/{len(chunks)} å—...")
            
            system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„æ–‡æœ¬ç¼–è¾‘ä¸“å®¶ã€‚è¯·å¯¹è¿™æ®µè½¬å½•æ–‡æœ¬ç‰‡æ®µè¿›è¡Œç®€å•ä¼˜åŒ–ã€‚

è¿™æ˜¯å®Œæ•´è½¬å½•çš„ç¬¬{i+1}éƒ¨åˆ†ï¼Œå…±{len(chunks)}éƒ¨åˆ†ã€‚

ç®€å•ä¼˜åŒ–è¦æ±‚ï¼š
1. **ä¸¥æ ¼ä¿æŒåŸå§‹è¯­è¨€({lang_instruction})**ï¼Œç»å¯¹ä¸ç¿»è¯‘
2. **ä»…ä¿®æ­£æ˜æ˜¾çš„é”™åˆ«å­—å’Œè¯­æ³•é”™è¯¯**
3. **ç¨å¾®è°ƒæ•´å¥å­æµç•…åº¦**ï¼Œä½†ä¸å¤§å¹…æ”¹å†™
4. **ä¿æŒåŸæ–‡ç»“æ„å’Œé•¿åº¦**ï¼Œä¸åšå¤æ‚çš„æ®µè½é‡ç»„
5. **ä¿æŒåŸæ„100%ä¸å˜**

æ³¨æ„ï¼šè¿™åªæ˜¯åˆæ­¥æ¸…ç†ï¼Œä¸è¦åšå¤æ‚çš„é‡å†™æˆ–é‡æ–°ç»„ç»‡ã€‚"""

            user_prompt = f"""ç®€å•ä¼˜åŒ–ä»¥ä¸‹{lang_instruction}æ–‡æœ¬ç‰‡æ®µï¼ˆä»…ä¿®é”™åˆ«å­—å’Œè¯­æ³•ï¼‰ï¼š

{chunk}

è¾“å‡ºæ¸…ç†åçš„æ–‡æœ¬ï¼Œä¿æŒåŸæ–‡ç»“æ„ã€‚"""

            try:
                response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=1200,  # é€‚åº”4000 tokensæ€»é™åˆ¶
                    temperature=0.1
                )
                
                optimized_chunk = response.choices[0].message.content
                optimized_chunks.append(optimized_chunk)
                
            except Exception as e:
                logger.error(f"ä¼˜åŒ–ç¬¬ {i+1} å—å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ä½¿ç”¨åŸºæœ¬æ¸…ç†
                cleaned_chunk = self._basic_transcript_cleanup(chunk)
                optimized_chunks.append(cleaned_chunk)
        
        # åˆå¹¶æ‰€æœ‰ä¼˜åŒ–åçš„å—
        merged_text = "\n\n".join(optimized_chunks)
        
        # å¯¹åˆå¹¶åçš„æ–‡æœ¬è¿›è¡ŒäºŒæ¬¡æ®µè½æ•´ç†
        logger.info("æ­£åœ¨è¿›è¡Œæœ€ç»ˆæ®µè½æ•´ç†...")
        final_result = await self._final_paragraph_organization(merged_text, lang_instruction)
        
        logger.info("åˆ†å—ä¼˜åŒ–å®Œæˆ")
        return final_result

    # ===== JS openaiService.js ç§»æ¤ï¼šåˆ†å—/ä¸Šä¸‹æ–‡/å»é‡/æ ¼å¼åŒ– =====

    def _ensure_markdown_paragraphs(self, text: str) -> str:
        """ç¡®ä¿Markdownæ®µè½ç©ºè¡Œã€æ ‡é¢˜åç©ºè¡Œã€å‹ç¼©å¤šä½™ç©ºè¡Œã€‚"""
        if not text:
            return text
        formatted = text.replace("\r\n", "\n")
        import re
        # æ ‡é¢˜ååŠ ç©ºè¡Œ
        formatted = re.sub(r"(^#{1,6}\s+.*)\n([^\n#])", r"\1\n\n\2", formatted, flags=re.M)
        # å‹ç¼©â‰¥3ä¸ªæ¢è¡Œä¸º2ä¸ª
        formatted = re.sub(r"\n{3,}", "\n\n", formatted)
        # å»é¦–å°¾ç©ºè¡Œ
        formatted = re.sub(r"^\n+", "", formatted)
        formatted = re.sub(r"\n+$", "", formatted)
        return formatted

    async def _format_single_chunk(self, chunk_text: str, transcript_language: str = 'zh') -> str:
        """å•å—ä¼˜åŒ–ï¼ˆä¿®æ­£+æ ¼å¼åŒ–ï¼‰ï¼Œéµå¾ª4000 tokens é™åˆ¶ã€‚"""
        # æ„å»ºä¸JSç‰ˆä¸€è‡´çš„ç³»ç»Ÿ/ç”¨æˆ·æç¤º
        if transcript_language == 'zh':
            prompt = (
                "è¯·å¯¹ä»¥ä¸‹éŸ³é¢‘è½¬å½•æ–‡æœ¬è¿›è¡Œæ™ºèƒ½ä¼˜åŒ–å’Œæ ¼å¼åŒ–ï¼Œè¦æ±‚ï¼š\n\n"
                "**å†…å®¹ä¼˜åŒ–ï¼ˆæ­£ç¡®æ€§ä¼˜å…ˆï¼‰ï¼š**\n"
                "1. é”™è¯¯ä¿®æ­£ï¼ˆè½¬å½•é”™è¯¯/é”™åˆ«å­—/åŒéŸ³å­—/ä¸“æœ‰åè¯ï¼‰\n"
                "2. é€‚åº¦æ”¹å–„è¯­æ³•ï¼Œè¡¥å…¨ä¸å®Œæ•´å¥å­ï¼Œä¿æŒåŸæ„å’Œè¯­è¨€ä¸å˜\n"
                "3. å£è¯­å¤„ç†ï¼šä¿ç•™è‡ªç„¶å£è¯­ä¸é‡å¤è¡¨è¾¾ï¼Œä¸è¦åˆ å‡å†…å®¹ï¼Œä»…æ·»åŠ å¿…è¦æ ‡ç‚¹\n"
                "4. **ç»å¯¹ä¸è¦æ”¹å˜äººç§°ä»£è¯ï¼ˆI/æˆ‘ã€you/ä½ ç­‰ï¼‰å’Œè¯´è¯è€…è§†è§’**\n\n"
                "**åˆ†æ®µè§„åˆ™ï¼š**\n"
                "- æŒ‰ä¸»é¢˜å’Œé€»è¾‘å«ä¹‰åˆ†æ®µï¼Œæ¯æ®µåŒ…å«1-8ä¸ªç›¸å…³å¥å­\n"
                "- å•æ®µé•¿åº¦ä¸è¶…è¿‡400å­—ç¬¦\n"
                "- é¿å…è¿‡å¤šçš„çŸ­æ®µè½ï¼Œåˆå¹¶ç›¸å…³å†…å®¹\n\n"
                "**æ ¼å¼è¦æ±‚ï¼š**Markdown æ®µè½ï¼Œæ®µè½é—´ç©ºè¡Œ\n\n"
                f"åŸå§‹è½¬å½•æ–‡æœ¬ï¼š\n{chunk_text}"
            )
            system_prompt = (
                "ä½ æ˜¯ä¸“ä¸šçš„éŸ³é¢‘è½¬å½•æ–‡æœ¬ä¼˜åŒ–åŠ©æ‰‹ï¼Œä¿®æ­£é”™è¯¯ã€æ”¹å–„é€šé¡ºåº¦å’Œæ’ç‰ˆæ ¼å¼ï¼Œ"
                "å¿…é¡»ä¿æŒåŸæ„ï¼Œä¸å¾—åˆ å‡å£è¯­/é‡å¤/ç»†èŠ‚ï¼›ä»…ç§»é™¤æ—¶é—´æˆ³æˆ–å…ƒä¿¡æ¯ã€‚"
                "ç»å¯¹ä¸è¦æ”¹å˜äººç§°ä»£è¯æˆ–è¯´è¯è€…è§†è§’ã€‚è¿™å¯èƒ½æ˜¯è®¿è°ˆå¯¹è¯ï¼Œè®¿è°ˆè€…ç”¨'you'ï¼Œè¢«è®¿è€…ç”¨'I/we'ã€‚"
            )
        else:
            prompt = (
                "Please intelligently optimize and format the following audio transcript text:\n\n"
                "Content Optimization (Accuracy First):\n"
                "1. Error Correction (typos, homophones, proper nouns)\n"
                "2. Moderate grammar improvement, complete incomplete sentences, keep original language/meaning\n"
                "3. Speech processing: keep natural fillers and repetitions, do NOT remove content; only add punctuation if needed\n"
                "4. **NEVER change pronouns (I, you, he, she, etc.) or speaker perspective**\n\n"
                "Segmentation Rules: Group 1-8 related sentences per paragraph by topic/logic; paragraph length NOT exceed 400 characters; avoid too many short paragraphs\n\n"
                "Format: Markdown paragraphs with blank lines between paragraphs\n\n"
                f"Original transcript text:\n{chunk_text}"
            )
            system_prompt = (
                "You are a professional transcript formatting assistant. Fix errors and improve fluency "
                "without changing meaning or removing any content; only timestamps/meta may be removed; keep Markdown paragraphs with blank lines. "
                "NEVER change pronouns or speaker perspective. This may be an interview: interviewer uses 'you', interviewee uses 'I/we'."
            )

        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=4000,  # å¯¹é½JSï¼šä¼˜åŒ–/æ ¼å¼åŒ–é˜¶æ®µæœ€å¤§tokensâ‰ˆ4000
                temperature=0.1
            )
            optimized_text = response.choices[0].message.content or ""
            # ç§»é™¤è¯¸å¦‚ "# Transcript" / "## Transcript" ç­‰æ ‡é¢˜
            optimized_text = self._remove_transcript_heading(optimized_text)
            enforced = self._enforce_paragraph_max_chars(optimized_text.strip(), max_chars=400)
            return self._ensure_markdown_paragraphs(enforced)
        except Exception as e:
            logger.error(f"å•å—æ–‡æœ¬ä¼˜åŒ–å¤±è´¥: {e}")
            return self._apply_basic_formatting(chunk_text)

    def _smart_split_long_chunk(self, text: str, max_chars_per_chunk: int) -> list:
        """åœ¨å¥å­/ç©ºæ ¼è¾¹ç•Œå¤„å®‰å…¨åˆ‡åˆ†è¶…é•¿æ–‡æœ¬ã€‚"""
        chunks = []
        pos = 0
        while pos < len(text):
            end = min(pos + max_chars_per_chunk, len(text))
            if end < len(text):
                # ä¼˜å…ˆå¥å­è¾¹ç•Œ
                sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
                best = -1
                for ch in sentence_endings:
                    idx = text.rfind(ch, pos, end)
                    if idx > best:
                        best = idx
                if best > pos + int(max_chars_per_chunk * 0.7):
                    end = best + 1
                else:
                    # æ¬¡é€‰ï¼šç©ºæ ¼è¾¹ç•Œ
                    space_idx = text.rfind(' ', pos, end)
                    if space_idx > pos + int(max_chars_per_chunk * 0.8):
                        end = space_idx
            chunks.append(text[pos:end].strip())
            pos = end
        return [c for c in chunks if c]

    def _find_safe_cut_point(self, text: str) -> int:
        """æ‰¾åˆ°å®‰å…¨çš„åˆ‡å‰²ç‚¹ï¼ˆæ®µè½>å¥å­>çŸ­è¯­ï¼‰ã€‚"""
        import re
        # æ®µè½
        p = text.rfind("\n\n")
        if p > 0:
            return p + 2
        # å¥å­
        last_sentence_end = -1
        for m in re.finditer(r"[ã€‚ï¼ï¼Ÿ\.!?]\s*", text):
            last_sentence_end = m.end()
        if last_sentence_end > 20:
            return last_sentence_end
        # çŸ­è¯­
        last_phrase_end = -1
        for m in re.finditer(r"[ï¼Œï¼›,;]\s*", text):
            last_phrase_end = m.end()
        if last_phrase_end > 20:
            return last_phrase_end
        return len(text)

    def _find_overlap_between_texts(self, text1: str, text2: str) -> str:
        """æ£€æµ‹ç›¸é‚»ä¸¤æ®µçš„é‡å å†…å®¹ï¼Œç”¨äºå»é‡ã€‚"""
        max_len = min(len(text1), len(text2))
        # é€æ­¥ä»é•¿åˆ°çŸ­å°è¯•
        for length in range(max_len, 19, -1):
            suffix = text1[-length:]
            prefix = text2[:length]
            if suffix == prefix:
                cut = self._find_safe_cut_point(prefix)
                if cut > 20:
                    return prefix[:cut]
                return suffix
        return ""

    def _apply_basic_formatting(self, text: str) -> str:
        """å½“AIå¤±è´¥æ—¶çš„å›é€€ï¼šæŒ‰å¥å­æ‹¼æ®µï¼Œæ®µè½â‰¤250å­—ç¬¦ï¼ŒåŒæ¢è¡Œåˆ†éš”ã€‚"""
        if not text or not text.strip():
            return text
        import re
        parts = re.split(r"([ã€‚ï¼ï¼Ÿ\.!?]+\s*)", text)
        sentences = []
        current = ""
        for i, part in enumerate(parts):
            if i % 2 == 0:
                current += part
            else:
                current += part
                if current.strip():
                    sentences.append(current.strip())
                    current = ""
        if current.strip():
            sentences.append(current.strip())
        paras = []
        cur = ""
        sentence_count = 0
        for s in sentences:
            candidate = (cur + " " + s).strip() if cur else s
            sentence_count += 1
            # æ”¹è¿›çš„åˆ†æ®µé€»è¾‘ï¼šè€ƒè™‘å¥å­æ•°é‡å’Œé•¿åº¦
            should_break = False
            if len(candidate) > 400 and cur:  # æ®µè½è¿‡é•¿
                should_break = True
            elif len(candidate) > 200 and sentence_count >= 3:  # ä¸­ç­‰é•¿åº¦ä¸”å¥å­æ•°è¶³å¤Ÿ
                should_break = True
            elif sentence_count >= 6:  # å¥å­æ•°è¿‡å¤š
                should_break = True
            
            if should_break:
                paras.append(cur.strip())
                cur = s
                sentence_count = 1
            else:
                cur = candidate
        if cur.strip():
            paras.append(cur.strip())
        return self._ensure_markdown_paragraphs("\n\n".join(paras))

    async def _format_long_transcript_in_chunks(self, raw_transcript: str, transcript_language: str, max_chars_per_chunk: int) -> str:
        """æ™ºèƒ½åˆ†å—+ä¸Šä¸‹æ–‡+å»é‡ åˆæˆä¼˜åŒ–æ–‡æœ¬ï¼ˆJSç­–ç•¥ç§»æ¤ï¼‰ã€‚"""
        import re
        # å…ˆæŒ‰å¥å­åˆ‡åˆ†ï¼Œç»„è£…ä¸è¶…è¿‡max_chars_per_chunkçš„å—
        parts = re.split(r"([ã€‚ï¼ï¼Ÿ\.!?]+\s*)", raw_transcript)
        sentences = []
        buf = ""
        for i, part in enumerate(parts):
            if i % 2 == 0:
                buf += part
            else:
                buf += part
                if buf.strip():
                    sentences.append(buf.strip())
                    buf = ""
        if buf.strip():
            sentences.append(buf.strip())

        chunks = []
        cur = ""
        for s in sentences:
            candidate = (cur + " " + s).strip() if cur else s
            if len(candidate) > max_chars_per_chunk and cur:
                chunks.append(cur.strip())
                cur = s
            else:
                cur = candidate
        if cur.strip():
            chunks.append(cur.strip())

        # å¯¹ä»ç„¶è¿‡é•¿çš„å—äºŒæ¬¡å®‰å…¨åˆ‡åˆ†
        final_chunks = []
        for c in chunks:
            if len(c) <= max_chars_per_chunk:
                final_chunks.append(c)
            else:
                final_chunks.extend(self._smart_split_long_chunk(c, max_chars_per_chunk))

        logger.info(f"æ–‡æœ¬åˆ†ä¸º {len(final_chunks)} å—å¤„ç†")

        optimized = []
        for i, c in enumerate(final_chunks):
            chunk_with_context = c
            if i > 0:
                prev_tail = final_chunks[i - 1][-100:]
                marker = f"[ä¸Šæ–‡ç»­ï¼š{prev_tail}]" if transcript_language == 'zh' else f"[Context continued: {prev_tail}]"
                chunk_with_context = marker + "\n\n" + c
            try:
                oc = await self._format_single_chunk(chunk_with_context, transcript_language)
                # ç§»é™¤ä¸Šä¸‹æ–‡æ ‡è®°
                oc = re.sub(r"^\[(ä¸Šæ–‡ç»­|Context continued)ï¼š?:?.*?\]\s*", "", oc, flags=re.S)
                optimized.append(oc)
            except Exception as e:
                logger.warning(f"ç¬¬ {i+1} å—ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ ¼å¼åŒ–: {e}")
                optimized.append(self._apply_basic_formatting(c))

        # é‚»æ¥å—å»é‡
        deduped = []
        for i, c in enumerate(optimized):
            cur_txt = c
            if i > 0 and deduped:
                prev = deduped[-1]
                overlap = self._find_overlap_between_texts(prev[-200:], cur_txt[:200])
                if overlap:
                    cur_txt = cur_txt[len(overlap):].lstrip()
                    if not cur_txt:
                        continue
            if cur_txt.strip():
                deduped.append(cur_txt)

        merged = "\n\n".join(deduped)
        merged = self._remove_transcript_heading(merged)
        enforced = self._enforce_paragraph_max_chars(merged, max_chars=400)
        return self._ensure_markdown_paragraphs(enforced)

    def _remove_timestamps_and_meta(self, text: str) -> str:
        """ä»…ç§»é™¤æ—¶é—´æˆ³è¡Œä¸æ˜æ˜¾å…ƒä¿¡æ¯ï¼ˆæ ‡é¢˜ã€æ£€æµ‹è¯­è¨€ç­‰ï¼‰ï¼Œä¿ç•™åŸæ–‡å£è¯­/é‡å¤ã€‚"""
        lines = text.split('\n')
        kept = []
        for line in lines:
            s = line.strip()
            # è·³è¿‡æ—¶é—´æˆ³ä¸å…ƒä¿¡æ¯
            if (s.startswith('**[') and s.endswith(']**')):
                continue
            if s.startswith('# '):
                # è·³è¿‡é¡¶çº§æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯è§†é¢‘æ ‡é¢˜ï¼Œå¯åœ¨æœ€ç»ˆåŠ å›ï¼‰
                continue
            if s.startswith('**æ£€æµ‹è¯­è¨€:**') or s.startswith('**è¯­è¨€æ¦‚ç‡:**'):
                continue
            kept.append(line)
        # è§„èŒƒç©ºè¡Œ
        cleaned = '\n'.join(kept)
        return cleaned

    def _enforce_paragraph_max_chars(self, text: str, max_chars: int = 400) -> str:
        """æŒ‰æ®µè½æ‹†åˆ†å¹¶ç¡®ä¿æ¯æ®µä¸è¶…è¿‡max_charsï¼Œå¿…è¦æ—¶æŒ‰å¥å­è¾¹ç•Œæ‹†ä¸ºå¤šæ®µã€‚"""
        if not text:
            return text
        import re
        paragraphs = [p for p in re.split(r"\n\s*\n", text) if p is not None]
        new_paragraphs = []
        for para in paragraphs:
            para = para.strip()
            if len(para) <= max_chars:
                new_paragraphs.append(para)
                continue
            # å¥å­åˆ‡åˆ†
            parts = re.split(r"([ã€‚ï¼ï¼Ÿ\.!?]+\s*)", para)
            sentences = []
            buf = ""
            for i, part in enumerate(parts):
                if i % 2 == 0:
                    buf += part
                else:
                    buf += part
                    if buf.strip():
                        sentences.append(buf.strip())
                        buf = ""
            if buf.strip():
                sentences.append(buf.strip())
            cur = ""
            for s in sentences:
                candidate = (cur + (" " if cur else "") + s).strip()
                if len(candidate) > max_chars and cur:
                    new_paragraphs.append(cur)
                    cur = s
                else:
                    cur = candidate
            if cur:
                new_paragraphs.append(cur)
        return "\n\n".join([p.strip() for p in new_paragraphs if p is not None])

    def _remove_transcript_heading(self, text: str) -> str:
        """ç§»é™¤å¼€å¤´æˆ–æ®µè½ä¸­çš„ä»¥ Transcript ä¸ºæ ‡é¢˜çš„è¡Œï¼ˆä»»æ„çº§åˆ«#ï¼‰ï¼Œä¸æ”¹å˜æ­£æ–‡ã€‚"""
        if not text:
            return text
        import re
        # ç§»é™¤å½¢å¦‚ '## Transcript'ã€'# Transcript Text'ã€'### transcript' çš„æ ‡é¢˜è¡Œ
        lines = text.split('\n')
        filtered = []
        for line in lines:
            stripped = line.strip()
            if re.match(r"^#{1,6}\s*transcript(\s+text)?\s*$", stripped, flags=re.I):
                continue
            filtered.append(line)
        return '\n'.join(filtered)

    def _split_into_chunks(self, text: str, max_tokens: int) -> list:
        """
        å°†åŸå§‹è½¬å½•æ–‡æœ¬æ™ºèƒ½åˆ†å‰²æˆåˆé€‚å¤§å°çš„å—
        ç­–ç•¥ï¼šå…ˆæå–çº¯æ–‡æœ¬ï¼ŒæŒ‰å¥å­å’Œæ®µè½è‡ªç„¶åˆ†å‰²
        """
        import re
        
        # 1. å…ˆæå–çº¯æ–‡æœ¬å†…å®¹ï¼ˆç§»é™¤æ—¶é—´æˆ³ã€æ ‡é¢˜ç­‰ï¼‰
        pure_text = self._extract_pure_text(text)
        
        # 2. æŒ‰å¥å­åˆ†å‰²ï¼Œä¿æŒå¥å­å®Œæ•´æ€§
        sentences = self._split_into_sentences(pure_text)
        
        # 3. æŒ‰tokené™åˆ¶ç»„è£…æˆå—
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = self._estimate_tokens(sentence)
            
            # æ£€æŸ¥æ˜¯å¦èƒ½åŠ å…¥å½“å‰å—
            if current_tokens + sentence_tokens > max_tokens and current_chunk:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                chunks.append(self._join_sentences(current_chunk))
                current_chunk = [sentence]
                current_tokens = sentence_tokens
            else:
                # æ·»åŠ åˆ°å½“å‰å—
                current_chunk.append(sentence)
                current_tokens += sentence_tokens
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(self._join_sentences(current_chunk))
        
        return chunks
    
    def _extract_pure_text(self, raw_transcript: str) -> str:
        """
        ä»åŸå§‹è½¬å½•ä¸­æå–çº¯æ–‡æœ¬ï¼Œç§»é™¤æ—¶é—´æˆ³å’Œå…ƒæ•°æ®
        """
        lines = raw_transcript.split('\n')
        text_lines = []
        
        for line in lines:
            line = line.strip()
            # è·³è¿‡æ—¶é—´æˆ³ã€æ ‡é¢˜ã€å…ƒæ•°æ®
            if (line.startswith('**[') and line.endswith(']**') or
                line.startswith('#') or
                line.startswith('**æ£€æµ‹è¯­è¨€:**') or
                line.startswith('**è¯­è¨€æ¦‚ç‡:**') or
                not line):
                continue
            text_lines.append(line)
        
        return ' '.join(text_lines)
    
    def _split_into_sentences(self, text: str) -> list:
        """
        æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬ï¼Œè€ƒè™‘ä¸­è‹±æ–‡å·®å¼‚
        """
        import re
        
        # ä¸­è‹±æ–‡å¥å­ç»“æŸç¬¦
        sentence_endings = r'[.!?ã€‚ï¼ï¼Ÿ;ï¼›]+'
        
        # åˆ†å‰²å¥å­ï¼Œä¿ç•™å¥å·
        parts = re.split(f'({sentence_endings})', text)
        
        sentences = []
        current = ""
        
        for i, part in enumerate(parts):
            if re.match(sentence_endings, part):
                # è¿™æ˜¯å¥å­ç»“æŸç¬¦ï¼ŒåŠ åˆ°å½“å‰å¥å­
                current += part
                if current.strip():
                    sentences.append(current.strip())
                current = ""
            else:
                # è¿™æ˜¯å¥å­å†…å®¹
                current += part
        
        # å¤„ç†æœ€åæ²¡æœ‰å¥å·çš„éƒ¨åˆ†
        if current.strip():
            sentences.append(current.strip())
        
        return [s for s in sentences if s.strip()]
    

    
    def _join_sentences(self, sentences: list) -> str:
        """
        é‡æ–°ç»„åˆå¥å­ä¸ºæ®µè½
        """
        return ' '.join(sentences)

    def _basic_transcript_cleanup(self, raw_transcript: str) -> str:
        """
        åŸºæœ¬çš„è½¬å½•æ–‡æœ¬æ¸…ç†ï¼šç§»é™¤æ—¶é—´æˆ³å’Œæ ‡é¢˜ä¿¡æ¯
        å½“GPTä¼˜åŒ–å¤±è´¥æ—¶çš„åå¤‡æ–¹æ¡ˆ
        """
        lines = raw_transcript.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # è·³è¿‡æ—¶é—´æˆ³è¡Œ
            if line.strip().startswith('**[') and line.strip().endswith(']**'):
                continue
            # è·³è¿‡æ ‡é¢˜è¡Œ
            if line.strip().startswith('# ') or line.strip().startswith('## '):
                continue
            # è·³è¿‡æ£€æµ‹è¯­è¨€ç­‰å…ƒä¿¡æ¯è¡Œ
            if line.strip().startswith('**æ£€æµ‹è¯­è¨€:**') or line.strip().startswith('**è¯­è¨€æ¦‚ç‡:**'):
                continue
            # ä¿ç•™éç©ºæ–‡æœ¬è¡Œ
            if line.strip():
                cleaned_lines.append(line.strip())
        
        # å°†å¥å­é‡æ–°ç»„åˆå¹¶æ™ºèƒ½åˆ†æ®µ
        text = ' '.join(cleaned_lines)
        
        # æ›´æ™ºèƒ½çš„åˆ†å¥å¤„ç†ï¼Œè€ƒè™‘ä¸­è‹±æ–‡å·®å¼‚
        import re
        
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å¥
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        paragraphs = []
        current_paragraph = []
        
        for i, sentence in enumerate(sentences):
            if sentence:
                current_paragraph.append(sentence)
                
                # æ™ºèƒ½åˆ†æ®µæ¡ä»¶ï¼š
                # 1. æ¯3ä¸ªå¥å­ä¸€æ®µï¼ˆåŸºæœ¬è§„åˆ™ï¼‰
                # 2. é‡åˆ°è¯é¢˜è½¬æ¢è¯æ±‡æ—¶å¼ºåˆ¶åˆ†æ®µ
                # 3. é¿å…è¶…é•¿æ®µè½
                topic_change_keywords = [
                    'é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å', 'æ¥ä¸‹æ¥', 'å¦å¤–', 'æ­¤å¤–', 'æœ€å', 'æ€»ä¹‹',
                    'first', 'second', 'third', 'next', 'also', 'however', 'finally',
                    'ç°åœ¨', 'é‚£ä¹ˆ', 'æ‰€ä»¥', 'å› æ­¤', 'ä½†æ˜¯', 'ç„¶è€Œ',
                    'now', 'so', 'therefore', 'but', 'however'
                ]
                
                should_break = False
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ®µ
                if len(current_paragraph) >= 3:  # åŸºæœ¬é•¿åº¦æ¡ä»¶
                    should_break = True
                elif len(current_paragraph) >= 2:  # è¾ƒçŸ­ä½†é‡åˆ°è¯é¢˜è½¬æ¢
                    for keyword in topic_change_keywords:
                        if sentence.lower().startswith(keyword.lower()):
                            should_break = True
                            break
                
                if should_break or len(current_paragraph) >= 4:  # æœ€å¤§é•¿åº¦é™åˆ¶
                    # ç»„åˆå½“å‰æ®µè½
                    paragraph_text = '. '.join(current_paragraph)
                    if not paragraph_text.endswith('.'):
                        paragraph_text += '.'
                    paragraphs.append(paragraph_text)
                    current_paragraph = []
        
        # æ·»åŠ å‰©ä½™çš„å¥å­
        if current_paragraph:
            paragraph_text = '. '.join(current_paragraph)
            if not paragraph_text.endswith('.'):
                paragraph_text += '.'
            paragraphs.append(paragraph_text)
        
        return '\n\n'.join(paragraphs)

    async def _final_paragraph_organization(self, text: str, lang_instruction: str) -> str:
        """
        å¯¹åˆå¹¶åçš„æ–‡æœ¬è¿›è¡Œæœ€ç»ˆçš„æ®µè½æ•´ç†
        ä½¿ç”¨æ”¹è¿›çš„promptå’Œå·¥ç¨‹éªŒè¯
        """
        try:
            # ä¼°ç®—æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœå¤ªé•¿åˆ™åˆ†å—å¤„ç†
            estimated_tokens = self._estimate_tokens(text)
            if estimated_tokens > 3000:  # å¯¹äºå¾ˆé•¿çš„æ–‡æœ¬ï¼Œåˆ†å—å¤„ç†
                return await self._organize_long_text_paragraphs(text, lang_instruction)
            
            system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„{lang_instruction}æ–‡æœ¬æ®µè½æ•´ç†ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æŒ‰ç…§è¯­ä¹‰å’Œé€»è¾‘é‡æ–°ç»„ç»‡æ®µè½ã€‚

ğŸ¯ **æ ¸å¿ƒåŸåˆ™**ï¼š
1. **ä¸¥æ ¼ä¿æŒåŸå§‹è¯­è¨€({lang_instruction})**ï¼Œç»ä¸ç¿»è¯‘
2. **ä¿æŒæ‰€æœ‰å†…å®¹å®Œæ•´**ï¼Œä¸åˆ é™¤ä¸æ·»åŠ ä»»ä½•ä¿¡æ¯
3. **æŒ‰è¯­ä¹‰é€»è¾‘åˆ†æ®µ**ï¼šæ¯æ®µå›´ç»•ä¸€ä¸ªå®Œæ•´çš„æ€æƒ³æˆ–è¯é¢˜
4. **ä¸¥æ ¼æ§åˆ¶æ®µè½é•¿åº¦**ï¼šæ¯æ®µç»ä¸è¶…è¿‡250è¯
5. **ä¿æŒè‡ªç„¶æµç•…**ï¼šæ®µè½é—´åº”æœ‰é€»è¾‘è¿æ¥

ğŸ“ **åˆ†æ®µæ ‡å‡†**ï¼š
- **è¯­ä¹‰å®Œæ•´æ€§**ï¼šæ¯æ®µè®²è¿°ä¸€ä¸ªå®Œæ•´æ¦‚å¿µæˆ–äº‹ä»¶
- **é€‚ä¸­é•¿åº¦**ï¼š3-7ä¸ªå¥å­ï¼Œæ¯æ®µç»ä¸è¶…è¿‡250è¯
- **é€»è¾‘è¾¹ç•Œ**ï¼šåœ¨è¯é¢˜è½¬æ¢ã€æ—¶é—´è½¬æ¢ã€è§‚ç‚¹è½¬æ¢å¤„åˆ†æ®µ
- **è‡ªç„¶æ–­ç‚¹**ï¼šéµå¾ªè¯´è¯è€…çš„è‡ªç„¶åœé¡¿å’Œé€»è¾‘

âš ï¸ **ä¸¥ç¦**ï¼š
- åˆ›é€ è¶…è¿‡250è¯çš„å·¨å‹æ®µè½
- å¼ºè¡Œåˆå¹¶ä¸ç›¸å…³çš„å†…å®¹
- æ‰“æ–­å®Œæ•´çš„æ•…äº‹æˆ–è®ºè¿°

è¾“å‡ºæ ¼å¼ï¼šæ®µè½é—´ç”¨ç©ºè¡Œåˆ†éš”ã€‚"""

            user_prompt = f"""è¯·é‡æ–°æ•´ç†ä»¥ä¸‹{lang_instruction}æ–‡æœ¬çš„æ®µè½ç»“æ„ã€‚ä¸¥æ ¼æŒ‰ç…§è¯­ä¹‰å’Œé€»è¾‘è¿›è¡Œåˆ†æ®µï¼Œç¡®ä¿æ¯æ®µä¸è¶…è¿‡200è¯ï¼š

{text}

é‡æ–°åˆ†æ®µåçš„æ–‡æœ¬ï¼š"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=4000,  # å¯¹é½JSï¼šæ®µè½æ•´ç†é˜¶æ®µæœ€å¤§tokensâ‰ˆ4000
                temperature=0.05  # é™ä½æ¸©åº¦ï¼Œæé«˜ä¸€è‡´æ€§
            )
            
            organized_text = response.choices[0].message.content
            
            # å·¥ç¨‹éªŒè¯ï¼šæ£€æŸ¥æ®µè½é•¿åº¦
            validated_text = self._validate_paragraph_lengths(organized_text)
            
            return validated_text
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆæ®µè½æ•´ç†å¤±è´¥: {e}")
            # å¤±è´¥æ—¶ä½¿ç”¨åŸºç¡€åˆ†æ®µå¤„ç†
            return self._basic_paragraph_fallback(text)

    async def _organize_long_text_paragraphs(self, text: str, lang_instruction: str) -> str:
        """
        å¯¹äºå¾ˆé•¿çš„æ–‡æœ¬ï¼Œåˆ†å—è¿›è¡Œæ®µè½æ•´ç†
        """
        try:
            # æŒ‰ç°æœ‰æ®µè½åˆ†å‰²
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            organized_chunks = []
            
            current_chunk = []
            current_tokens = 0
            max_chunk_tokens = 2500  # é€‚åº”4000 tokensé™åˆ¶çš„chunkå¤§å°
            
            for para in paragraphs:
                para_tokens = self._estimate_tokens(para)
                
                if current_tokens + para_tokens > max_chunk_tokens and current_chunk:
                    # å¤„ç†å½“å‰chunk
                    chunk_text = '\n\n'.join(current_chunk)
                    organized_chunk = await self._organize_single_chunk(chunk_text, lang_instruction)
                    organized_chunks.append(organized_chunk)
                    
                    current_chunk = [para]
                    current_tokens = para_tokens
                else:
                    current_chunk.append(para)
                    current_tokens += para_tokens
            
            # å¤„ç†æœ€åä¸€ä¸ªchunk
            if current_chunk:
                chunk_text = '\n\n'.join(current_chunk)
                organized_chunk = await self._organize_single_chunk(chunk_text, lang_instruction)
                organized_chunks.append(organized_chunk)
            
            return '\n\n'.join(organized_chunks)
            
        except Exception as e:
            logger.error(f"é•¿æ–‡æœ¬æ®µè½æ•´ç†å¤±è´¥: {e}")
            return self._basic_paragraph_fallback(text)

    async def _organize_single_chunk(self, text: str, lang_instruction: str) -> str:
        """
        æ•´ç†å•ä¸ªæ–‡æœ¬å—çš„æ®µè½
        """
        system_prompt = f"""You are a {lang_instruction} paragraph organization expert. Reorganize paragraphs by semantics, ensuring each paragraph does not exceed 200 words.

Core requirements:
1. Strictly maintain the original {lang_instruction} language
2. Organize by semantic logic, one theme per paragraph
3. Each paragraph must not exceed 250 words
4. Separate paragraphs with blank lines
5. Keep content complete, do not reduce information"""

        user_prompt = f"""Re-paragraph the following text in {lang_instruction}, ensuring each paragraph does not exceed 200 words:

{text}"""

        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=1200,  # é€‚åº”4000 tokensæ€»é™åˆ¶
            temperature=0.05
        )
        
        return response.choices[0].message.content

    def _validate_paragraph_lengths(self, text: str) -> str:
        """
        éªŒè¯æ®µè½é•¿åº¦ï¼Œå¦‚æœæœ‰è¶…é•¿æ®µè½åˆ™å°è¯•åˆ†å‰²
        """
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        validated_paragraphs = []
        
        for para in paragraphs:
            word_count = len(para.split())
            
            if word_count > 300:  # å¦‚æœæ®µè½è¶…è¿‡300è¯
                logger.warning(f"æ£€æµ‹åˆ°è¶…é•¿æ®µè½({word_count}è¯)ï¼Œå°è¯•åˆ†å‰²")
                # å°è¯•æŒ‰å¥å­åˆ†å‰²é•¿æ®µè½
                split_paras = self._split_long_paragraph(para)
                validated_paragraphs.extend(split_paras)
            else:
                validated_paragraphs.append(para)
        
        return '\n\n'.join(validated_paragraphs)

    def _split_long_paragraph(self, paragraph: str) -> list:
        """
        åˆ†å‰²è¿‡é•¿çš„æ®µè½
        """
        import re
        
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', paragraph)
        sentences = [s.strip() + '.' for s in sentences if s.strip()]
        
        split_paragraphs = []
        current_para = []
        current_words = 0
        
        for sentence in sentences:
            sentence_words = len(sentence.split())
            
            if current_words + sentence_words > 200 and current_para:
                # å½“å‰æ®µè½è¾¾åˆ°é•¿åº¦é™åˆ¶
                split_paragraphs.append(' '.join(current_para))
                current_para = [sentence]
                current_words = sentence_words
            else:
                current_para.append(sentence)
                current_words += sentence_words
        
        # æ·»åŠ æœ€åä¸€æ®µ
        if current_para:
            split_paragraphs.append(' '.join(current_para))
        
        return split_paragraphs

    def _basic_paragraph_fallback(self, text: str) -> str:
        """
        åŸºç¡€åˆ†æ®µfallbackæœºåˆ¶
        å½“GPTæ•´ç†å¤±è´¥æ—¶ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™åˆ†æ®µ
        """
        import re
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        basic_paragraphs = []
        
        for para in paragraphs:
            word_count = len(para.split())
            
            if word_count > 250:
                # é•¿æ®µè½æŒ‰å¥å­åˆ†å‰²
                split_paras = self._split_long_paragraph(para)
                basic_paragraphs.extend(split_paras)
            elif word_count < 30 and basic_paragraphs:
                # çŸ­æ®µè½ä¸ä¸Šä¸€æ®µåˆå¹¶ï¼ˆå¦‚æœåˆå¹¶åä¸è¶…è¿‡200è¯ï¼‰
                last_para = basic_paragraphs[-1]
                combined_words = len(last_para.split()) + word_count
                
                if combined_words <= 200:
                    basic_paragraphs[-1] = last_para + ' ' + para
                else:
                    basic_paragraphs.append(para)
            else:
                basic_paragraphs.append(para)
        
        return '\n\n'.join(basic_paragraphs)

    async def summarize(self, transcript: str, target_language: str = "zh", video_title: str = None) -> str:
        """
        ç”Ÿæˆè§†é¢‘è½¬å½•çš„æ‘˜è¦
        
        Args:
            transcript: è½¬å½•æ–‡æœ¬
            target_language: ç›®æ ‡è¯­è¨€ä»£ç 
            
        Returns:
            æ‘˜è¦æ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        try:
            if not self.client:
                logger.warning("OpenAI APIä¸å¯ç”¨ï¼Œç”Ÿæˆå¤‡ç”¨æ‘˜è¦")
                return self._generate_fallback_summary(transcript, target_language, video_title)
            
            # ä¼°ç®—è½¬å½•æ–‡æœ¬é•¿åº¦ï¼Œå†³å®šæ˜¯å¦éœ€è¦åˆ†å—æ‘˜è¦
            estimated_tokens = self._estimate_tokens(transcript)
            max_summarize_tokens = 4000  # æé«˜é™åˆ¶ï¼Œä¼˜å…ˆä½¿ç”¨å•æ–‡æœ¬å¤„ç†ä»¥è·å¾—æ›´å¥½çš„æ€»ç»“è´¨é‡
            
            if estimated_tokens <= max_summarize_tokens:
                # çŸ­æ–‡æœ¬ç›´æ¥æ‘˜è¦
                return await self._summarize_single_text(transcript, target_language, video_title)
            else:
                # é•¿æ–‡æœ¬åˆ†å—æ‘˜è¦
                logger.info(f"æ–‡æœ¬è¾ƒé•¿({estimated_tokens} tokens)ï¼Œå¯ç”¨åˆ†å—æ‘˜è¦")
                return await self._summarize_with_chunks(transcript, target_language, video_title, max_summarize_tokens)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {str(e)}")
            return self._generate_fallback_summary(transcript, target_language, video_title)

    async def _summarize_single_text(self, transcript: str, target_language: str, video_title: str = None) -> str:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œæ‘˜è¦
        """
        # è·å–ç›®æ ‡è¯­è¨€åç§°
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        
        # æ„å»ºè‹±æ–‡æç¤ºè¯ï¼Œé€‚ç”¨äºæ‰€æœ‰ç›®æ ‡è¯­è¨€
        system_prompt = f"""You are a professional content analyst. Please generate a comprehensive, well-structured summary in {language_name} for the following text.

Summary Requirements:
1. Extract the main topics and core viewpoints from the text
2. Maintain clear logical structure, highlighting the core arguments
3. Include important discussions, viewpoints, and conclusions
4. Use concise and clear language
5. Appropriately preserve the speaker's expression style and key opinions

Paragraph Organization Requirements (Core):
1. **Organize by semantic and logical themes** - Start a new paragraph whenever the topic shifts, discussion focus changes, or when transitioning from one viewpoint to another
2. **Each paragraph should focus on one main point or theme**
3. **Paragraphs must be separated by blank lines (double line breaks \\n\\n)**
4. **Consider the logical flow of content and reasonably divide paragraph boundaries**

Format Requirements:
1. Use Markdown format with double line breaks between paragraphs
2. Each paragraph should be a complete logical unit
3. Write entirely in {language_name}
4. Aim for substantial content (600-1200 words when appropriate)"""

        user_prompt = f"""Based on the following content, write a comprehensive, well-structured summary in {language_name}:

{transcript}

Requirements:
- Focus on natural paragraphs, avoiding decorative headings
- Cover all key ideas and arguments, preserving important examples and data
- Ensure balanced coverage of both early and later content
- Use restrained but comprehensive language
- Organize content logically with proper paragraph breaks"""

        logger.info(f"æ­£åœ¨ç”Ÿæˆ{language_name}æ‘˜è¦...")
        
        # è°ƒç”¨OpenAI API
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=3500,  # æ§åˆ¶åœ¨å®‰å…¨èŒƒå›´å†…ï¼Œé¿å…è¶…å‡ºæ¨¡å‹é™åˆ¶
            temperature=0.3
        )
        
        summary = response.choices[0].message.content

        return self._format_summary_with_meta(summary, target_language, video_title)

    async def _summarize_with_chunks(self, transcript: str, target_language: str, video_title: str, max_tokens: int) -> str:
        """
        åˆ†å—æ‘˜è¦é•¿æ–‡æœ¬
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")

        # ä½¿ç”¨JSç­–ç•¥ï¼šæŒ‰å­—ç¬¦è¿›è¡Œæ™ºèƒ½åˆ†å—ï¼ˆæ®µè½>å¥å­ï¼‰
        chunks = self._smart_chunk_text(transcript, max_chars_per_chunk=4000)
        logger.info(f"åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œæ‘˜è¦")
        
        chunk_summaries = []
        
        # æ¯å—ç”Ÿæˆå±€éƒ¨æ‘˜è¦
        for i, chunk in enumerate(chunks):
            logger.info(f"æ­£åœ¨æ‘˜è¦ç¬¬ {i+1}/{len(chunks)} å—...")
            
            system_prompt = f"""You are a summarization expert. Please write a high-density summary for this text chunk in {language_name}.

This is part {i+1} of {len(chunks)} of the complete content (Part {i+1}/{len(chunks)}).

Output preferences: Focus on natural paragraphs, use minimal bullet points if necessary; highlight new information and its relationship to the main narrative; avoid vague repetition and formatted headings; moderate length (suggested 120-220 words)."""

            user_prompt = f"""[Part {i+1}/{len(chunks)}] Summarize the key points of the following text in {language_name} (natural paragraphs preferred, minimal bullet points, 120-220 words):

{chunk}

Avoid using any subheadings or decorative separators, output content only."""

            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=1000,  # æå‡åˆ†å—æ‘˜è¦å®¹é‡ä»¥æ¶µç›–æ›´å¤šç»†èŠ‚
                    temperature=0.3
                )
                
                chunk_summary = response.choices[0].message.content
                chunk_summaries.append(chunk_summary)
                
            except Exception as e:
                logger.error(f"æ‘˜è¦ç¬¬ {i+1} å—å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ç”Ÿæˆç®€å•æ‘˜è¦
                simple_summary = f"ç¬¬{i+1}éƒ¨åˆ†å†…å®¹æ¦‚è¿°ï¼š" + chunk[:200] + "..."
                chunk_summaries.append(simple_summary)
        
        # åˆå¹¶æ‰€æœ‰å±€éƒ¨æ‘˜è¦ï¼ˆå¸¦ç¼–å·ï¼‰ï¼Œå¦‚åˆ†å—è¾ƒå¤šåˆ™åˆ†å±‚æ•´åˆï¼ˆä¸å¼•å…¥å°æ ‡é¢˜ï¼‰
        combined_summaries = "\n\n".join([f"[Part {idx+1}]\n" + s for idx, s in enumerate(chunk_summaries)])

        logger.info("æ­£åœ¨æ•´åˆæœ€ç»ˆæ‘˜è¦...")
        if len(chunk_summaries) > 10:
            final_summary = await self._integrate_hierarchical_summaries(chunk_summaries, target_language)
        else:
            final_summary = await self._integrate_chunk_summaries(combined_summaries, target_language)

        return self._format_summary_with_meta(final_summary, target_language, video_title)

    def _smart_chunk_text(self, text: str, max_chars_per_chunk: int = 3500) -> list:
        """æ™ºèƒ½åˆ†å—ï¼ˆå…ˆæ®µè½åå¥å­ï¼‰ï¼ŒæŒ‰å­—ç¬¦ä¸Šé™åˆ‡åˆ†ã€‚"""
        chunks = []
        paragraphs = [p for p in text.split('\n\n') if p.strip()]
        cur = ""
        for p in paragraphs:
            candidate = (cur + "\n\n" + p).strip() if cur else p
            if len(candidate) > max_chars_per_chunk and cur:
                chunks.append(cur.strip())
                cur = p
            else:
                cur = candidate
        if cur.strip():
            chunks.append(cur.strip())

        # äºŒæ¬¡æŒ‰å¥å­åˆ‡åˆ†è¿‡é•¿å—
        import re
        final_chunks = []
        for c in chunks:
            if len(c) <= max_chars_per_chunk:
                final_chunks.append(c)
            else:
                sentences = [s.strip() for s in re.split(r"[ã€‚ï¼ï¼Ÿ\.!?]+", c) if s.strip()]
                scur = ""
                for s in sentences:
                    candidate = (scur + 'ã€‚' + s).strip() if scur else s
                    if len(candidate) > max_chars_per_chunk and scur:
                        final_chunks.append(scur.strip())
                        scur = s
                    else:
                        scur = candidate
                if scur.strip():
                    final_chunks.append(scur.strip())
        return final_chunks

    async def _integrate_chunk_summaries(self, combined_summaries: str, target_language: str) -> str:
        """
        æ•´åˆåˆ†å—æ‘˜è¦ä¸ºæœ€ç»ˆè¿è´¯æ‘˜è¦
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        
        try:
            system_prompt = f"""You are a content integration expert. Please integrate multiple segmented summaries into a complete, coherent summary in {language_name}.

Integration Requirements:
1. Remove duplicate content and maintain clear logic
2. Reorganize content by themes or chronological order
3. Each paragraph must be separated by double line breaks
4. Ensure output is in Markdown format with double line breaks between paragraphs
5. Use concise and clear language
6. Form a complete content summary
7. Cover all parts comprehensively without omission"""

            user_prompt = f"""Please integrate the following segmented summaries into a complete, coherent summary in {language_name}:

{combined_summaries}

Requirements:
- Remove duplicate content and maintain clear logic
- Reorganize content by themes or chronological order
- Each paragraph must be separated by double line breaks
- Ensure output is in Markdown format with double line breaks between paragraphs
- Use concise and clear language
- Form a complete content summary"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=2500,  # æ§åˆ¶è¾“å‡ºè§„æ¨¡ï¼Œå…¼é¡¾ä¸Šä¸‹æ–‡å®‰å…¨
                temperature=0.3
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"æ•´åˆæ‘˜è¦å¤±è´¥: {e}")
            # å¤±è´¥æ—¶ç›´æ¥åˆå¹¶
            return combined_summaries

    def _format_summary_with_meta(self, summary: str, target_language: str, video_title: str = None) -> str:
        """
        ä¸ºæ‘˜è¦æ·»åŠ æ ‡é¢˜å’Œå…ƒä¿¡æ¯
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        meta_labels = self._get_summary_labels(target_language)
        
        # ä¸åŠ ä»»ä½•å°æ ‡é¢˜/å…è´£å£°æ˜ï¼Œå¯ä¿ç•™è§†é¢‘æ ‡é¢˜ä½œä¸ºä¸€çº§æ ‡é¢˜
        if video_title:
            prefix = f"# {video_title}\n\n"
        else:
            prefix = ""
        return prefix + summary

        try:
            resp = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                max_tokens=1200,
                temperature=0.2,
            )
            return resp.choices[0].message.content
        except Exception:
            return text
    
    def _generate_fallback_summary(self, transcript: str, target_language: str, video_title: str = None) -> str:
        """
        ç”Ÿæˆå¤‡ç”¨æ‘˜è¦ï¼ˆå½“OpenAI APIä¸å¯ç”¨æ—¶ï¼‰
        
        Args:
            transcript: è½¬å½•æ–‡æœ¬
            video_title: è§†é¢‘æ ‡é¢˜
            target_language: ç›®æ ‡è¯­è¨€ä»£ç 
            
        Returns:
            å¤‡ç”¨æ‘˜è¦æ–‡æœ¬
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        
        # ç®€å•çš„æ–‡æœ¬å¤„ç†ï¼Œæå–å…³é”®ä¿¡æ¯
        lines = transcript.split('\n')
        content_lines = [line for line in lines if line.strip() and not line.startswith('#') and not line.startswith('**')]
        
        # è®¡ç®—å¤§æ¦‚çš„é•¿åº¦
        total_chars = sum(len(line) for line in content_lines)
        
        # ä½¿ç”¨ç›®æ ‡è¯­è¨€çš„æ ‡ç­¾
        meta_labels = self._get_summary_labels(target_language)
        fallback_labels = self._get_fallback_labels(target_language)
        
        # ç›´æ¥ä½¿ç”¨è§†é¢‘æ ‡é¢˜ä½œä¸ºä¸»æ ‡é¢˜  
        title = video_title if video_title else "Summary"
        
        summary = f"""# {title}

**{meta_labels['language_label']}:** {language_name}
**{fallback_labels['notice']}:** {fallback_labels['api_unavailable']}



## {fallback_labels['overview_title']}

**{fallback_labels['content_length']}:** {fallback_labels['about']} {total_chars} {fallback_labels['characters']}
**{fallback_labels['paragraph_count']}:** {len(content_lines)} {fallback_labels['paragraphs']}

## {fallback_labels['main_content']}

{fallback_labels['content_description']}

{fallback_labels['suggestions_intro']}

1. {fallback_labels['suggestion_1']}
2. {fallback_labels['suggestion_2']}
3. {fallback_labels['suggestion_3']}

## {fallback_labels['recommendations']}

- {fallback_labels['recommendation_1']}
- {fallback_labels['recommendation_2']}


<br/>

<p style="color: #888; font-style: italic; text-align: center; margin-top: 16px;"><em>{fallback_labels['fallback_disclaimer']}</em></p>"""
        
        return summary
    
    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def get_supported_languages(self) -> dict:
        """
        è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
        
        Returns:
            è¯­è¨€ä»£ç åˆ°è¯­è¨€åç§°çš„æ˜ å°„
        """
        return self.language_map.copy()
    
    def _detect_transcript_language(self, transcript: str) -> str:
        """
        æ£€æµ‹è½¬å½•æ–‡æœ¬çš„ä¸»è¦è¯­è¨€
        
        Args:
            transcript: è½¬å½•æ–‡æœ¬
            
        Returns:
            æ£€æµ‹åˆ°çš„è¯­è¨€ä»£ç 
        """
        # ç®€å•çš„è¯­è¨€æ£€æµ‹é€»è¾‘ï¼šæŸ¥æ‰¾è½¬å½•æ–‡æœ¬ä¸­çš„è¯­è¨€æ ‡è®°
        if "**æ£€æµ‹è¯­è¨€:**" in transcript:
            # ä»Whisperè½¬å½•ä¸­æå–æ£€æµ‹åˆ°çš„è¯­è¨€
            lines = transcript.split('\n')
            for line in lines:
                if "**æ£€æµ‹è¯­è¨€:**" in line:
                    # æå–è¯­è¨€ä»£ç ï¼Œä¾‹å¦‚: "**æ£€æµ‹è¯­è¨€:** en"
                    lang = line.split(":")[-1].strip()
                    return lang
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯­è¨€æ ‡è®°ï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦æ£€æµ‹
        # è®¡ç®—è‹±æ–‡å­—ç¬¦ã€ä¸­æ–‡å­—ç¬¦ç­‰çš„æ¯”ä¾‹
        total_chars = len(transcript)
        if total_chars == 0:
            return "en"  # é»˜è®¤è‹±æ–‡
            
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = sum(1 for char in transcript if '\u4e00' <= char <= '\u9fff')
        chinese_ratio = chinese_chars / total_chars
        
        # ç»Ÿè®¡è‹±æ–‡å­—æ¯
        english_chars = sum(1 for char in transcript if char.isascii() and char.isalpha())
        english_ratio = english_chars / total_chars
        
        # æ ¹æ®æ¯”ä¾‹åˆ¤æ–­
        if chinese_ratio > 0.3:
            return "zh"
        elif english_ratio > 0.3:
            return "en"
        else:
            return "en"  # é»˜è®¤è‹±æ–‡
    
    def _get_language_instruction(self, lang_code: str) -> str:
        """
        æ ¹æ®è¯­è¨€ä»£ç è·å–ä¼˜åŒ–æŒ‡ä»¤ä¸­ä½¿ç”¨çš„è¯­è¨€åç§°
        
        Args:
            lang_code: è¯­è¨€ä»£ç 
            
        Returns:
            è¯­è¨€åç§°
        """
        language_instructions = {
            "en": "English",
            "zh": "ä¸­æ–‡",
            "ja": "æ—¥æœ¬èª",
            "ko": "í•œêµ­ì–´",
            "es": "EspaÃ±ol",
            "fr": "FranÃ§ais",
            "de": "Deutsch",
            "it": "Italiano",
            "pt": "PortuguÃªs",
            "ru": "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
            "ar": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
        }
        return language_instructions.get(lang_code, "English")
    

    def _get_summary_labels(self, lang_code: str) -> dict:
        """
        è·å–æ‘˜è¦é¡µé¢çš„å¤šè¯­è¨€æ ‡ç­¾
        
        Args:
            lang_code: è¯­è¨€ä»£ç 
            
        Returns:
            æ ‡ç­¾å­—å…¸
        """
        labels = {
            "en": {
                "language_label": "Summary Language",
                "disclaimer": "This summary is automatically generated by AI for reference only"
            },
            "zh": {
                "language_label": "æ‘˜è¦è¯­è¨€",
                "disclaimer": "æœ¬æ‘˜è¦ç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ"
            },
            "ja": {
                "language_label": "è¦ç´„è¨€èª",
                "disclaimer": "ã“ã®è¦ç´„ã¯AIã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¦ãŠã‚Šã€å‚è€ƒç”¨ã§ã™"
            },
            "ko": {
                "language_label": "ìš”ì•½ ì–¸ì–´",
                "disclaimer": "ì´ ìš”ì•½ì€ AIì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìœ¼ë©° ì°¸ê³ ìš©ì…ë‹ˆë‹¤"
            },
            "es": {
                "language_label": "Idioma del Resumen",
                "disclaimer": "Este resumen es generado automÃ¡ticamente por IA, solo para referencia"
            },
            "fr": {
                "language_label": "Langue du RÃ©sumÃ©",
                "disclaimer": "Ce rÃ©sumÃ© est gÃ©nÃ©rÃ© automatiquement par IA, Ã  titre de rÃ©fÃ©rence uniquement"
            },
            "de": {
                "language_label": "Zusammenfassungssprache",
                "disclaimer": "Diese Zusammenfassung wird automatisch von KI generiert, nur zur Referenz"
            },
            "it": {
                "language_label": "Lingua del Riassunto",
                "disclaimer": "Questo riassunto Ã¨ generato automaticamente dall'IA, solo per riferimento"
            },
            "pt": {
                "language_label": "Idioma do Resumo",
                "disclaimer": "Este resumo Ã© gerado automaticamente por IA, apenas para referÃªncia"
            },
            "ru": {
                "language_label": "Ğ¯Ğ·Ñ‹Ğº Ñ€ĞµĞ·ÑĞ¼Ğµ",
                "disclaimer": "Ğ­Ñ‚Ğ¾ Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ˜Ğ˜, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ¸"
            },
            "ar": {
                "language_label": "Ù„ØºØ© Ø§Ù„Ù…Ù„Ø®Øµ",
                "disclaimer": "Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ø®Øµ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ù„Ù„Ù…Ø±Ø¬Ø¹ ÙÙ‚Ø·"
            }
        }
        return labels.get(lang_code, labels["en"])
    
    def _get_fallback_labels(self, lang_code: str) -> dict:
        """
        è·å–å¤‡ç”¨æ‘˜è¦çš„å¤šè¯­è¨€æ ‡ç­¾
        
        Args:
            lang_code: è¯­è¨€ä»£ç 
            
        Returns:
            æ ‡ç­¾å­—å…¸
        """
        labels = {
            "en": {
                "notice": "Notice",
                "api_unavailable": "OpenAI API is unavailable, this is a simplified summary",
                "overview_title": "Transcript Overview",
                "content_length": "Content Length",
                "about": "About",
                "characters": "characters",
                "paragraph_count": "Paragraph Count",
                "paragraphs": "paragraphs",
                "main_content": "Main Content",
                "content_description": "The transcript contains complete video speech content. Since AI summary cannot be generated currently, we recommend:",
                "suggestions_intro": "For detailed information, we suggest you:",
                "suggestion_1": "Review the complete transcript text for detailed information",
                "suggestion_2": "Focus on important paragraphs marked with timestamps",
                "suggestion_3": "Manually extract key points and takeaways",
                "recommendations": "Recommendations",
                "recommendation_1": "Configure OpenAI API key for better summary functionality",
                "recommendation_2": "Or use other AI services for text summarization",
                "fallback_disclaimer": "This is an automatically generated fallback summary"
            },
            "zh": {
                "notice": "æ³¨æ„",
                "api_unavailable": "ç”±äºOpenAI APIä¸å¯ç”¨ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ‘˜è¦",
                "overview_title": "è½¬å½•æ¦‚è§ˆ",
                "content_length": "å†…å®¹é•¿åº¦",
                "about": "çº¦",
                "characters": "å­—ç¬¦",
                "paragraph_count": "æ®µè½æ•°é‡",
                "paragraphs": "æ®µ",
                "main_content": "ä¸»è¦å†…å®¹",
                "content_description": "è½¬å½•æ–‡æœ¬åŒ…å«äº†å®Œæ•´çš„è§†é¢‘è¯­éŸ³å†…å®¹ã€‚ç”±äºå½“å‰æ— æ³•ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ï¼Œå»ºè®®æ‚¨ï¼š",
                "suggestions_intro": "ä¸ºè·å–è¯¦ç»†ä¿¡æ¯ï¼Œå»ºè®®æ‚¨ï¼š",
                "suggestion_1": "æŸ¥çœ‹å®Œæ•´çš„è½¬å½•æ–‡æœ¬ä»¥è·å–è¯¦ç»†ä¿¡æ¯",
                "suggestion_2": "å…³æ³¨æ—¶é—´æˆ³æ ‡è®°çš„é‡è¦æ®µè½",
                "suggestion_3": "æ‰‹åŠ¨æå–å…³é”®è§‚ç‚¹å’Œè¦ç‚¹",
                "recommendations": "å»ºè®®",
                "recommendation_1": "é…ç½®OpenAI APIå¯†é’¥ä»¥è·å¾—æ›´å¥½çš„æ‘˜è¦åŠŸèƒ½",
                "recommendation_2": "æˆ–è€…ä½¿ç”¨å…¶ä»–AIæœåŠ¡è¿›è¡Œæ–‡æœ¬æ€»ç»“",
                "fallback_disclaimer": "æœ¬æ‘˜è¦ä¸ºè‡ªåŠ¨ç”Ÿæˆçš„å¤‡ç”¨ç‰ˆæœ¬"
            }
        }
        return labels.get(lang_code, labels["en"])
    
    def is_available(self) -> bool:
        """
        æ£€æŸ¥æ‘˜è¦æœåŠ¡æ˜¯å¦å¯ç”¨
        
        Returns:
            True if OpenAI API is configured, False otherwise
        """
        return self.client is not None
