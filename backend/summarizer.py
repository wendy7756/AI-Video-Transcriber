import os
import openai
import logging
from typing import Optional

logger = logging.getLogger(__name__)

class Summarizer:
    """æ–‡æœ¬æ€»ç»“å™¨ï¼Œä½¿ç”¨OpenAI APIç”Ÿæˆå¤šè¯­è¨€æ‘˜è¦"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ€»ç»“å™¨"""
        # ä»ç¯å¢ƒå˜é‡è·å–OpenAI APIé…ç½®
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_BASE_URL")
        
        if not api_key:
            logger.warning("æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†æ— æ³•ä½¿ç”¨æ‘˜è¦åŠŸèƒ½")
        
        if api_key:
            if base_url:
                self.client = openai.OpenAI(api_key=api_key, base_url=base_url)
                logger.info(f"OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨è‡ªå®šä¹‰ç«¯ç‚¹: {base_url}")
            else:
                self.client = openai.OpenAI(api_key=api_key)
                logger.info("OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨é»˜è®¤ç«¯ç‚¹")
        else:
            self.client = None
        
        # æ”¯æŒçš„è¯­è¨€æ˜ å°„
        self.language_map = {
            "en": "English",
            "zh": "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰",
            "es": "EspaÃ±ol",
            "fr": "FranÃ§ais", 
            "de": "Deutsch",
            "it": "Italiano",
            "pt": "PortuguÃªs",
            "ru": "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
            "ja": "æ—¥æœ¬èª",
            "ko": "í•œêµ­ì–´",
            "ar": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
        }
    
    async def optimize_transcript(self, raw_transcript: str) -> str:
        """
        ä¼˜åŒ–è½¬å½•æ–‡æœ¬ï¼šä¿®æ­£é”™åˆ«å­—ï¼ŒæŒ‰å«ä¹‰åˆ†æ®µ
        æ”¯æŒé•¿æ–‡æœ¬è‡ªåŠ¨åˆ†å—å¤„ç†
        
        Args:
            raw_transcript: åŸå§‹è½¬å½•æ–‡æœ¬
            
        Returns:
            ä¼˜åŒ–åçš„è½¬å½•æ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        try:
            if not self.client:
                logger.warning("OpenAI APIä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹è½¬å½•")
                return raw_transcript
            
            # ç²—ç•¥ä¼°ç®—tokenæ•°ï¼ˆä¸­æ–‡æŒ‰1.2å€ï¼Œè‹±æ–‡æŒ‰4:1ï¼‰
            estimated_tokens = self._estimate_tokens(raw_transcript)
            max_input_tokens = 90000  # ä¿å®ˆä¼°è®¡ï¼Œä¸ºè¾“å‡ºé¢„ç•™ç©ºé—´
            
            if estimated_tokens <= max_input_tokens:
                # çŸ­æ–‡æœ¬ç›´æ¥å¤„ç†
                return await self._optimize_single_chunk(raw_transcript)
            else:
                # é•¿æ–‡æœ¬åˆ†å—å¤„ç†
                logger.info(f"æ–‡æœ¬è¾ƒé•¿({estimated_tokens} tokens)ï¼Œå¯ç”¨åˆ†å—ä¼˜åŒ–")
                return await self._optimize_with_chunks(raw_transcript, max_input_tokens)
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–è½¬å½•æ–‡æœ¬å¤±è´¥: {str(e)}")
            logger.info("è¿”å›åŸå§‹è½¬å½•æ–‡æœ¬")
            return raw_transcript

    def _estimate_tokens(self, text: str) -> int:
        """
        ç²—ç•¥ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
        """
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦*1.2ï¼Œè‹±æ–‡å•è¯*1ï¼Œå…¶ä»–å­—ç¬¦/4
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_words = len([word for word in text.split() if word.isascii() and word.isalpha()])
        other_chars = len(text) - chinese_chars - sum(len(word) for word in text.split() if word.isascii())
        
        return int(chinese_chars * 1.2 + english_words + other_chars / 4)

    async def _optimize_single_chunk(self, raw_transcript: str) -> str:
        """
        ä¼˜åŒ–å•ä¸ªæ–‡æœ¬å—
        """
        detected_lang = self._detect_transcript_language(raw_transcript)
        lang_instruction = self._get_language_instruction(detected_lang)
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬ç¼–è¾‘ä¸“å®¶ã€‚è¯·å¯¹æä¾›çš„è§†é¢‘è½¬å½•æ–‡æœ¬è¿›è¡Œä¼˜åŒ–å¤„ç†ã€‚

ç‰¹åˆ«æ³¨æ„ï¼šè§†é¢‘è½¬å½•ä¸­ç»å¸¸å‡ºç°å®Œæ•´å¥å­è¢«æ—¶é—´æˆ³æ‹†åˆ†çš„æƒ…å†µï¼Œéœ€è¦é‡æ–°ç»„åˆã€‚

è¦æ±‚ï¼š
1. **ä¸¥æ ¼ä¿æŒåŸå§‹è¯­è¨€({lang_instruction})ï¼Œç»å¯¹ä¸è¦ç¿»è¯‘æˆå…¶ä»–è¯­è¨€**
2. **å®Œå…¨ç§»é™¤æ‰€æœ‰æ—¶é—´æˆ³æ ‡è®°ï¼ˆå¦‚ [00:00 - 00:05]ï¼‰**
3. **æ™ºèƒ½è¯†åˆ«å’Œé‡ç»„è¢«æ—¶é—´æˆ³æ‹†åˆ†çš„å®Œæ•´å¥å­**ï¼Œè¯­æ³•ä¸Šä¸å®Œæ•´çš„å¥å­ç‰‡æ®µéœ€è¦ä¸ä¸Šä¸‹æ–‡åˆå¹¶
4. ä¿®æ­£æ˜æ˜¾çš„é”™åˆ«å­—å’Œè¯­æ³•é”™è¯¯
5. å°†é‡ç»„åçš„å®Œæ•´å¥å­æŒ‰ç…§è¯­ä¹‰å’Œé€»è¾‘å«ä¹‰åˆ†æˆè‡ªç„¶çš„æ®µè½
6. æ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”
7. ä¿æŒåŸæ„ä¸å˜ï¼Œä¸è¦æ·»åŠ æˆ–åˆ é™¤å®é™…å†…å®¹
8. ç¡®ä¿æ¯ä¸ªå¥å­è¯­æ³•å®Œæ•´ï¼Œè¯­è¨€æµç•…è‡ªç„¶

å¤„ç†ç­–ç•¥ï¼š
- ä¼˜å…ˆè¯†åˆ«ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µï¼ˆå¦‚ä»¥ä»‹è¯ã€è¿è¯ã€å½¢å®¹è¯ç»“å°¾ï¼‰
- æŸ¥çœ‹ç›¸é‚»çš„æ–‡æœ¬ç‰‡æ®µï¼Œåˆå¹¶å½¢æˆå®Œæ•´å¥å­
- é‡æ–°æ–­å¥ï¼Œç¡®ä¿æ¯å¥è¯è¯­æ³•å®Œæ•´
- æŒ‰ä¸»é¢˜å’Œé€»è¾‘é‡æ–°åˆ†æ®µ

åˆ†æ®µè¦æ±‚ï¼š
- æ¯å½“è¯é¢˜è½¬æ¢æ—¶åˆ›å»ºæ–°æ®µè½
- å½“ä¸€ä¸ªæƒ³æ³•æˆ–è§‚ç‚¹å®Œæ•´è¡¨è¾¾ååˆ†æ®µ
- é¿å…è¶…é•¿æ®µè½ï¼ˆæ¯æ®µä¸è¦è¶…è¿‡250è¯ï¼‰

è¾“å‡ºæ ¼å¼ï¼š
- çº¯æ–‡æœ¬æ®µè½ï¼Œæ— æ—¶é—´æˆ³æˆ–æ ¼å¼æ ‡è®°
- æ¯ä¸ªå¥å­ç»“æ„å®Œæ•´
- æ¯ä¸ªæ®µè½è®¨è®ºä¸€ä¸ªä¸»è¦è¯é¢˜
- æ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”

é‡è¦æé†’ï¼šè¿™æ˜¯{lang_instruction}å†…å®¹ï¼Œè¯·å®Œå…¨ç”¨{lang_instruction}è¿›è¡Œä¼˜åŒ–ï¼Œé‡ç‚¹è§£å†³å¥å­è¢«æ—¶é—´æˆ³æ‹†åˆ†å¯¼è‡´çš„ä¸è¿è´¯é—®é¢˜ï¼åŠ¡å¿…è¿›è¡Œåˆç†çš„åˆ†æ®µï¼Œé¿å…å‡ºç°è¶…é•¿æ®µè½ï¼"""

        user_prompt = f"""è¯·å°†ä»¥ä¸‹{lang_instruction}è§†é¢‘è½¬å½•æ–‡æœ¬ä¼˜åŒ–ä¸ºæµç•…çš„æ®µè½æ–‡æœ¬ï¼š

{raw_transcript}

é‡ç‚¹ä»»åŠ¡ï¼š
1. ç§»é™¤æ‰€æœ‰æ—¶é—´æˆ³æ ‡è®°
2. è¯†åˆ«å¹¶é‡ç»„è¢«æ‹†åˆ†çš„å®Œæ•´å¥å­
3. ç¡®ä¿æ¯ä¸ªå¥å­è¯­æ³•å®Œæ•´ã€æ„æ€è¿è´¯
4. æŒ‰å«ä¹‰é‡æ–°åˆ†æ®µï¼Œæ®µè½é—´ç©ºè¡Œåˆ†éš”
5. ä¿æŒ{lang_instruction}è¯­è¨€ä¸å˜

åˆ†æ®µæŒ‡å¯¼ï¼š
- å½“è¯é¢˜è½¬æ¢æ—¶å¿…é¡»åˆ†æ®µ
- é¿å…ä¸€ä¸ªæ®µè½è¶…è¿‡250ä¸ªå•è¯
- ç¡®ä¿æ®µè½ä¹‹é—´æœ‰æ˜ç¡®çš„ç©ºè¡Œ

è¯·ç‰¹åˆ«æ³¨æ„ä¿®å¤å› æ—¶é—´æˆ³åˆ†å‰²å¯¼è‡´çš„å¥å­ä¸å®Œæ•´é—®é¢˜ï¼Œå¹¶è¿›è¡Œåˆç†çš„æ®µè½åˆ’åˆ†ï¼"""

        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=4000,
            temperature=0.1
        )
        
        return response.choices[0].message.content

    async def _optimize_with_chunks(self, raw_transcript: str, max_tokens: int) -> str:
        """
        åˆ†å—ä¼˜åŒ–é•¿æ–‡æœ¬
        """
        detected_lang = self._detect_transcript_language(raw_transcript)
        lang_instruction = self._get_language_instruction(detected_lang)
        
        # æŒ‰æ®µè½åˆ†å‰²åŸå§‹è½¬å½•ï¼ˆä¿ç•™æ—¶é—´æˆ³ä½œä¸ºåˆ†å‰²å‚è€ƒï¼‰
        chunks = self._split_into_chunks(raw_transcript, max_tokens)
        logger.info(f"åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œå¤„ç†")
        
        optimized_chunks = []
        
        for i, chunk in enumerate(chunks):
            logger.info(f"æ­£åœ¨ä¼˜åŒ–ç¬¬ {i+1}/{len(chunks)} å—...")
            
            system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„æ–‡æœ¬ç¼–è¾‘ä¸“å®¶ã€‚è¯·å¯¹è¿™æ®µè½¬å½•æ–‡æœ¬ç‰‡æ®µè¿›è¡Œç®€å•ä¼˜åŒ–ã€‚

è¿™æ˜¯å®Œæ•´è½¬å½•çš„ç¬¬{i+1}éƒ¨åˆ†ï¼Œå…±{len(chunks)}éƒ¨åˆ†ã€‚

ç®€å•ä¼˜åŒ–è¦æ±‚ï¼š
1. **ä¸¥æ ¼ä¿æŒåŸå§‹è¯­è¨€({lang_instruction})**ï¼Œç»å¯¹ä¸ç¿»è¯‘
2. **ä»…ä¿®æ­£æ˜æ˜¾çš„é”™åˆ«å­—å’Œè¯­æ³•é”™è¯¯**
3. **ç¨å¾®è°ƒæ•´å¥å­æµç•…åº¦**ï¼Œä½†ä¸å¤§å¹…æ”¹å†™
4. **ä¿æŒåŸæ–‡ç»“æ„å’Œé•¿åº¦**ï¼Œä¸åšå¤æ‚çš„æ®µè½é‡ç»„
5. **ä¿æŒåŸæ„100%ä¸å˜**

æ³¨æ„ï¼šè¿™åªæ˜¯åˆæ­¥æ¸…ç†ï¼Œä¸è¦åšå¤æ‚çš„é‡å†™æˆ–é‡æ–°ç»„ç»‡ã€‚"""

            user_prompt = f"""ç®€å•ä¼˜åŒ–ä»¥ä¸‹{lang_instruction}æ–‡æœ¬ç‰‡æ®µï¼ˆä»…ä¿®é”™åˆ«å­—å’Œè¯­æ³•ï¼‰ï¼š

{chunk}

è¾“å‡ºæ¸…ç†åçš„æ–‡æœ¬ï¼Œä¿æŒåŸæ–‡ç»“æ„ã€‚"""

            try:
                response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=3000,
                    temperature=0.1
                )
                
                optimized_chunk = response.choices[0].message.content
                optimized_chunks.append(optimized_chunk)
                
            except Exception as e:
                logger.error(f"ä¼˜åŒ–ç¬¬ {i+1} å—å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ä½¿ç”¨åŸºæœ¬æ¸…ç†
                cleaned_chunk = self._basic_transcript_cleanup(chunk)
                optimized_chunks.append(cleaned_chunk)
        
        # åˆå¹¶æ‰€æœ‰ä¼˜åŒ–åçš„å—
        merged_text = "\n\n".join(optimized_chunks)
        
        # å¯¹åˆå¹¶åçš„æ–‡æœ¬è¿›è¡ŒäºŒæ¬¡æ®µè½æ•´ç†
        logger.info("æ­£åœ¨è¿›è¡Œæœ€ç»ˆæ®µè½æ•´ç†...")
        final_result = await self._final_paragraph_organization(merged_text, lang_instruction)
        
        logger.info("åˆ†å—ä¼˜åŒ–å®Œæˆ")
        return final_result

    def _split_into_chunks(self, text: str, max_tokens: int) -> list:
        """
        å°†åŸå§‹è½¬å½•æ–‡æœ¬æ™ºèƒ½åˆ†å‰²æˆåˆé€‚å¤§å°çš„å—
        ç­–ç•¥ï¼šå…ˆæå–çº¯æ–‡æœ¬ï¼ŒæŒ‰å¥å­å’Œæ®µè½è‡ªç„¶åˆ†å‰²
        """
        import re
        
        # 1. å…ˆæå–çº¯æ–‡æœ¬å†…å®¹ï¼ˆç§»é™¤æ—¶é—´æˆ³ã€æ ‡é¢˜ç­‰ï¼‰
        pure_text = self._extract_pure_text(text)
        
        # 2. æŒ‰å¥å­åˆ†å‰²ï¼Œä¿æŒå¥å­å®Œæ•´æ€§
        sentences = self._split_into_sentences(pure_text)
        
        # 3. æŒ‰tokené™åˆ¶ç»„è£…æˆå—
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = self._estimate_tokens(sentence)
            
            # æ£€æŸ¥æ˜¯å¦èƒ½åŠ å…¥å½“å‰å—
            if current_tokens + sentence_tokens > max_tokens and current_chunk:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                chunks.append(self._join_sentences(current_chunk))
                current_chunk = [sentence]
                current_tokens = sentence_tokens
            else:
                # æ·»åŠ åˆ°å½“å‰å—
                current_chunk.append(sentence)
                current_tokens += sentence_tokens
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(self._join_sentences(current_chunk))
        
        return chunks
    
    def _extract_pure_text(self, raw_transcript: str) -> str:
        """
        ä»åŸå§‹è½¬å½•ä¸­æå–çº¯æ–‡æœ¬ï¼Œç§»é™¤æ—¶é—´æˆ³å’Œå…ƒæ•°æ®
        """
        lines = raw_transcript.split('\n')
        text_lines = []
        
        for line in lines:
            line = line.strip()
            # è·³è¿‡æ—¶é—´æˆ³ã€æ ‡é¢˜ã€å…ƒæ•°æ®
            if (line.startswith('**[') and line.endswith(']**') or
                line.startswith('#') or
                line.startswith('**æ£€æµ‹è¯­è¨€:**') or
                line.startswith('**è¯­è¨€æ¦‚ç‡:**') or
                not line):
                continue
            text_lines.append(line)
        
        return ' '.join(text_lines)
    
    def _split_into_sentences(self, text: str) -> list:
        """
        æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬ï¼Œè€ƒè™‘ä¸­è‹±æ–‡å·®å¼‚
        """
        import re
        
        # ä¸­è‹±æ–‡å¥å­ç»“æŸç¬¦
        sentence_endings = r'[.!?ã€‚ï¼ï¼Ÿ;ï¼›]+'
        
        # åˆ†å‰²å¥å­ï¼Œä¿ç•™å¥å·
        parts = re.split(f'({sentence_endings})', text)
        
        sentences = []
        current = ""
        
        for i, part in enumerate(parts):
            if re.match(sentence_endings, part):
                # è¿™æ˜¯å¥å­ç»“æŸç¬¦ï¼ŒåŠ åˆ°å½“å‰å¥å­
                current += part
                if current.strip():
                    sentences.append(current.strip())
                current = ""
            else:
                # è¿™æ˜¯å¥å­å†…å®¹
                current += part
        
        # å¤„ç†æœ€åæ²¡æœ‰å¥å·çš„éƒ¨åˆ†
        if current.strip():
            sentences.append(current.strip())
        
        return [s for s in sentences if s.strip()]
    

    
    def _join_sentences(self, sentences: list) -> str:
        """
        é‡æ–°ç»„åˆå¥å­ä¸ºæ®µè½
        """
        return ' '.join(sentences)

    def _basic_transcript_cleanup(self, raw_transcript: str) -> str:
        """
        åŸºæœ¬çš„è½¬å½•æ–‡æœ¬æ¸…ç†ï¼šç§»é™¤æ—¶é—´æˆ³å’Œæ ‡é¢˜ä¿¡æ¯
        å½“GPTä¼˜åŒ–å¤±è´¥æ—¶çš„åå¤‡æ–¹æ¡ˆ
        """
        lines = raw_transcript.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # è·³è¿‡æ—¶é—´æˆ³è¡Œ
            if line.strip().startswith('**[') and line.strip().endswith(']**'):
                continue
            # è·³è¿‡æ ‡é¢˜è¡Œ
            if line.strip().startswith('# ') or line.strip().startswith('## '):
                continue
            # è·³è¿‡æ£€æµ‹è¯­è¨€ç­‰å…ƒä¿¡æ¯è¡Œ
            if line.strip().startswith('**æ£€æµ‹è¯­è¨€:**') or line.strip().startswith('**è¯­è¨€æ¦‚ç‡:**'):
                continue
            # ä¿ç•™éç©ºæ–‡æœ¬è¡Œ
            if line.strip():
                cleaned_lines.append(line.strip())
        
        # å°†å¥å­é‡æ–°ç»„åˆå¹¶æ™ºèƒ½åˆ†æ®µ
        text = ' '.join(cleaned_lines)
        
        # æ›´æ™ºèƒ½çš„åˆ†å¥å¤„ç†ï¼Œè€ƒè™‘ä¸­è‹±æ–‡å·®å¼‚
        import re
        
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å¥
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        paragraphs = []
        current_paragraph = []
        
        for i, sentence in enumerate(sentences):
            if sentence:
                current_paragraph.append(sentence)
                
                # æ™ºèƒ½åˆ†æ®µæ¡ä»¶ï¼š
                # 1. æ¯3ä¸ªå¥å­ä¸€æ®µï¼ˆåŸºæœ¬è§„åˆ™ï¼‰
                # 2. é‡åˆ°è¯é¢˜è½¬æ¢è¯æ±‡æ—¶å¼ºåˆ¶åˆ†æ®µ
                # 3. é¿å…è¶…é•¿æ®µè½
                topic_change_keywords = [
                    'é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å', 'æ¥ä¸‹æ¥', 'å¦å¤–', 'æ­¤å¤–', 'æœ€å', 'æ€»ä¹‹',
                    'first', 'second', 'third', 'next', 'also', 'however', 'finally',
                    'ç°åœ¨', 'é‚£ä¹ˆ', 'æ‰€ä»¥', 'å› æ­¤', 'ä½†æ˜¯', 'ç„¶è€Œ',
                    'now', 'so', 'therefore', 'but', 'however'
                ]
                
                should_break = False
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ®µ
                if len(current_paragraph) >= 3:  # åŸºæœ¬é•¿åº¦æ¡ä»¶
                    should_break = True
                elif len(current_paragraph) >= 2:  # è¾ƒçŸ­ä½†é‡åˆ°è¯é¢˜è½¬æ¢
                    for keyword in topic_change_keywords:
                        if sentence.lower().startswith(keyword.lower()):
                            should_break = True
                            break
                
                if should_break or len(current_paragraph) >= 4:  # æœ€å¤§é•¿åº¦é™åˆ¶
                    # ç»„åˆå½“å‰æ®µè½
                    paragraph_text = '. '.join(current_paragraph)
                    if not paragraph_text.endswith('.'):
                        paragraph_text += '.'
                    paragraphs.append(paragraph_text)
                    current_paragraph = []
        
        # æ·»åŠ å‰©ä½™çš„å¥å­
        if current_paragraph:
            paragraph_text = '. '.join(current_paragraph)
            if not paragraph_text.endswith('.'):
                paragraph_text += '.'
            paragraphs.append(paragraph_text)
        
        return '\n\n'.join(paragraphs)

    async def _final_paragraph_organization(self, text: str, lang_instruction: str) -> str:
        """
        å¯¹åˆå¹¶åçš„æ–‡æœ¬è¿›è¡Œæœ€ç»ˆçš„æ®µè½æ•´ç†
        ä½¿ç”¨æ”¹è¿›çš„promptå’Œå·¥ç¨‹éªŒè¯
        """
        try:
            # ä¼°ç®—æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœå¤ªé•¿åˆ™åˆ†å—å¤„ç†
            estimated_tokens = self._estimate_tokens(text)
            if estimated_tokens > 15000:  # å¯¹äºå¾ˆé•¿çš„æ–‡æœ¬ï¼Œåˆ†å—å¤„ç†
                return await self._organize_long_text_paragraphs(text, lang_instruction)
            
            system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„{lang_instruction}æ–‡æœ¬æ®µè½æ•´ç†ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æŒ‰ç…§è¯­ä¹‰å’Œé€»è¾‘é‡æ–°ç»„ç»‡æ®µè½ã€‚

ğŸ¯ **æ ¸å¿ƒåŸåˆ™**ï¼š
1. **ä¸¥æ ¼ä¿æŒåŸå§‹è¯­è¨€({lang_instruction})**ï¼Œç»ä¸ç¿»è¯‘
2. **ä¿æŒæ‰€æœ‰å†…å®¹å®Œæ•´**ï¼Œä¸åˆ é™¤ä¸æ·»åŠ ä»»ä½•ä¿¡æ¯
3. **æŒ‰è¯­ä¹‰é€»è¾‘åˆ†æ®µ**ï¼šæ¯æ®µå›´ç»•ä¸€ä¸ªå®Œæ•´çš„æ€æƒ³æˆ–è¯é¢˜
4. **ä¸¥æ ¼æ§åˆ¶æ®µè½é•¿åº¦**ï¼šæ¯æ®µç»ä¸è¶…è¿‡250è¯
5. **ä¿æŒè‡ªç„¶æµç•…**ï¼šæ®µè½é—´åº”æœ‰é€»è¾‘è¿æ¥

ğŸ“ **åˆ†æ®µæ ‡å‡†**ï¼š
- **è¯­ä¹‰å®Œæ•´æ€§**ï¼šæ¯æ®µè®²è¿°ä¸€ä¸ªå®Œæ•´æ¦‚å¿µæˆ–äº‹ä»¶
- **é€‚ä¸­é•¿åº¦**ï¼š3-7ä¸ªå¥å­ï¼Œæ¯æ®µç»ä¸è¶…è¿‡250è¯
- **é€»è¾‘è¾¹ç•Œ**ï¼šåœ¨è¯é¢˜è½¬æ¢ã€æ—¶é—´è½¬æ¢ã€è§‚ç‚¹è½¬æ¢å¤„åˆ†æ®µ
- **è‡ªç„¶æ–­ç‚¹**ï¼šéµå¾ªè¯´è¯è€…çš„è‡ªç„¶åœé¡¿å’Œé€»è¾‘

âš ï¸ **ä¸¥ç¦**ï¼š
- åˆ›é€ è¶…è¿‡250è¯çš„å·¨å‹æ®µè½
- å¼ºè¡Œåˆå¹¶ä¸ç›¸å…³çš„å†…å®¹
- æ‰“æ–­å®Œæ•´çš„æ•…äº‹æˆ–è®ºè¿°

è¾“å‡ºæ ¼å¼ï¼šæ®µè½é—´ç”¨ç©ºè¡Œåˆ†éš”ã€‚"""

            user_prompt = f"""è¯·é‡æ–°æ•´ç†ä»¥ä¸‹{lang_instruction}æ–‡æœ¬çš„æ®µè½ç»“æ„ã€‚ä¸¥æ ¼æŒ‰ç…§è¯­ä¹‰å’Œé€»è¾‘è¿›è¡Œåˆ†æ®µï¼Œç¡®ä¿æ¯æ®µä¸è¶…è¿‡200è¯ï¼š

{text}

é‡æ–°åˆ†æ®µåçš„æ–‡æœ¬ï¼š"""

            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=4000,  # å¢åŠ tokené™åˆ¶
                temperature=0.05  # é™ä½æ¸©åº¦ï¼Œæé«˜ä¸€è‡´æ€§
            )
            
            organized_text = response.choices[0].message.content
            
            # å·¥ç¨‹éªŒè¯ï¼šæ£€æŸ¥æ®µè½é•¿åº¦
            validated_text = self._validate_paragraph_lengths(organized_text)
            
            return validated_text
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆæ®µè½æ•´ç†å¤±è´¥: {e}")
            # å¤±è´¥æ—¶ä½¿ç”¨åŸºç¡€åˆ†æ®µå¤„ç†
            return self._basic_paragraph_fallback(text)

    async def _organize_long_text_paragraphs(self, text: str, lang_instruction: str) -> str:
        """
        å¯¹äºå¾ˆé•¿çš„æ–‡æœ¬ï¼Œåˆ†å—è¿›è¡Œæ®µè½æ•´ç†
        """
        try:
            # æŒ‰ç°æœ‰æ®µè½åˆ†å‰²
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            organized_chunks = []
            
            current_chunk = []
            current_tokens = 0
            max_chunk_tokens = 12000  # ä¿å®ˆçš„chunkå¤§å°
            
            for para in paragraphs:
                para_tokens = self._estimate_tokens(para)
                
                if current_tokens + para_tokens > max_chunk_tokens and current_chunk:
                    # å¤„ç†å½“å‰chunk
                    chunk_text = '\n\n'.join(current_chunk)
                    organized_chunk = await self._organize_single_chunk(chunk_text, lang_instruction)
                    organized_chunks.append(organized_chunk)
                    
                    current_chunk = [para]
                    current_tokens = para_tokens
                else:
                    current_chunk.append(para)
                    current_tokens += para_tokens
            
            # å¤„ç†æœ€åä¸€ä¸ªchunk
            if current_chunk:
                chunk_text = '\n\n'.join(current_chunk)
                organized_chunk = await self._organize_single_chunk(chunk_text, lang_instruction)
                organized_chunks.append(organized_chunk)
            
            return '\n\n'.join(organized_chunks)
            
        except Exception as e:
            logger.error(f"é•¿æ–‡æœ¬æ®µè½æ•´ç†å¤±è´¥: {e}")
            return self._basic_paragraph_fallback(text)

    async def _organize_single_chunk(self, text: str, lang_instruction: str) -> str:
        """
        æ•´ç†å•ä¸ªæ–‡æœ¬å—çš„æ®µè½
        """
        system_prompt = f"""ä½ æ˜¯{lang_instruction}æ®µè½æ•´ç†ä¸“å®¶ã€‚æŒ‰è¯­ä¹‰é‡æ–°ç»„ç»‡æ®µè½ï¼Œç¡®ä¿æ¯æ®µä¸è¶…è¿‡200è¯ã€‚

æ ¸å¿ƒè¦æ±‚ï¼š
1. ä¸¥æ ¼ä¿æŒ{lang_instruction}åŸè¯­è¨€
2. æŒ‰è¯­ä¹‰é€»è¾‘åˆ†æ®µï¼Œæ¯æ®µä¸€ä¸ªä¸»é¢˜
3. æ¯æ®µç»ä¸è¶…è¿‡250è¯
4. æ®µè½é—´ç©ºè¡Œåˆ†éš”
5. ä¿æŒå†…å®¹å®Œæ•´ï¼Œä¸åˆ å‡ä¿¡æ¯"""

        user_prompt = f"""é‡æ–°åˆ†æ®µä»¥ä¸‹æ–‡æœ¬ï¼Œç¡®ä¿æ¯æ®µä¸è¶…è¿‡200è¯ï¼š

{text}"""

        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=3000,
            temperature=0.05
        )
        
        return response.choices[0].message.content

    def _validate_paragraph_lengths(self, text: str) -> str:
        """
        éªŒè¯æ®µè½é•¿åº¦ï¼Œå¦‚æœæœ‰è¶…é•¿æ®µè½åˆ™å°è¯•åˆ†å‰²
        """
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        validated_paragraphs = []
        
        for para in paragraphs:
            word_count = len(para.split())
            
            if word_count > 300:  # å¦‚æœæ®µè½è¶…è¿‡300è¯
                logger.warning(f"æ£€æµ‹åˆ°è¶…é•¿æ®µè½({word_count}è¯)ï¼Œå°è¯•åˆ†å‰²")
                # å°è¯•æŒ‰å¥å­åˆ†å‰²é•¿æ®µè½
                split_paras = self._split_long_paragraph(para)
                validated_paragraphs.extend(split_paras)
            else:
                validated_paragraphs.append(para)
        
        return '\n\n'.join(validated_paragraphs)

    def _split_long_paragraph(self, paragraph: str) -> list:
        """
        åˆ†å‰²è¿‡é•¿çš„æ®µè½
        """
        import re
        
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', paragraph)
        sentences = [s.strip() + '.' for s in sentences if s.strip()]
        
        split_paragraphs = []
        current_para = []
        current_words = 0
        
        for sentence in sentences:
            sentence_words = len(sentence.split())
            
            if current_words + sentence_words > 200 and current_para:
                # å½“å‰æ®µè½è¾¾åˆ°é•¿åº¦é™åˆ¶
                split_paragraphs.append(' '.join(current_para))
                current_para = [sentence]
                current_words = sentence_words
            else:
                current_para.append(sentence)
                current_words += sentence_words
        
        # æ·»åŠ æœ€åä¸€æ®µ
        if current_para:
            split_paragraphs.append(' '.join(current_para))
        
        return split_paragraphs

    def _basic_paragraph_fallback(self, text: str) -> str:
        """
        åŸºç¡€åˆ†æ®µfallbackæœºåˆ¶
        å½“GPTæ•´ç†å¤±è´¥æ—¶ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™åˆ†æ®µ
        """
        import re
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        basic_paragraphs = []
        
        for para in paragraphs:
            word_count = len(para.split())
            
            if word_count > 250:
                # é•¿æ®µè½æŒ‰å¥å­åˆ†å‰²
                split_paras = self._split_long_paragraph(para)
                basic_paragraphs.extend(split_paras)
            elif word_count < 30 and basic_paragraphs:
                # çŸ­æ®µè½ä¸ä¸Šä¸€æ®µåˆå¹¶ï¼ˆå¦‚æœåˆå¹¶åä¸è¶…è¿‡200è¯ï¼‰
                last_para = basic_paragraphs[-1]
                combined_words = len(last_para.split()) + word_count
                
                if combined_words <= 200:
                    basic_paragraphs[-1] = last_para + ' ' + para
                else:
                    basic_paragraphs.append(para)
            else:
                basic_paragraphs.append(para)
        
        return '\n\n'.join(basic_paragraphs)

    async def summarize(self, transcript: str, target_language: str = "zh", video_title: str = None) -> str:
        """
        ç”Ÿæˆè§†é¢‘è½¬å½•çš„æ‘˜è¦
        
        Args:
            transcript: è½¬å½•æ–‡æœ¬
            target_language: ç›®æ ‡è¯­è¨€ä»£ç 
            
        Returns:
            æ‘˜è¦æ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        try:
            if not self.client:
                logger.warning("OpenAI APIä¸å¯ç”¨ï¼Œç”Ÿæˆå¤‡ç”¨æ‘˜è¦")
                return self._generate_fallback_summary(transcript, target_language, video_title)
            
            # ä¼°ç®—è½¬å½•æ–‡æœ¬é•¿åº¦ï¼Œå†³å®šæ˜¯å¦éœ€è¦åˆ†å—æ‘˜è¦
            estimated_tokens = self._estimate_tokens(transcript)
            max_summarize_tokens = 60000  # æ‘˜è¦è¾“å…¥tokené™åˆ¶æ›´ä¿å®ˆ
            
            if estimated_tokens <= max_summarize_tokens:
                # çŸ­æ–‡æœ¬ç›´æ¥æ‘˜è¦
                return await self._summarize_single_text(transcript, target_language, video_title)
            else:
                # é•¿æ–‡æœ¬åˆ†å—æ‘˜è¦
                logger.info(f"æ–‡æœ¬è¾ƒé•¿({estimated_tokens} tokens)ï¼Œå¯ç”¨åˆ†å—æ‘˜è¦")
                return await self._summarize_with_chunks(transcript, target_language, video_title, max_summarize_tokens)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {str(e)}")
            return self._generate_fallback_summary(transcript, target_language, video_title)

    async def _summarize_single_text(self, transcript: str, target_language: str, video_title: str = None) -> str:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œæ‘˜è¦
        """
        # è·å–ç›®æ ‡è¯­è¨€åç§°
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        
        # æ„å»ºæç¤ºè¯
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ€»ç»“ä¸“å®¶ã€‚è¯·ä¸ºè§†é¢‘å†…å®¹ç”Ÿæˆä¸€ä¸ªè¿è´¯ã€æ·±åº¦çš„æ€»ç»“ã€‚

è¦æ±‚ï¼š
1. ä½¿ç”¨{language_name}è¿›è¡Œæ€»ç»“
2. ç”Ÿæˆè¿è´¯çš„æ®µè½å¼æ€»ç»“ï¼Œé¿å…ç®€å•çš„è¦ç‚¹ç½—åˆ—
3. å°†ç›¸å…³è§‚ç‚¹æ•´åˆåœ¨ä¸€èµ·ï¼Œå½¢æˆå®Œæ•´çš„è®ºè¿°
4. ä¿æŒé€»è¾‘å±‚æ¬¡ï¼Œä»æ•´ä½“åˆ°å…·ä½“
5. çªå‡ºæ ¸å¿ƒæ€æƒ³å’Œå…³é”®æ´å¯Ÿ
6. è¯­è¨€æµç•…è‡ªç„¶ï¼Œè¯»èµ·æ¥åƒä¸€ç¯‡å®Œæ•´çš„æ–‡ç« 
7. é€‚å½“ä½¿ç”¨è¿‡æ¸¡å¥è¿æ¥ä¸åŒéƒ¨åˆ†

æ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨è‡ªç„¶çš„æ®µè½ç»“æ„
- æ¯æ®µä¸“æ³¨ä¸€ä¸ªä¸»è¦æ–¹é¢
- æ®µè½é—´æœ‰é€»è¾‘é€’è¿›å…³ç³»
- é¿å…ç®€å•çš„åˆ—è¡¨å’Œè¦ç‚¹
- æ•´ä½“åº”è¯¥æ˜¯ä¸€ä¸ªå®Œæ•´ã€è¿è´¯çš„å™è¿°"""

        user_prompt = f"""è¯·ä¸ºä»¥ä¸‹è§†é¢‘å†…å®¹ç”Ÿæˆä¸€ä¸ªè¿è´¯çš„{language_name}æ€»ç»“ï¼š

{transcript}

è¦æ±‚ï¼šå†™æˆæµç•…çš„æ®µè½æ–‡ç« ï¼Œæ•´åˆç›¸å…³è§‚ç‚¹ï¼Œé¿å…ç®€å•ç½—åˆ—ï¼Œçªå‡ºæ ¸å¿ƒæ€æƒ³å’Œé€»è¾‘è„‰ç»œã€‚"""

        logger.info(f"æ­£åœ¨ç”Ÿæˆ{language_name}æ‘˜è¦...")
        
        # è°ƒç”¨OpenAI API
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=3000,
            temperature=0.3
        )
        
        summary = response.choices[0].message.content
        
        # æ·»åŠ å…ƒä¿¡æ¯å’Œæ ‡é¢˜
        return self._format_summary_with_meta(summary, target_language, video_title)

    async def _summarize_with_chunks(self, transcript: str, target_language: str, video_title: str, max_tokens: int) -> str:
        """
        åˆ†å—æ‘˜è¦é•¿æ–‡æœ¬
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        
        # å°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºå—
        chunks = self._split_into_chunks(transcript, max_tokens)
        logger.info(f"åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œæ‘˜è¦")
        
        chunk_summaries = []
        
        # æ¯å—ç”Ÿæˆå±€éƒ¨æ‘˜è¦
        for i, chunk in enumerate(chunks):
            logger.info(f"æ­£åœ¨æ‘˜è¦ç¬¬ {i+1}/{len(chunks)} å—...")
            
            system_prompt = f"""ä½ æ˜¯æ‘˜è¦ä¸“å®¶ã€‚è¯·ä¸ºè¿™æ®µæ–‡æœ¬ç”Ÿæˆç®€æ´çš„{language_name}æ‘˜è¦ã€‚

è¿™æ˜¯å®Œæ•´å†…å®¹çš„ç¬¬{i+1}éƒ¨åˆ†ï¼Œå…±{len(chunks)}éƒ¨åˆ†ã€‚

è¦æ±‚ï¼š
1. ä½¿ç”¨{language_name}
2. æŠ“ä½æœ¬æ®µçš„æ ¸å¿ƒè¦ç‚¹
3. ä¿æŒç®€æ´ä½†å®Œæ•´
4. ä¸ºåç»­åˆå¹¶åšå‡†å¤‡"""

            user_prompt = f"""æ‘˜è¦ä»¥ä¸‹æ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹ï¼š

{chunk}

è¾“å‡ºç®€æ´çš„{language_name}æ‘˜è¦ã€‚"""

            try:
                response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=800,
                    temperature=0.3
                )
                
                chunk_summary = response.choices[0].message.content
                chunk_summaries.append(chunk_summary)
                
            except Exception as e:
                logger.error(f"æ‘˜è¦ç¬¬ {i+1} å—å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ç”Ÿæˆç®€å•æ‘˜è¦
                simple_summary = f"ç¬¬{i+1}éƒ¨åˆ†å†…å®¹æ¦‚è¿°ï¼š" + chunk[:200] + "..."
                chunk_summaries.append(simple_summary)
        
        # åˆå¹¶æ‰€æœ‰å±€éƒ¨æ‘˜è¦
        combined_summaries = "\n\n".join(chunk_summaries)
        
        # å¯¹åˆå¹¶çš„æ‘˜è¦è¿›è¡Œæœ€ç»ˆæ•´åˆ
        logger.info("æ­£åœ¨æ•´åˆæœ€ç»ˆæ‘˜è¦...")
        final_summary = await self._integrate_chunk_summaries(combined_summaries, target_language)
        
        # æ·»åŠ å…ƒä¿¡æ¯å’Œæ ‡é¢˜
        return self._format_summary_with_meta(final_summary, target_language, video_title)

    async def _integrate_chunk_summaries(self, combined_summaries: str, target_language: str) -> str:
        """
        æ•´åˆåˆ†å—æ‘˜è¦ä¸ºæœ€ç»ˆè¿è´¯æ‘˜è¦
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        
        try:
            system_prompt = f"""ä½ æ˜¯å†…å®¹æ•´åˆä¸“å®¶ã€‚è¯·å°†å¤šä¸ªåˆ†æ®µæ‘˜è¦æ•´åˆä¸ºä¸€ä¸ªè¿è´¯å®Œæ•´çš„{language_name}æ€»ç»“ã€‚

è¦æ±‚ï¼š
1. æ•´åˆæ‰€æœ‰è¦ç‚¹ï¼Œæ¶ˆé™¤é‡å¤
2. å»ºç«‹é€»è¾‘è”ç³»å’Œå±‚æ¬¡ç»“æ„
3. å†™æˆè¿è´¯çš„æ®µè½å¼æ–‡ç« 
4. çªå‡ºæ ¸å¿ƒä¸»é¢˜å’Œå…³é”®è§‚ç‚¹
5. è¯­è¨€æµç•…è‡ªç„¶ï¼Œé¿å…ç”Ÿç¡¬æ‹¼æ¥"""

            user_prompt = f"""è¯·å°†ä»¥ä¸‹åˆ†æ®µæ‘˜è¦æ•´åˆä¸ºä¸€ç¯‡è¿è´¯çš„{language_name}æ€»ç»“æ–‡ç« ï¼š

{combined_summaries}

è¦æ±‚ï¼šæ•´åˆæ‰€æœ‰å†…å®¹ï¼Œå½¢æˆé€»è¾‘æ¸…æ™°ã€è¯­è¨€æµç•…çš„å®Œæ•´æ–‡ç« ã€‚"""

            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=2000,
                temperature=0.3
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"æ•´åˆæ‘˜è¦å¤±è´¥: {e}")
            # å¤±è´¥æ—¶ç›´æ¥åˆå¹¶
            return combined_summaries

    def _format_summary_with_meta(self, summary: str, target_language: str, video_title: str = None) -> str:
        """
        ä¸ºæ‘˜è¦æ·»åŠ æ ‡é¢˜å’Œå…ƒä¿¡æ¯
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        meta_labels = self._get_summary_labels(target_language)
        
        # ç›´æ¥ä½¿ç”¨è§†é¢‘æ ‡é¢˜ä½œä¸ºä¸»æ ‡é¢˜
        title = video_title if video_title else "Summary"
        
        summary_with_meta = f"""# {title}

**{meta_labels['language_label']}:** {language_name}


{summary}


<br/>

<p style="color: #888; font-style: italic; text-align: center; margin-top: 24px;"><em>{meta_labels['disclaimer']}</em></p>"""
        
        return summary_with_meta
    
    def _generate_fallback_summary(self, transcript: str, target_language: str, video_title: str = None) -> str:
        """
        ç”Ÿæˆå¤‡ç”¨æ‘˜è¦ï¼ˆå½“OpenAI APIä¸å¯ç”¨æ—¶ï¼‰
        
        Args:
            transcript: è½¬å½•æ–‡æœ¬
            video_title: è§†é¢‘æ ‡é¢˜
            target_language: ç›®æ ‡è¯­è¨€ä»£ç 
            
        Returns:
            å¤‡ç”¨æ‘˜è¦æ–‡æœ¬
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        
        # ç®€å•çš„æ–‡æœ¬å¤„ç†ï¼Œæå–å…³é”®ä¿¡æ¯
        lines = transcript.split('\n')
        content_lines = [line for line in lines if line.strip() and not line.startswith('#') and not line.startswith('**')]
        
        # è®¡ç®—å¤§æ¦‚çš„é•¿åº¦
        total_chars = sum(len(line) for line in content_lines)
        
        # ä½¿ç”¨ç›®æ ‡è¯­è¨€çš„æ ‡ç­¾
        meta_labels = self._get_summary_labels(target_language)
        fallback_labels = self._get_fallback_labels(target_language)
        
        # ç›´æ¥ä½¿ç”¨è§†é¢‘æ ‡é¢˜ä½œä¸ºä¸»æ ‡é¢˜  
        title = video_title if video_title else "Summary"
        
        summary = f"""# {title}

**{meta_labels['language_label']}:** {language_name}
**{fallback_labels['notice']}:** {fallback_labels['api_unavailable']}



## {fallback_labels['overview_title']}

**{fallback_labels['content_length']}:** {fallback_labels['about']} {total_chars} {fallback_labels['characters']}
**{fallback_labels['paragraph_count']}:** {len(content_lines)} {fallback_labels['paragraphs']}

## {fallback_labels['main_content']}

{fallback_labels['content_description']}

{fallback_labels['suggestions_intro']}

1. {fallback_labels['suggestion_1']}
2. {fallback_labels['suggestion_2']}
3. {fallback_labels['suggestion_3']}

## {fallback_labels['recommendations']}

- {fallback_labels['recommendation_1']}
- {fallback_labels['recommendation_2']}


<br/>

<p style="color: #888; font-style: italic; text-align: center; margin-top: 16px;"><em>{fallback_labels['fallback_disclaimer']}</em></p>"""
        
        return summary
    
    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def get_supported_languages(self) -> dict:
        """
        è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
        
        Returns:
            è¯­è¨€ä»£ç åˆ°è¯­è¨€åç§°çš„æ˜ å°„
        """
        return self.language_map.copy()
    
    def _detect_transcript_language(self, transcript: str) -> str:
        """
        æ£€æµ‹è½¬å½•æ–‡æœ¬çš„ä¸»è¦è¯­è¨€
        
        Args:
            transcript: è½¬å½•æ–‡æœ¬
            
        Returns:
            æ£€æµ‹åˆ°çš„è¯­è¨€ä»£ç 
        """
        # ç®€å•çš„è¯­è¨€æ£€æµ‹é€»è¾‘ï¼šæŸ¥æ‰¾è½¬å½•æ–‡æœ¬ä¸­çš„è¯­è¨€æ ‡è®°
        if "**æ£€æµ‹è¯­è¨€:**" in transcript:
            # ä»Whisperè½¬å½•ä¸­æå–æ£€æµ‹åˆ°çš„è¯­è¨€
            lines = transcript.split('\n')
            for line in lines:
                if "**æ£€æµ‹è¯­è¨€:**" in line:
                    # æå–è¯­è¨€ä»£ç ï¼Œä¾‹å¦‚: "**æ£€æµ‹è¯­è¨€:** en"
                    lang = line.split(":")[-1].strip()
                    return lang
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯­è¨€æ ‡è®°ï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦æ£€æµ‹
        # è®¡ç®—è‹±æ–‡å­—ç¬¦ã€ä¸­æ–‡å­—ç¬¦ç­‰çš„æ¯”ä¾‹
        total_chars = len(transcript)
        if total_chars == 0:
            return "en"  # é»˜è®¤è‹±æ–‡
            
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = sum(1 for char in transcript if '\u4e00' <= char <= '\u9fff')
        chinese_ratio = chinese_chars / total_chars
        
        # ç»Ÿè®¡è‹±æ–‡å­—æ¯
        english_chars = sum(1 for char in transcript if char.isascii() and char.isalpha())
        english_ratio = english_chars / total_chars
        
        # æ ¹æ®æ¯”ä¾‹åˆ¤æ–­
        if chinese_ratio > 0.3:
            return "zh"
        elif english_ratio > 0.3:
            return "en"
        else:
            return "en"  # é»˜è®¤è‹±æ–‡
    
    def _get_language_instruction(self, lang_code: str) -> str:
        """
        æ ¹æ®è¯­è¨€ä»£ç è·å–ä¼˜åŒ–æŒ‡ä»¤ä¸­ä½¿ç”¨çš„è¯­è¨€åç§°
        
        Args:
            lang_code: è¯­è¨€ä»£ç 
            
        Returns:
            è¯­è¨€åç§°
        """
        language_instructions = {
            "en": "English",
            "zh": "ä¸­æ–‡",
            "ja": "æ—¥æœ¬èª",
            "ko": "í•œêµ­ì–´",
            "es": "EspaÃ±ol",
            "fr": "FranÃ§ais",
            "de": "Deutsch",
            "it": "Italiano",
            "pt": "PortuguÃªs",
            "ru": "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
            "ar": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
        }
        return language_instructions.get(lang_code, "English")
    

    def _get_summary_labels(self, lang_code: str) -> dict:
        """
        è·å–æ‘˜è¦é¡µé¢çš„å¤šè¯­è¨€æ ‡ç­¾
        
        Args:
            lang_code: è¯­è¨€ä»£ç 
            
        Returns:
            æ ‡ç­¾å­—å…¸
        """
        labels = {
            "en": {
                "language_label": "Summary Language",
                "disclaimer": "This summary is automatically generated by AI for reference only"
            },
            "zh": {
                "language_label": "æ‘˜è¦è¯­è¨€",
                "disclaimer": "æœ¬æ‘˜è¦ç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ"
            },
            "ja": {
                "language_label": "è¦ç´„è¨€èª",
                "disclaimer": "ã“ã®è¦ç´„ã¯AIã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¦ãŠã‚Šã€å‚è€ƒç”¨ã§ã™"
            },
            "ko": {
                "language_label": "ìš”ì•½ ì–¸ì–´",
                "disclaimer": "ì´ ìš”ì•½ì€ AIì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìœ¼ë©° ì°¸ê³ ìš©ì…ë‹ˆë‹¤"
            },
            "es": {
                "language_label": "Idioma del Resumen",
                "disclaimer": "Este resumen es generado automÃ¡ticamente por IA, solo para referencia"
            },
            "fr": {
                "language_label": "Langue du RÃ©sumÃ©",
                "disclaimer": "Ce rÃ©sumÃ© est gÃ©nÃ©rÃ© automatiquement par IA, Ã  titre de rÃ©fÃ©rence uniquement"
            },
            "de": {
                "language_label": "Zusammenfassungssprache",
                "disclaimer": "Diese Zusammenfassung wird automatisch von KI generiert, nur zur Referenz"
            },
            "it": {
                "language_label": "Lingua del Riassunto",
                "disclaimer": "Questo riassunto Ã¨ generato automaticamente dall'IA, solo per riferimento"
            },
            "pt": {
                "language_label": "Idioma do Resumo",
                "disclaimer": "Este resumo Ã© gerado automaticamente por IA, apenas para referÃªncia"
            },
            "ru": {
                "language_label": "Ğ¯Ğ·Ñ‹Ğº Ñ€ĞµĞ·ÑĞ¼Ğµ",
                "disclaimer": "Ğ­Ñ‚Ğ¾ Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ˜Ğ˜, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ¸"
            },
            "ar": {
                "language_label": "Ù„ØºØ© Ø§Ù„Ù…Ù„Ø®Øµ",
                "disclaimer": "Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ø®Øµ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ù„Ù„Ù…Ø±Ø¬Ø¹ ÙÙ‚Ø·"
            }
        }
        return labels.get(lang_code, labels["en"])
    
    def _get_fallback_labels(self, lang_code: str) -> dict:
        """
        è·å–å¤‡ç”¨æ‘˜è¦çš„å¤šè¯­è¨€æ ‡ç­¾
        
        Args:
            lang_code: è¯­è¨€ä»£ç 
            
        Returns:
            æ ‡ç­¾å­—å…¸
        """
        labels = {
            "en": {
                "notice": "Notice",
                "api_unavailable": "OpenAI API is unavailable, this is a simplified summary",
                "overview_title": "Transcript Overview",
                "content_length": "Content Length",
                "about": "About",
                "characters": "characters",
                "paragraph_count": "Paragraph Count",
                "paragraphs": "paragraphs",
                "main_content": "Main Content",
                "content_description": "The transcript contains complete video speech content. Since AI summary cannot be generated currently, we recommend:",
                "suggestions_intro": "For detailed information, we suggest you:",
                "suggestion_1": "Review the complete transcript text for detailed information",
                "suggestion_2": "Focus on important paragraphs marked with timestamps",
                "suggestion_3": "Manually extract key points and takeaways",
                "recommendations": "Recommendations",
                "recommendation_1": "Configure OpenAI API key for better summary functionality",
                "recommendation_2": "Or use other AI services for text summarization",
                "fallback_disclaimer": "This is an automatically generated fallback summary"
            },
            "zh": {
                "notice": "æ³¨æ„",
                "api_unavailable": "ç”±äºOpenAI APIä¸å¯ç”¨ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ‘˜è¦",
                "overview_title": "è½¬å½•æ¦‚è§ˆ",
                "content_length": "å†…å®¹é•¿åº¦",
                "about": "çº¦",
                "characters": "å­—ç¬¦",
                "paragraph_count": "æ®µè½æ•°é‡",
                "paragraphs": "æ®µ",
                "main_content": "ä¸»è¦å†…å®¹",
                "content_description": "è½¬å½•æ–‡æœ¬åŒ…å«äº†å®Œæ•´çš„è§†é¢‘è¯­éŸ³å†…å®¹ã€‚ç”±äºå½“å‰æ— æ³•ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ï¼Œå»ºè®®æ‚¨ï¼š",
                "suggestions_intro": "ä¸ºè·å–è¯¦ç»†ä¿¡æ¯ï¼Œå»ºè®®æ‚¨ï¼š",
                "suggestion_1": "æŸ¥çœ‹å®Œæ•´çš„è½¬å½•æ–‡æœ¬ä»¥è·å–è¯¦ç»†ä¿¡æ¯",
                "suggestion_2": "å…³æ³¨æ—¶é—´æˆ³æ ‡è®°çš„é‡è¦æ®µè½",
                "suggestion_3": "æ‰‹åŠ¨æå–å…³é”®è§‚ç‚¹å’Œè¦ç‚¹",
                "recommendations": "å»ºè®®",
                "recommendation_1": "é…ç½®OpenAI APIå¯†é’¥ä»¥è·å¾—æ›´å¥½çš„æ‘˜è¦åŠŸèƒ½",
                "recommendation_2": "æˆ–è€…ä½¿ç”¨å…¶ä»–AIæœåŠ¡è¿›è¡Œæ–‡æœ¬æ€»ç»“",
                "fallback_disclaimer": "æœ¬æ‘˜è¦ä¸ºè‡ªåŠ¨ç”Ÿæˆçš„å¤‡ç”¨ç‰ˆæœ¬"
            }
        }
        return labels.get(lang_code, labels["en"])
    
    def is_available(self) -> bool:
        """
        æ£€æŸ¥æ‘˜è¦æœåŠ¡æ˜¯å¦å¯ç”¨
        
        Returns:
            True if OpenAI API is configured, False otherwise
        """
        return self.client is not None
