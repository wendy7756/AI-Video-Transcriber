import os
import openai
import logging
from typing import Optional

logger = logging.getLogger(__name__)

class Summarizer:
    """æ–‡æœ¬æ€»ç»“å™¨ï¼Œä½¿ç”¨OpenAI APIç”Ÿæˆå¤šè¯­è¨€æ‘˜è¦"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ€»ç»“å™¨"""
        # ä»ç¯å¢ƒå˜é‡è·å–OpenAI APIé…ç½®
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_BASE_URL")
        
        if not api_key:
            logger.warning("æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†æ— æ³•ä½¿ç”¨æ‘˜è¦åŠŸèƒ½")
        
        if api_key:
            if base_url:
                self.client = openai.OpenAI(api_key=api_key, base_url=base_url)
                logger.info(f"OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨è‡ªå®šä¹‰ç«¯ç‚¹: {base_url}")
            else:
                self.client = openai.OpenAI(api_key=api_key)
                logger.info("OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨é»˜è®¤ç«¯ç‚¹")
        else:
            self.client = None
        
        # æ”¯æŒçš„è¯­è¨€æ˜ å°„
        self.language_map = {
            "en": "English",
            "zh": "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰",
            "es": "EspaÃ±ol",
            "fr": "FranÃ§ais", 
            "de": "Deutsch",
            "it": "Italiano",
            "pt": "PortuguÃªs",
            "ru": "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
            "ja": "æ—¥æœ¬èª",
            "ko": "í•œêµ­ì–´",
            "ar": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
        }
    
    async def optimize_transcript(self, raw_transcript: str) -> str:
        """
        ä¼˜åŒ–è½¬å½•æ–‡æœ¬ï¼šä¿®æ­£é”™åˆ«å­—ï¼ŒæŒ‰å«ä¹‰åˆ†æ®µ
        æ”¯æŒé•¿æ–‡æœ¬è‡ªåŠ¨åˆ†å—å¤„ç†
        
        Args:
            raw_transcript: åŸå§‹è½¬å½•æ–‡æœ¬
            
        Returns:
            ä¼˜åŒ–åçš„è½¬å½•æ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        try:
            if not self.client:
                logger.warning("OpenAI APIä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹è½¬å½•")
                return raw_transcript

            # é¢„å¤„ç†ï¼šä»…ç§»é™¤æ—¶é—´æˆ³ä¸å…ƒä¿¡æ¯ï¼Œä¿ç•™å…¨éƒ¨å£è¯­/é‡å¤å†…å®¹
            preprocessed = self._remove_timestamps_and_meta(raw_transcript)
            # ä½¿ç”¨JSç­–ç•¥ï¼šæŒ‰å­—ç¬¦é•¿åº¦åˆ†å—ï¼ˆæ›´è´´è¿‘tokensä¸Šé™ï¼Œé¿å…ä¼°ç®—è¯¯å·®ï¼‰
            detected_lang_code = self._detect_transcript_language(preprocessed)
            max_chars_per_chunk = 4000  # å¯¹é½JSï¼šæ¯å—æœ€å¤§çº¦4000å­—ç¬¦

            if len(preprocessed) > max_chars_per_chunk:
                logger.info(f"æ–‡æœ¬è¾ƒé•¿({len(preprocessed)} chars)ï¼Œå¯ç”¨åˆ†å—ä¼˜åŒ–")
                return await self._format_long_transcript_in_chunks(preprocessed, detected_lang_code, max_chars_per_chunk)
            else:
                return await self._format_single_chunk(preprocessed, detected_lang_code)

        except Exception as e:
            logger.error(f"ä¼˜åŒ–è½¬å½•æ–‡æœ¬å¤±è´¥: {str(e)}")
            logger.info("è¿”å›åŸå§‹è½¬å½•æ–‡æœ¬")
            return raw_transcript

    def _estimate_tokens(self, text: str) -> int:
        """
        æ”¹è¿›çš„tokenæ•°é‡ä¼°ç®—ç®—æ³•
        æ›´ä¿å®ˆçš„ä¼°ç®—ï¼Œè€ƒè™‘ç³»ç»Ÿpromptå’Œæ ¼å¼åŒ–å¼€é”€
        """
        # æ›´ä¿å®ˆçš„ä¼°ç®—ï¼šè€ƒè™‘å®é™…ä½¿ç”¨ä¸­çš„tokenè†¨èƒ€
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_words = len([word for word in text.split() if word.isascii() and word.isalpha()])
        
        # è®¡ç®—åŸºç¡€tokens
        base_tokens = chinese_chars * 1.5 + english_words * 1.3
        
        # è€ƒè™‘markdownæ ¼å¼ã€æ—¶é—´æˆ³ç­‰å¼€é”€ï¼ˆçº¦30%é¢å¤–å¼€é”€ï¼‰
        format_overhead = len(text) * 0.15
        
        # è€ƒè™‘ç³»ç»Ÿpromptå¼€é”€ï¼ˆçº¦2000-3000 tokensï¼‰
        system_prompt_overhead = 2500
        
        total_estimated = int(base_tokens + format_overhead + system_prompt_overhead)
        
        return total_estimated

    async def _optimize_single_chunk(self, raw_transcript: str) -> str:
        """
        ä¼˜åŒ–å•ä¸ªæ–‡æœ¬å—
        """
        detected_lang = self._detect_transcript_language(raw_transcript)
        lang_instruction = self._get_language_instruction(detected_lang)
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬ç¼–è¾‘ä¸“å®¶ã€‚è¯·å¯¹æä¾›çš„è§†é¢‘è½¬å½•æ–‡æœ¬è¿›è¡Œä¼˜åŒ–å¤„ç†ã€‚

ç‰¹åˆ«æ³¨æ„ï¼šè§†é¢‘è½¬å½•ä¸­ç»å¸¸å‡ºç°å®Œæ•´å¥å­è¢«æ—¶é—´æˆ³æ‹†åˆ†çš„æƒ…å†µï¼Œéœ€è¦é‡æ–°ç»„åˆã€‚

è¦æ±‚ï¼š
1. **ä¸¥æ ¼ä¿æŒåŸå§‹è¯­è¨€({lang_instruction})ï¼Œç»å¯¹ä¸è¦ç¿»è¯‘æˆå…¶ä»–è¯­è¨€**
2. **å®Œå…¨ç§»é™¤æ‰€æœ‰æ—¶é—´æˆ³æ ‡è®°ï¼ˆå¦‚ [00:00 - 00:05]ï¼‰**
3. **æ™ºèƒ½è¯†åˆ«å’Œé‡ç»„è¢«æ—¶é—´æˆ³æ‹†åˆ†çš„å®Œæ•´å¥å­**ï¼Œè¯­æ³•ä¸Šä¸å®Œæ•´çš„å¥å­ç‰‡æ®µéœ€è¦ä¸ä¸Šä¸‹æ–‡åˆå¹¶
4. ä¿®æ­£æ˜æ˜¾çš„é”™åˆ«å­—å’Œè¯­æ³•é”™è¯¯
5. å°†é‡ç»„åçš„å®Œæ•´å¥å­æŒ‰ç…§è¯­ä¹‰å’Œé€»è¾‘å«ä¹‰åˆ†æˆè‡ªç„¶çš„æ®µè½
6. æ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”
7. ä¿æŒåŸæ„ä¸å˜ï¼Œä¸è¦æ·»åŠ æˆ–åˆ é™¤å®é™…å†…å®¹
8. ç¡®ä¿æ¯ä¸ªå¥å­è¯­æ³•å®Œæ•´ï¼Œè¯­è¨€æµç•…è‡ªç„¶

å¤„ç†ç­–ç•¥ï¼š
- ä¼˜å…ˆè¯†åˆ«ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µï¼ˆå¦‚ä»¥ä»‹è¯ã€è¿è¯ã€å½¢å®¹è¯ç»“å°¾ï¼‰
- æŸ¥çœ‹ç›¸é‚»çš„æ–‡æœ¬ç‰‡æ®µï¼Œåˆå¹¶å½¢æˆå®Œæ•´å¥å­
- é‡æ–°æ–­å¥ï¼Œç¡®ä¿æ¯å¥è¯è¯­æ³•å®Œæ•´
- æŒ‰ä¸»é¢˜å’Œé€»è¾‘é‡æ–°åˆ†æ®µ

åˆ†æ®µè¦æ±‚ï¼š
- æ¯å½“è¯é¢˜è½¬æ¢æ—¶åˆ›å»ºæ–°æ®µè½
- å½“ä¸€ä¸ªæƒ³æ³•æˆ–è§‚ç‚¹å®Œæ•´è¡¨è¾¾ååˆ†æ®µ
- é¿å…è¶…é•¿æ®µè½ï¼ˆæ¯æ®µä¸è¦è¶…è¿‡250è¯ï¼‰

è¾“å‡ºæ ¼å¼ï¼š
- çº¯æ–‡æœ¬æ®µè½ï¼Œæ— æ—¶é—´æˆ³æˆ–æ ¼å¼æ ‡è®°
- æ¯ä¸ªå¥å­ç»“æ„å®Œæ•´
- æ¯ä¸ªæ®µè½è®¨è®ºä¸€ä¸ªä¸»è¦è¯é¢˜
- æ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”

é‡è¦æé†’ï¼šè¿™æ˜¯{lang_instruction}å†…å®¹ï¼Œè¯·å®Œå…¨ç”¨{lang_instruction}è¿›è¡Œä¼˜åŒ–ï¼Œé‡ç‚¹è§£å†³å¥å­è¢«æ—¶é—´æˆ³æ‹†åˆ†å¯¼è‡´çš„ä¸è¿è´¯é—®é¢˜ï¼åŠ¡å¿…è¿›è¡Œåˆç†çš„åˆ†æ®µï¼Œé¿å…å‡ºç°è¶…é•¿æ®µè½ï¼"""

        user_prompt = f"""è¯·å°†ä»¥ä¸‹{lang_instruction}è§†é¢‘è½¬å½•æ–‡æœ¬ä¼˜åŒ–ä¸ºæµç•…çš„æ®µè½æ–‡æœ¬ï¼š

{raw_transcript}

é‡ç‚¹ä»»åŠ¡ï¼š
1. ç§»é™¤æ‰€æœ‰æ—¶é—´æˆ³æ ‡è®°
2. è¯†åˆ«å¹¶é‡ç»„è¢«æ‹†åˆ†çš„å®Œæ•´å¥å­
3. ç¡®ä¿æ¯ä¸ªå¥å­è¯­æ³•å®Œæ•´ã€æ„æ€è¿è´¯
4. æŒ‰å«ä¹‰é‡æ–°åˆ†æ®µï¼Œæ®µè½é—´ç©ºè¡Œåˆ†éš”
5. ä¿æŒ{lang_instruction}è¯­è¨€ä¸å˜

åˆ†æ®µæŒ‡å¯¼ï¼š
- å½“è¯é¢˜è½¬æ¢æ—¶å¿…é¡»åˆ†æ®µ
- é¿å…ä¸€ä¸ªæ®µè½è¶…è¿‡250ä¸ªå•è¯
- ç¡®ä¿æ®µè½ä¹‹é—´æœ‰æ˜ç¡®çš„ç©ºè¡Œ

è¯·ç‰¹åˆ«æ³¨æ„ä¿®å¤å› æ—¶é—´æˆ³åˆ†å‰²å¯¼è‡´çš„å¥å­ä¸å®Œæ•´é—®é¢˜ï¼Œå¹¶è¿›è¡Œåˆç†çš„æ®µè½åˆ’åˆ†ï¼"""

        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=4000,  # å¯¹é½JSï¼šä¼˜åŒ–/æ ¼å¼åŒ–é˜¶æ®µæœ€å¤§tokensâ‰ˆ4000
            temperature=0.1
        )
        
        return response.choices[0].message.content

    async def _optimize_with_chunks(self, raw_transcript: str, max_tokens: int) -> str:
        """
        åˆ†å—ä¼˜åŒ–é•¿æ–‡æœ¬
        """
        detected_lang = self._detect_transcript_language(raw_transcript)
        lang_instruction = self._get_language_instruction(detected_lang)
        
        # æŒ‰æ®µè½åˆ†å‰²åŸå§‹è½¬å½•ï¼ˆä¿ç•™æ—¶é—´æˆ³ä½œä¸ºåˆ†å‰²å‚è€ƒï¼‰
        chunks = self._split_into_chunks(raw_transcript, max_tokens)
        logger.info(f"åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œå¤„ç†")
        
        optimized_chunks = []
        
        for i, chunk in enumerate(chunks):
            logger.info(f"æ­£åœ¨ä¼˜åŒ–ç¬¬ {i+1}/{len(chunks)} å—...")
            
            system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„æ–‡æœ¬ç¼–è¾‘ä¸“å®¶ã€‚è¯·å¯¹è¿™æ®µè½¬å½•æ–‡æœ¬ç‰‡æ®µè¿›è¡Œç®€å•ä¼˜åŒ–ã€‚

è¿™æ˜¯å®Œæ•´è½¬å½•çš„ç¬¬{i+1}éƒ¨åˆ†ï¼Œå…±{len(chunks)}éƒ¨åˆ†ã€‚

ç®€å•ä¼˜åŒ–è¦æ±‚ï¼š
1. **ä¸¥æ ¼ä¿æŒåŸå§‹è¯­è¨€({lang_instruction})**ï¼Œç»å¯¹ä¸ç¿»è¯‘
2. **ä»…ä¿®æ­£æ˜æ˜¾çš„é”™åˆ«å­—å’Œè¯­æ³•é”™è¯¯**
3. **ç¨å¾®è°ƒæ•´å¥å­æµç•…åº¦**ï¼Œä½†ä¸å¤§å¹…æ”¹å†™
4. **ä¿æŒåŸæ–‡ç»“æ„å’Œé•¿åº¦**ï¼Œä¸åšå¤æ‚çš„æ®µè½é‡ç»„
5. **ä¿æŒåŸæ„100%ä¸å˜**

æ³¨æ„ï¼šè¿™åªæ˜¯åˆæ­¥æ¸…ç†ï¼Œä¸è¦åšå¤æ‚çš„é‡å†™æˆ–é‡æ–°ç»„ç»‡ã€‚"""

            user_prompt = f"""ç®€å•ä¼˜åŒ–ä»¥ä¸‹{lang_instruction}æ–‡æœ¬ç‰‡æ®µï¼ˆä»…ä¿®é”™åˆ«å­—å’Œè¯­æ³•ï¼‰ï¼š

{chunk}

è¾“å‡ºæ¸…ç†åçš„æ–‡æœ¬ï¼Œä¿æŒåŸæ–‡ç»“æ„ã€‚"""

            try:
                response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=1200,  # é€‚åº”4000 tokensæ€»é™åˆ¶
                    temperature=0.1
                )
                
                optimized_chunk = response.choices[0].message.content
                optimized_chunks.append(optimized_chunk)
                
            except Exception as e:
                logger.error(f"ä¼˜åŒ–ç¬¬ {i+1} å—å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ä½¿ç”¨åŸºæœ¬æ¸…ç†
                cleaned_chunk = self._basic_transcript_cleanup(chunk)
                optimized_chunks.append(cleaned_chunk)
        
        # åˆå¹¶æ‰€æœ‰ä¼˜åŒ–åçš„å—
        merged_text = "\n\n".join(optimized_chunks)
        
        # å¯¹åˆå¹¶åçš„æ–‡æœ¬è¿›è¡ŒäºŒæ¬¡æ®µè½æ•´ç†
        logger.info("æ­£åœ¨è¿›è¡Œæœ€ç»ˆæ®µè½æ•´ç†...")
        final_result = await self._final_paragraph_organization(merged_text, lang_instruction)
        
        logger.info("åˆ†å—ä¼˜åŒ–å®Œæˆ")
        return final_result

    # ===== JS openaiService.js ç§»æ¤ï¼šåˆ†å—/ä¸Šä¸‹æ–‡/å»é‡/æ ¼å¼åŒ– =====

    def _ensure_markdown_paragraphs(self, text: str) -> str:
        """ç¡®ä¿Markdownæ®µè½ç©ºè¡Œã€æ ‡é¢˜åç©ºè¡Œã€å‹ç¼©å¤šä½™ç©ºè¡Œã€‚"""
        if not text:
            return text
        formatted = text.replace("\r\n", "\n")
        import re
        # æ ‡é¢˜ååŠ ç©ºè¡Œ
        formatted = re.sub(r"(^#{1,6}\s+.*)\n([^\n#])", r"\1\n\n\2", formatted, flags=re.M)
        # å‹ç¼©â‰¥3ä¸ªæ¢è¡Œä¸º2ä¸ª
        formatted = re.sub(r"\n{3,}", "\n\n", formatted)
        # å»é¦–å°¾ç©ºè¡Œ
        formatted = re.sub(r"^\n+", "", formatted)
        formatted = re.sub(r"\n+$", "", formatted)
        return formatted

    async def _format_single_chunk(self, chunk_text: str, transcript_language: str = 'zh') -> str:
        """å•å—ä¼˜åŒ–ï¼ˆä¿®æ­£+æ ¼å¼åŒ–ï¼‰ï¼Œéµå¾ª4000 tokens é™åˆ¶ã€‚"""
        # æ„å»ºä¸JSç‰ˆä¸€è‡´çš„ç³»ç»Ÿ/ç”¨æˆ·æç¤º
        if transcript_language == 'zh':
            prompt = (
                "è¯·å¯¹ä»¥ä¸‹éŸ³é¢‘è½¬å½•æ–‡æœ¬è¿›è¡Œæ™ºèƒ½ä¼˜åŒ–å’Œæ ¼å¼åŒ–ï¼Œè¦æ±‚ï¼š\n\n"
                "**å†…å®¹ä¼˜åŒ–ï¼ˆæ­£ç¡®æ€§ä¼˜å…ˆï¼‰ï¼š**\n"
                "1. é”™è¯¯ä¿®æ­£ï¼ˆè½¬å½•é”™è¯¯/é”™åˆ«å­—/åŒéŸ³å­—/ä¸“æœ‰åè¯ï¼‰\n"
                "2. é€‚åº¦æ”¹å–„è¯­æ³•ï¼Œè¡¥å…¨ä¸å®Œæ•´å¥å­ï¼Œä¿æŒåŸæ„å’Œè¯­è¨€ä¸å˜\n"
                "3. å£è¯­å¤„ç†ï¼šä¿ç•™è‡ªç„¶å£è¯­ä¸é‡å¤è¡¨è¾¾ï¼Œä¸è¦åˆ å‡å†…å®¹ï¼Œä»…æ·»åŠ å¿…è¦æ ‡ç‚¹\n\n"
                "**åˆ†æ®µè§„åˆ™ï¼š**\n"
                "- è¯é¢˜/ç¯èŠ‚/è¯´è¯äººå˜åŒ–æ—¶åˆ†æ®µï¼›å•æ®µä¸è¶…è¿‡250å­—ç¬¦\n\n"
                "**æ ¼å¼è¦æ±‚ï¼š**Markdown æ®µè½ï¼Œæ®µè½é—´ç©ºè¡Œ\n\n"
                f"åŸå§‹è½¬å½•æ–‡æœ¬ï¼š\n{chunk_text}"
            )
            system_prompt = (
                "ä½ æ˜¯ä¸“ä¸šçš„éŸ³é¢‘è½¬å½•æ–‡æœ¬ä¼˜åŒ–åŠ©æ‰‹ï¼Œä¿®æ­£é”™è¯¯ã€æ”¹å–„é€šé¡ºåº¦å’Œæ’ç‰ˆæ ¼å¼ï¼Œ"
                "å¿…é¡»ä¿æŒåŸæ„ï¼Œä¸å¾—åˆ å‡å£è¯­/é‡å¤/ç»†èŠ‚ï¼›ä»…ç§»é™¤æ—¶é—´æˆ³æˆ–å…ƒä¿¡æ¯ã€‚"
            )
        else:
            prompt = (
                "Please intelligently optimize and format the following audio transcript text:\n\n"
                "Content Optimization (Accuracy First):\n"
                "1. Error Correction (typos, homophones, proper nouns)\n"
                "2. Moderate grammar improvement, complete incomplete sentences, keep original language/meaning\n"
                "3. Speech processing: keep natural fillers and repetitions, do NOT remove content; only add punctuation if needed\n\n"
                "Segmentation Rules: split on topic/section/speaker change; each paragraph must NOT exceed 250 characters\n\n"
                "Format: Markdown paragraphs with blank lines between paragraphs\n\n"
                f"Original transcript text:\n{chunk_text}"
            )
            system_prompt = (
                "You are a professional transcript formatting assistant. Fix errors and improve fluency "
                "without changing meaning or removing any content; only timestamps/meta may be removed; keep Markdown paragraphs with blank lines."
            )

        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=4000,  # å¯¹é½JSï¼šä¼˜åŒ–/æ ¼å¼åŒ–é˜¶æ®µæœ€å¤§tokensâ‰ˆ4000
                temperature=0.1
            )
            optimized_text = response.choices[0].message.content or ""
            # ç§»é™¤è¯¸å¦‚ "# Transcript" / "## Transcript" ç­‰æ ‡é¢˜
            optimized_text = self._remove_transcript_heading(optimized_text)
            enforced = self._enforce_paragraph_max_chars(optimized_text.strip(), max_chars=250)
            return self._ensure_markdown_paragraphs(enforced)
        except Exception as e:
            logger.error(f"å•å—æ–‡æœ¬ä¼˜åŒ–å¤±è´¥: {e}")
            return self._apply_basic_formatting(chunk_text)

    def _smart_split_long_chunk(self, text: str, max_chars_per_chunk: int) -> list:
        """åœ¨å¥å­/ç©ºæ ¼è¾¹ç•Œå¤„å®‰å…¨åˆ‡åˆ†è¶…é•¿æ–‡æœ¬ã€‚"""
        chunks = []
        pos = 0
        while pos < len(text):
            end = min(pos + max_chars_per_chunk, len(text))
            if end < len(text):
                # ä¼˜å…ˆå¥å­è¾¹ç•Œ
                sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
                best = -1
                for ch in sentence_endings:
                    idx = text.rfind(ch, pos, end)
                    if idx > best:
                        best = idx
                if best > pos + int(max_chars_per_chunk * 0.7):
                    end = best + 1
                else:
                    # æ¬¡é€‰ï¼šç©ºæ ¼è¾¹ç•Œ
                    space_idx = text.rfind(' ', pos, end)
                    if space_idx > pos + int(max_chars_per_chunk * 0.8):
                        end = space_idx
            chunks.append(text[pos:end].strip())
            pos = end
        return [c for c in chunks if c]

    def _find_safe_cut_point(self, text: str) -> int:
        """æ‰¾åˆ°å®‰å…¨çš„åˆ‡å‰²ç‚¹ï¼ˆæ®µè½>å¥å­>çŸ­è¯­ï¼‰ã€‚"""
        import re
        # æ®µè½
        p = text.rfind("\n\n")
        if p > 0:
            return p + 2
        # å¥å­
        last_sentence_end = -1
        for m in re.finditer(r"[ã€‚ï¼ï¼Ÿ\.!?]\s*", text):
            last_sentence_end = m.end()
        if last_sentence_end > 20:
            return last_sentence_end
        # çŸ­è¯­
        last_phrase_end = -1
        for m in re.finditer(r"[ï¼Œï¼›,;]\s*", text):
            last_phrase_end = m.end()
        if last_phrase_end > 20:
            return last_phrase_end
        return len(text)

    def _find_overlap_between_texts(self, text1: str, text2: str) -> str:
        """æ£€æµ‹ç›¸é‚»ä¸¤æ®µçš„é‡å å†…å®¹ï¼Œç”¨äºå»é‡ã€‚"""
        max_len = min(len(text1), len(text2))
        # é€æ­¥ä»é•¿åˆ°çŸ­å°è¯•
        for length in range(max_len, 19, -1):
            suffix = text1[-length:]
            prefix = text2[:length]
            if suffix == prefix:
                cut = self._find_safe_cut_point(prefix)
                if cut > 20:
                    return prefix[:cut]
                return suffix
        return ""

    def _apply_basic_formatting(self, text: str) -> str:
        """å½“AIå¤±è´¥æ—¶çš„å›é€€ï¼šæŒ‰å¥å­æ‹¼æ®µï¼Œæ®µè½â‰¤250å­—ç¬¦ï¼ŒåŒæ¢è¡Œåˆ†éš”ã€‚"""
        if not text or not text.strip():
            return text
        import re
        parts = re.split(r"([ã€‚ï¼ï¼Ÿ\.!?]+\s*)", text)
        sentences = []
        current = ""
        for i, part in enumerate(parts):
            if i % 2 == 0:
                current += part
            else:
                current += part
                if current.strip():
                    sentences.append(current.strip())
                    current = ""
        if current.strip():
            sentences.append(current.strip())
        paras = []
        cur = ""
        for s in sentences:
            candidate = (cur + " " + s).strip() if cur else s
            if len(candidate) > 250 and cur:
                paras.append(cur.strip())
                cur = s
            else:
                cur = candidate
        if cur.strip():
            paras.append(cur.strip())
        return self._ensure_markdown_paragraphs("\n\n".join(paras))

    async def _format_long_transcript_in_chunks(self, raw_transcript: str, transcript_language: str, max_chars_per_chunk: int) -> str:
        """æ™ºèƒ½åˆ†å—+ä¸Šä¸‹æ–‡+å»é‡ åˆæˆä¼˜åŒ–æ–‡æœ¬ï¼ˆJSç­–ç•¥ç§»æ¤ï¼‰ã€‚"""
        import re
        # å…ˆæŒ‰å¥å­åˆ‡åˆ†ï¼Œç»„è£…ä¸è¶…è¿‡max_chars_per_chunkçš„å—
        parts = re.split(r"([ã€‚ï¼ï¼Ÿ\.!?]+\s*)", raw_transcript)
        sentences = []
        buf = ""
        for i, part in enumerate(parts):
            if i % 2 == 0:
                buf += part
            else:
                buf += part
                if buf.strip():
                    sentences.append(buf.strip())
                    buf = ""
        if buf.strip():
            sentences.append(buf.strip())

        chunks = []
        cur = ""
        for s in sentences:
            candidate = (cur + " " + s).strip() if cur else s
            if len(candidate) > max_chars_per_chunk and cur:
                chunks.append(cur.strip())
                cur = s
            else:
                cur = candidate
        if cur.strip():
            chunks.append(cur.strip())

        # å¯¹ä»ç„¶è¿‡é•¿çš„å—äºŒæ¬¡å®‰å…¨åˆ‡åˆ†
        final_chunks = []
        for c in chunks:
            if len(c) <= max_chars_per_chunk:
                final_chunks.append(c)
            else:
                final_chunks.extend(self._smart_split_long_chunk(c, max_chars_per_chunk))

        logger.info(f"æ–‡æœ¬åˆ†ä¸º {len(final_chunks)} å—å¤„ç†")

        optimized = []
        for i, c in enumerate(final_chunks):
            chunk_with_context = c
            if i > 0:
                prev_tail = final_chunks[i - 1][-100:]
                marker = f"[ä¸Šæ–‡ç»­ï¼š{prev_tail}]" if transcript_language == 'zh' else f"[Context continued: {prev_tail}]"
                chunk_with_context = marker + "\n\n" + c
            try:
                oc = await self._format_single_chunk(chunk_with_context, transcript_language)
                # ç§»é™¤ä¸Šä¸‹æ–‡æ ‡è®°
                oc = re.sub(r"^\[(ä¸Šæ–‡ç»­|Context continued)ï¼š?:?.*?\]\s*", "", oc, flags=re.S)
                optimized.append(oc)
            except Exception as e:
                logger.warning(f"ç¬¬ {i+1} å—ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ ¼å¼åŒ–: {e}")
                optimized.append(self._apply_basic_formatting(c))

        # é‚»æ¥å—å»é‡
        deduped = []
        for i, c in enumerate(optimized):
            cur_txt = c
            if i > 0 and deduped:
                prev = deduped[-1]
                overlap = self._find_overlap_between_texts(prev[-200:], cur_txt[:200])
                if overlap:
                    cur_txt = cur_txt[len(overlap):].lstrip()
                    if not cur_txt:
                        continue
            if cur_txt.strip():
                deduped.append(cur_txt)

        merged = "\n\n".join(deduped)
        merged = self._remove_transcript_heading(merged)
        enforced = self._enforce_paragraph_max_chars(merged, max_chars=250)
        return self._ensure_markdown_paragraphs(enforced)

    def _remove_timestamps_and_meta(self, text: str) -> str:
        """ä»…ç§»é™¤æ—¶é—´æˆ³è¡Œä¸æ˜æ˜¾å…ƒä¿¡æ¯ï¼ˆæ ‡é¢˜ã€æ£€æµ‹è¯­è¨€ç­‰ï¼‰ï¼Œä¿ç•™åŸæ–‡å£è¯­/é‡å¤ã€‚"""
        lines = text.split('\n')
        kept = []
        for line in lines:
            s = line.strip()
            # è·³è¿‡æ—¶é—´æˆ³ä¸å…ƒä¿¡æ¯
            if (s.startswith('**[') and s.endswith(']**')):
                continue
            if s.startswith('# '):
                # è·³è¿‡é¡¶çº§æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯è§†é¢‘æ ‡é¢˜ï¼Œå¯åœ¨æœ€ç»ˆåŠ å›ï¼‰
                continue
            if s.startswith('**æ£€æµ‹è¯­è¨€:**') or s.startswith('**è¯­è¨€æ¦‚ç‡:**'):
                continue
            kept.append(line)
        # è§„èŒƒç©ºè¡Œ
        cleaned = '\n'.join(kept)
        return cleaned

    def _enforce_paragraph_max_chars(self, text: str, max_chars: int = 250) -> str:
        """æŒ‰æ®µè½æ‹†åˆ†å¹¶ç¡®ä¿æ¯æ®µä¸è¶…è¿‡max_charsï¼Œå¿…è¦æ—¶æŒ‰å¥å­è¾¹ç•Œæ‹†ä¸ºå¤šæ®µã€‚"""
        if not text:
            return text
        import re
        paragraphs = [p for p in re.split(r"\n\s*\n", text) if p is not None]
        new_paragraphs = []
        for para in paragraphs:
            para = para.strip()
            if len(para) <= max_chars:
                new_paragraphs.append(para)
                continue
            # å¥å­åˆ‡åˆ†
            parts = re.split(r"([ã€‚ï¼ï¼Ÿ\.!?]+\s*)", para)
            sentences = []
            buf = ""
            for i, part in enumerate(parts):
                if i % 2 == 0:
                    buf += part
                else:
                    buf += part
                    if buf.strip():
                        sentences.append(buf.strip())
                        buf = ""
            if buf.strip():
                sentences.append(buf.strip())
            cur = ""
            for s in sentences:
                candidate = (cur + (" " if cur else "") + s).strip()
                if len(candidate) > max_chars and cur:
                    new_paragraphs.append(cur)
                    cur = s
                else:
                    cur = candidate
            if cur:
                new_paragraphs.append(cur)
        return "\n\n".join([p.strip() for p in new_paragraphs if p is not None])

    def _remove_transcript_heading(self, text: str) -> str:
        """ç§»é™¤å¼€å¤´æˆ–æ®µè½ä¸­çš„ä»¥ Transcript ä¸ºæ ‡é¢˜çš„è¡Œï¼ˆä»»æ„çº§åˆ«#ï¼‰ï¼Œä¸æ”¹å˜æ­£æ–‡ã€‚"""
        if not text:
            return text
        import re
        # ç§»é™¤å½¢å¦‚ '## Transcript'ã€'# Transcript Text'ã€'### transcript' çš„æ ‡é¢˜è¡Œ
        lines = text.split('\n')
        filtered = []
        for line in lines:
            stripped = line.strip()
            if re.match(r"^#{1,6}\s*transcript(\s+text)?\s*$", stripped, flags=re.I):
                continue
            filtered.append(line)
        return '\n'.join(filtered)

    def _split_into_chunks(self, text: str, max_tokens: int) -> list:
        """
        å°†åŸå§‹è½¬å½•æ–‡æœ¬æ™ºèƒ½åˆ†å‰²æˆåˆé€‚å¤§å°çš„å—
        ç­–ç•¥ï¼šå…ˆæå–çº¯æ–‡æœ¬ï¼ŒæŒ‰å¥å­å’Œæ®µè½è‡ªç„¶åˆ†å‰²
        """
        import re
        
        # 1. å…ˆæå–çº¯æ–‡æœ¬å†…å®¹ï¼ˆç§»é™¤æ—¶é—´æˆ³ã€æ ‡é¢˜ç­‰ï¼‰
        pure_text = self._extract_pure_text(text)
        
        # 2. æŒ‰å¥å­åˆ†å‰²ï¼Œä¿æŒå¥å­å®Œæ•´æ€§
        sentences = self._split_into_sentences(pure_text)
        
        # 3. æŒ‰tokené™åˆ¶ç»„è£…æˆå—
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = self._estimate_tokens(sentence)
            
            # æ£€æŸ¥æ˜¯å¦èƒ½åŠ å…¥å½“å‰å—
            if current_tokens + sentence_tokens > max_tokens and current_chunk:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                chunks.append(self._join_sentences(current_chunk))
                current_chunk = [sentence]
                current_tokens = sentence_tokens
            else:
                # æ·»åŠ åˆ°å½“å‰å—
                current_chunk.append(sentence)
                current_tokens += sentence_tokens
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(self._join_sentences(current_chunk))
        
        return chunks
    
    def _extract_pure_text(self, raw_transcript: str) -> str:
        """
        ä»åŸå§‹è½¬å½•ä¸­æå–çº¯æ–‡æœ¬ï¼Œç§»é™¤æ—¶é—´æˆ³å’Œå…ƒæ•°æ®
        """
        lines = raw_transcript.split('\n')
        text_lines = []
        
        for line in lines:
            line = line.strip()
            # è·³è¿‡æ—¶é—´æˆ³ã€æ ‡é¢˜ã€å…ƒæ•°æ®
            if (line.startswith('**[') and line.endswith(']**') or
                line.startswith('#') or
                line.startswith('**æ£€æµ‹è¯­è¨€:**') or
                line.startswith('**è¯­è¨€æ¦‚ç‡:**') or
                not line):
                continue
            text_lines.append(line)
        
        return ' '.join(text_lines)
    
    def _split_into_sentences(self, text: str) -> list:
        """
        æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬ï¼Œè€ƒè™‘ä¸­è‹±æ–‡å·®å¼‚
        """
        import re
        
        # ä¸­è‹±æ–‡å¥å­ç»“æŸç¬¦
        sentence_endings = r'[.!?ã€‚ï¼ï¼Ÿ;ï¼›]+'
        
        # åˆ†å‰²å¥å­ï¼Œä¿ç•™å¥å·
        parts = re.split(f'({sentence_endings})', text)
        
        sentences = []
        current = ""
        
        for i, part in enumerate(parts):
            if re.match(sentence_endings, part):
                # è¿™æ˜¯å¥å­ç»“æŸç¬¦ï¼ŒåŠ åˆ°å½“å‰å¥å­
                current += part
                if current.strip():
                    sentences.append(current.strip())
                current = ""
            else:
                # è¿™æ˜¯å¥å­å†…å®¹
                current += part
        
        # å¤„ç†æœ€åæ²¡æœ‰å¥å·çš„éƒ¨åˆ†
        if current.strip():
            sentences.append(current.strip())
        
        return [s for s in sentences if s.strip()]
    

    
    def _join_sentences(self, sentences: list) -> str:
        """
        é‡æ–°ç»„åˆå¥å­ä¸ºæ®µè½
        """
        return ' '.join(sentences)

    def _basic_transcript_cleanup(self, raw_transcript: str) -> str:
        """
        åŸºæœ¬çš„è½¬å½•æ–‡æœ¬æ¸…ç†ï¼šç§»é™¤æ—¶é—´æˆ³å’Œæ ‡é¢˜ä¿¡æ¯
        å½“GPTä¼˜åŒ–å¤±è´¥æ—¶çš„åå¤‡æ–¹æ¡ˆ
        """
        lines = raw_transcript.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # è·³è¿‡æ—¶é—´æˆ³è¡Œ
            if line.strip().startswith('**[') and line.strip().endswith(']**'):
                continue
            # è·³è¿‡æ ‡é¢˜è¡Œ
            if line.strip().startswith('# ') or line.strip().startswith('## '):
                continue
            # è·³è¿‡æ£€æµ‹è¯­è¨€ç­‰å…ƒä¿¡æ¯è¡Œ
            if line.strip().startswith('**æ£€æµ‹è¯­è¨€:**') or line.strip().startswith('**è¯­è¨€æ¦‚ç‡:**'):
                continue
            # ä¿ç•™éç©ºæ–‡æœ¬è¡Œ
            if line.strip():
                cleaned_lines.append(line.strip())
        
        # å°†å¥å­é‡æ–°ç»„åˆå¹¶æ™ºèƒ½åˆ†æ®µ
        text = ' '.join(cleaned_lines)
        
        # æ›´æ™ºèƒ½çš„åˆ†å¥å¤„ç†ï¼Œè€ƒè™‘ä¸­è‹±æ–‡å·®å¼‚
        import re
        
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å¥
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        paragraphs = []
        current_paragraph = []
        
        for i, sentence in enumerate(sentences):
            if sentence:
                current_paragraph.append(sentence)
                
                # æ™ºèƒ½åˆ†æ®µæ¡ä»¶ï¼š
                # 1. æ¯3ä¸ªå¥å­ä¸€æ®µï¼ˆåŸºæœ¬è§„åˆ™ï¼‰
                # 2. é‡åˆ°è¯é¢˜è½¬æ¢è¯æ±‡æ—¶å¼ºåˆ¶åˆ†æ®µ
                # 3. é¿å…è¶…é•¿æ®µè½
                topic_change_keywords = [
                    'é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å', 'æ¥ä¸‹æ¥', 'å¦å¤–', 'æ­¤å¤–', 'æœ€å', 'æ€»ä¹‹',
                    'first', 'second', 'third', 'next', 'also', 'however', 'finally',
                    'ç°åœ¨', 'é‚£ä¹ˆ', 'æ‰€ä»¥', 'å› æ­¤', 'ä½†æ˜¯', 'ç„¶è€Œ',
                    'now', 'so', 'therefore', 'but', 'however'
                ]
                
                should_break = False
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ®µ
                if len(current_paragraph) >= 3:  # åŸºæœ¬é•¿åº¦æ¡ä»¶
                    should_break = True
                elif len(current_paragraph) >= 2:  # è¾ƒçŸ­ä½†é‡åˆ°è¯é¢˜è½¬æ¢
                    for keyword in topic_change_keywords:
                        if sentence.lower().startswith(keyword.lower()):
                            should_break = True
                            break
                
                if should_break or len(current_paragraph) >= 4:  # æœ€å¤§é•¿åº¦é™åˆ¶
                    # ç»„åˆå½“å‰æ®µè½
                    paragraph_text = '. '.join(current_paragraph)
                    if not paragraph_text.endswith('.'):
                        paragraph_text += '.'
                    paragraphs.append(paragraph_text)
                    current_paragraph = []
        
        # æ·»åŠ å‰©ä½™çš„å¥å­
        if current_paragraph:
            paragraph_text = '. '.join(current_paragraph)
            if not paragraph_text.endswith('.'):
                paragraph_text += '.'
            paragraphs.append(paragraph_text)
        
        return '\n\n'.join(paragraphs)

    async def _final_paragraph_organization(self, text: str, lang_instruction: str) -> str:
        """
        å¯¹åˆå¹¶åçš„æ–‡æœ¬è¿›è¡Œæœ€ç»ˆçš„æ®µè½æ•´ç†
        ä½¿ç”¨æ”¹è¿›çš„promptå’Œå·¥ç¨‹éªŒè¯
        """
        try:
            # ä¼°ç®—æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœå¤ªé•¿åˆ™åˆ†å—å¤„ç†
            estimated_tokens = self._estimate_tokens(text)
            if estimated_tokens > 3000:  # å¯¹äºå¾ˆé•¿çš„æ–‡æœ¬ï¼Œåˆ†å—å¤„ç†
                return await self._organize_long_text_paragraphs(text, lang_instruction)
            
            system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„{lang_instruction}æ–‡æœ¬æ®µè½æ•´ç†ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æŒ‰ç…§è¯­ä¹‰å’Œé€»è¾‘é‡æ–°ç»„ç»‡æ®µè½ã€‚

ğŸ¯ **æ ¸å¿ƒåŸåˆ™**ï¼š
1. **ä¸¥æ ¼ä¿æŒåŸå§‹è¯­è¨€({lang_instruction})**ï¼Œç»ä¸ç¿»è¯‘
2. **ä¿æŒæ‰€æœ‰å†…å®¹å®Œæ•´**ï¼Œä¸åˆ é™¤ä¸æ·»åŠ ä»»ä½•ä¿¡æ¯
3. **æŒ‰è¯­ä¹‰é€»è¾‘åˆ†æ®µ**ï¼šæ¯æ®µå›´ç»•ä¸€ä¸ªå®Œæ•´çš„æ€æƒ³æˆ–è¯é¢˜
4. **ä¸¥æ ¼æ§åˆ¶æ®µè½é•¿åº¦**ï¼šæ¯æ®µç»ä¸è¶…è¿‡250è¯
5. **ä¿æŒè‡ªç„¶æµç•…**ï¼šæ®µè½é—´åº”æœ‰é€»è¾‘è¿æ¥

ğŸ“ **åˆ†æ®µæ ‡å‡†**ï¼š
- **è¯­ä¹‰å®Œæ•´æ€§**ï¼šæ¯æ®µè®²è¿°ä¸€ä¸ªå®Œæ•´æ¦‚å¿µæˆ–äº‹ä»¶
- **é€‚ä¸­é•¿åº¦**ï¼š3-7ä¸ªå¥å­ï¼Œæ¯æ®µç»ä¸è¶…è¿‡250è¯
- **é€»è¾‘è¾¹ç•Œ**ï¼šåœ¨è¯é¢˜è½¬æ¢ã€æ—¶é—´è½¬æ¢ã€è§‚ç‚¹è½¬æ¢å¤„åˆ†æ®µ
- **è‡ªç„¶æ–­ç‚¹**ï¼šéµå¾ªè¯´è¯è€…çš„è‡ªç„¶åœé¡¿å’Œé€»è¾‘

âš ï¸ **ä¸¥ç¦**ï¼š
- åˆ›é€ è¶…è¿‡250è¯çš„å·¨å‹æ®µè½
- å¼ºè¡Œåˆå¹¶ä¸ç›¸å…³çš„å†…å®¹
- æ‰“æ–­å®Œæ•´çš„æ•…äº‹æˆ–è®ºè¿°

è¾“å‡ºæ ¼å¼ï¼šæ®µè½é—´ç”¨ç©ºè¡Œåˆ†éš”ã€‚"""

            user_prompt = f"""è¯·é‡æ–°æ•´ç†ä»¥ä¸‹{lang_instruction}æ–‡æœ¬çš„æ®µè½ç»“æ„ã€‚ä¸¥æ ¼æŒ‰ç…§è¯­ä¹‰å’Œé€»è¾‘è¿›è¡Œåˆ†æ®µï¼Œç¡®ä¿æ¯æ®µä¸è¶…è¿‡200è¯ï¼š

{text}

é‡æ–°åˆ†æ®µåçš„æ–‡æœ¬ï¼š"""

            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=4000,  # å¯¹é½JSï¼šæ®µè½æ•´ç†é˜¶æ®µæœ€å¤§tokensâ‰ˆ4000
                temperature=0.05  # é™ä½æ¸©åº¦ï¼Œæé«˜ä¸€è‡´æ€§
            )
            
            organized_text = response.choices[0].message.content
            
            # å·¥ç¨‹éªŒè¯ï¼šæ£€æŸ¥æ®µè½é•¿åº¦
            validated_text = self._validate_paragraph_lengths(organized_text)
            
            return validated_text
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆæ®µè½æ•´ç†å¤±è´¥: {e}")
            # å¤±è´¥æ—¶ä½¿ç”¨åŸºç¡€åˆ†æ®µå¤„ç†
            return self._basic_paragraph_fallback(text)

    async def _organize_long_text_paragraphs(self, text: str, lang_instruction: str) -> str:
        """
        å¯¹äºå¾ˆé•¿çš„æ–‡æœ¬ï¼Œåˆ†å—è¿›è¡Œæ®µè½æ•´ç†
        """
        try:
            # æŒ‰ç°æœ‰æ®µè½åˆ†å‰²
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            organized_chunks = []
            
            current_chunk = []
            current_tokens = 0
            max_chunk_tokens = 2500  # é€‚åº”4000 tokensé™åˆ¶çš„chunkå¤§å°
            
            for para in paragraphs:
                para_tokens = self._estimate_tokens(para)
                
                if current_tokens + para_tokens > max_chunk_tokens and current_chunk:
                    # å¤„ç†å½“å‰chunk
                    chunk_text = '\n\n'.join(current_chunk)
                    organized_chunk = await self._organize_single_chunk(chunk_text, lang_instruction)
                    organized_chunks.append(organized_chunk)
                    
                    current_chunk = [para]
                    current_tokens = para_tokens
                else:
                    current_chunk.append(para)
                    current_tokens += para_tokens
            
            # å¤„ç†æœ€åä¸€ä¸ªchunk
            if current_chunk:
                chunk_text = '\n\n'.join(current_chunk)
                organized_chunk = await self._organize_single_chunk(chunk_text, lang_instruction)
                organized_chunks.append(organized_chunk)
            
            return '\n\n'.join(organized_chunks)
            
        except Exception as e:
            logger.error(f"é•¿æ–‡æœ¬æ®µè½æ•´ç†å¤±è´¥: {e}")
            return self._basic_paragraph_fallback(text)

    async def _organize_single_chunk(self, text: str, lang_instruction: str) -> str:
        """
        æ•´ç†å•ä¸ªæ–‡æœ¬å—çš„æ®µè½
        """
        system_prompt = f"""ä½ æ˜¯{lang_instruction}æ®µè½æ•´ç†ä¸“å®¶ã€‚æŒ‰è¯­ä¹‰é‡æ–°ç»„ç»‡æ®µè½ï¼Œç¡®ä¿æ¯æ®µä¸è¶…è¿‡200è¯ã€‚

æ ¸å¿ƒè¦æ±‚ï¼š
1. ä¸¥æ ¼ä¿æŒ{lang_instruction}åŸè¯­è¨€
2. æŒ‰è¯­ä¹‰é€»è¾‘åˆ†æ®µï¼Œæ¯æ®µä¸€ä¸ªä¸»é¢˜
3. æ¯æ®µç»ä¸è¶…è¿‡250è¯
4. æ®µè½é—´ç©ºè¡Œåˆ†éš”
5. ä¿æŒå†…å®¹å®Œæ•´ï¼Œä¸åˆ å‡ä¿¡æ¯"""

        user_prompt = f"""é‡æ–°åˆ†æ®µä»¥ä¸‹æ–‡æœ¬ï¼Œç¡®ä¿æ¯æ®µä¸è¶…è¿‡200è¯ï¼š

{text}"""

        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=1200,  # é€‚åº”4000 tokensæ€»é™åˆ¶
            temperature=0.05
        )
        
        return response.choices[0].message.content

    def _validate_paragraph_lengths(self, text: str) -> str:
        """
        éªŒè¯æ®µè½é•¿åº¦ï¼Œå¦‚æœæœ‰è¶…é•¿æ®µè½åˆ™å°è¯•åˆ†å‰²
        """
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        validated_paragraphs = []
        
        for para in paragraphs:
            word_count = len(para.split())
            
            if word_count > 300:  # å¦‚æœæ®µè½è¶…è¿‡300è¯
                logger.warning(f"æ£€æµ‹åˆ°è¶…é•¿æ®µè½({word_count}è¯)ï¼Œå°è¯•åˆ†å‰²")
                # å°è¯•æŒ‰å¥å­åˆ†å‰²é•¿æ®µè½
                split_paras = self._split_long_paragraph(para)
                validated_paragraphs.extend(split_paras)
            else:
                validated_paragraphs.append(para)
        
        return '\n\n'.join(validated_paragraphs)

    def _split_long_paragraph(self, paragraph: str) -> list:
        """
        åˆ†å‰²è¿‡é•¿çš„æ®µè½
        """
        import re
        
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', paragraph)
        sentences = [s.strip() + '.' for s in sentences if s.strip()]
        
        split_paragraphs = []
        current_para = []
        current_words = 0
        
        for sentence in sentences:
            sentence_words = len(sentence.split())
            
            if current_words + sentence_words > 200 and current_para:
                # å½“å‰æ®µè½è¾¾åˆ°é•¿åº¦é™åˆ¶
                split_paragraphs.append(' '.join(current_para))
                current_para = [sentence]
                current_words = sentence_words
            else:
                current_para.append(sentence)
                current_words += sentence_words
        
        # æ·»åŠ æœ€åä¸€æ®µ
        if current_para:
            split_paragraphs.append(' '.join(current_para))
        
        return split_paragraphs

    def _basic_paragraph_fallback(self, text: str) -> str:
        """
        åŸºç¡€åˆ†æ®µfallbackæœºåˆ¶
        å½“GPTæ•´ç†å¤±è´¥æ—¶ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™åˆ†æ®µ
        """
        import re
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        basic_paragraphs = []
        
        for para in paragraphs:
            word_count = len(para.split())
            
            if word_count > 250:
                # é•¿æ®µè½æŒ‰å¥å­åˆ†å‰²
                split_paras = self._split_long_paragraph(para)
                basic_paragraphs.extend(split_paras)
            elif word_count < 30 and basic_paragraphs:
                # çŸ­æ®µè½ä¸ä¸Šä¸€æ®µåˆå¹¶ï¼ˆå¦‚æœåˆå¹¶åä¸è¶…è¿‡200è¯ï¼‰
                last_para = basic_paragraphs[-1]
                combined_words = len(last_para.split()) + word_count
                
                if combined_words <= 200:
                    basic_paragraphs[-1] = last_para + ' ' + para
                else:
                    basic_paragraphs.append(para)
            else:
                basic_paragraphs.append(para)
        
        return '\n\n'.join(basic_paragraphs)

    async def summarize(self, transcript: str, target_language: str = "zh", video_title: str = None) -> str:
        """
        ç”Ÿæˆè§†é¢‘è½¬å½•çš„æ‘˜è¦
        
        Args:
            transcript: è½¬å½•æ–‡æœ¬
            target_language: ç›®æ ‡è¯­è¨€ä»£ç 
            
        Returns:
            æ‘˜è¦æ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        try:
            if not self.client:
                logger.warning("OpenAI APIä¸å¯ç”¨ï¼Œç”Ÿæˆå¤‡ç”¨æ‘˜è¦")
                return self._generate_fallback_summary(transcript, target_language, video_title)
            
            # ä¼°ç®—è½¬å½•æ–‡æœ¬é•¿åº¦ï¼Œå†³å®šæ˜¯å¦éœ€è¦åˆ†å—æ‘˜è¦
            estimated_tokens = self._estimate_tokens(transcript)
            max_summarize_tokens = 4000  # æé«˜é™åˆ¶ï¼Œä¼˜å…ˆä½¿ç”¨å•æ–‡æœ¬å¤„ç†ä»¥è·å¾—æ›´å¥½çš„æ€»ç»“è´¨é‡
            
            if estimated_tokens <= max_summarize_tokens:
                # çŸ­æ–‡æœ¬ç›´æ¥æ‘˜è¦
                return await self._summarize_single_text(transcript, target_language, video_title)
            else:
                # é•¿æ–‡æœ¬åˆ†å—æ‘˜è¦
                logger.info(f"æ–‡æœ¬è¾ƒé•¿({estimated_tokens} tokens)ï¼Œå¯ç”¨åˆ†å—æ‘˜è¦")
                return await self._summarize_with_chunks(transcript, target_language, video_title, max_summarize_tokens)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {str(e)}")
            return self._generate_fallback_summary(transcript, target_language, video_title)

    async def _summarize_single_text(self, transcript: str, target_language: str, video_title: str = None) -> str:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œæ‘˜è¦
        """
        # è·å–ç›®æ ‡è¯­è¨€åç§°
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        
        # æ„å»ºæç¤ºè¯ï¼ˆæ”¾å®½æ ¼å¼ï¼Œå…è®¸è‡ªç„¶æ®µæˆ–å°‘é‡æ¡ç›®ï¼›å¼ºè°ƒè¦†ç›–åº¦ä¸ç¯‡å¹…ï¼‰
        system_prompt = f"""ä½ æ˜¯èµ„æ·±å†…å®¹ç¼–è¾‘ã€‚è¯·ç”¨{language_name}å¯¹å…¨æ–‡è¿›è¡Œé«˜è´¨é‡æ€»ç»“ï¼Œä¸å¼ºåˆ¶ä½¿ç”¨å°æ ‡é¢˜æˆ–å›ºå®šç»“æ„ï¼›ä»¥è‡ªç„¶æ®µä¸ºä¸»ï¼Œå¿…è¦æ—¶å¯ç”¨å°‘é‡æ¡ç›®ã€‚

ç›®æ ‡ï¼š
- å…¨é¢è¦†ç›–æ ¸å¿ƒè§‚ç‚¹ã€å…³é”®è®ºè¯ã€é‡è¦ä¾‹å­/æ•°æ®ã€å…³é”®ç»“è®º/å¯ç¤ºã€å¯æ‰§è¡Œå»ºè®®ï¼ˆå¦‚æœ‰ï¼‰ã€‚
- å¿ å®åŸæ–‡ï¼Œä¸è‡†é€ ï¼›é€»è¾‘æ¸…æ¥šã€è¯­è¨€è‡ªç„¶ï¼›é¿å…é€å¥å¤è¿°ï¼›é€‚åº¦å»é‡ä½†ä¸é—æ¼ä¿¡æ¯ã€‚
- ä¿æŒä¿¡æ¯å¯†åº¦ä¸å¯è¯»æ€§çš„å¹³è¡¡ã€‚
- ç¯‡å¹…å»ºè®®ï¼šä¸å°‘äº600å­—ï¼Œå¿…è¦æ—¶å¯è¾¾1200å­—ã€‚"""

        user_prompt = f"""è¯·åŸºäºä»¥ä¸‹å†…å®¹ï¼Œå†™å‡ºä¸€ç¯‡ä¿¡æ¯å…¨é¢ã€ç»“æ„æ¸…æ™°ã€ç¯‡å¹…å……è¶³çš„{language_name}æ€»ç»“ï¼š

{transcript}

è¦æ±‚ï¼š
- è‡ªç„¶æ®µä¸ºä¸»ï¼Œå¯è¾…ä»¥å°‘é‡æ¡ç›®ï¼›ä¸ä½¿ç”¨è£…é¥°æ€§å°æ ‡é¢˜æˆ–æ ¼å¼ï¼›
- è¦†ç›–å…¨æ–‡å…³é”®æ€æƒ³ä¸è®ºæ®ï¼Œä¿ç•™é‡è¦ä¾‹å­æˆ–æ•°æ®ï¼›
- å…³æ³¨å‰åŠæ®µä¸ååŠæ®µçš„å‡è¡¡è¦†ç›–ï¼›
- è¯­è¨€å…‹åˆ¶ä½†å†…å®¹å……åˆ†ï¼Œç¯‡å¹…ä¸å°‘äº600å­—ã€‚"""

        logger.info(f"æ­£åœ¨ç”Ÿæˆ{language_name}æ‘˜è¦...")
        
        # è°ƒç”¨OpenAI API
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=3500,  # æ§åˆ¶åœ¨å®‰å…¨èŒƒå›´å†…ï¼Œé¿å…è¶…å‡ºæ¨¡å‹é™åˆ¶
            temperature=0.3
        )
        
        summary = response.choices[0].message.content

        # è½»åº¦æ¶¦è‰²åå†è¿”å›
        polished = await self._polish_summary(summary, target_language)
        return self._format_summary_with_meta(polished, target_language, video_title)

    async def _summarize_with_chunks(self, transcript: str, target_language: str, video_title: str, max_tokens: int) -> str:
        """
        åˆ†å—æ‘˜è¦é•¿æ–‡æœ¬
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")

        # ä½¿ç”¨JSç­–ç•¥ï¼šæŒ‰å­—ç¬¦è¿›è¡Œæ™ºèƒ½åˆ†å—ï¼ˆæ®µè½>å¥å­ï¼‰
        chunks = self._smart_chunk_text(transcript, max_chars_per_chunk=4000)
        logger.info(f"åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œæ‘˜è¦")
        
        chunk_summaries = []
        
        # æ¯å—ç”Ÿæˆå±€éƒ¨æ‘˜è¦
        for i, chunk in enumerate(chunks):
            logger.info(f"æ­£åœ¨æ‘˜è¦ç¬¬ {i+1}/{len(chunks)} å—...")
            
            system_prompt = f"""ä½ æ˜¯æ‘˜è¦ä¸“å®¶ã€‚è¯·ä¸ºè¯¥åˆ†å—å†™ä¸€æ®µé«˜å¯†åº¦å°ç»“ï¼ˆ{language_name}ï¼‰ã€‚

è¿™æ˜¯å®Œæ•´å†…å®¹çš„ç¬¬{i+1}éƒ¨åˆ†ï¼Œå…±{len(chunks)}éƒ¨åˆ†ï¼ˆç¼–å·ï¼šPart {i+1}/{len(chunks)}ï¼‰ã€‚

è¾“å‡ºåå¥½ï¼šè‡ªç„¶æ®µä¸ºä¸»ï¼Œå¿…è¦æ—¶å¯ç”¨æå°‘é‡æ¡ç›®ï¼›çªå‡ºæ–°å¢ä¿¡æ¯åŠå…¶ä¸ä¸»çº¿çš„å…³ç³»ï¼›é¿å…ç©ºæ³›å¤è¿°ä¸æ ¼å¼åŒ–æ ‡é¢˜ï¼›ç¯‡å¹…é€‚ä¸­ï¼ˆå»ºè®®120-220å­—ï¼‰ã€‚"""

            user_prompt = f"""[Part {i+1}/{len(chunks)}] æ¦‚æ‹¬ä»¥ä¸‹æ–‡æœ¬çš„è¦ç‚¹ï¼ˆè‡ªç„¶æ®µä¸ºä¸»ï¼Œå¯å°‘é‡æ¡ç›®ï¼Œ120-220å­—ï¼‰ï¼š

{chunk}

è¯·é¿å…ä½¿ç”¨ä»»ä½•å°æ ‡é¢˜æˆ–è£…é¥°æ€§åˆ†éš”ï¼Œåªè¾“å‡ºå†…å®¹ã€‚"""

            try:
                response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=1000,  # æå‡åˆ†å—æ‘˜è¦å®¹é‡ä»¥æ¶µç›–æ›´å¤šç»†èŠ‚
                    temperature=0.3
                )
                
                chunk_summary = response.choices[0].message.content
                chunk_summaries.append(chunk_summary)
                
            except Exception as e:
                logger.error(f"æ‘˜è¦ç¬¬ {i+1} å—å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ç”Ÿæˆç®€å•æ‘˜è¦
                simple_summary = f"ç¬¬{i+1}éƒ¨åˆ†å†…å®¹æ¦‚è¿°ï¼š" + chunk[:200] + "..."
                chunk_summaries.append(simple_summary)
        
        # åˆå¹¶æ‰€æœ‰å±€éƒ¨æ‘˜è¦ï¼ˆå¸¦ç¼–å·ï¼‰ï¼Œå¦‚åˆ†å—è¾ƒå¤šåˆ™åˆ†å±‚æ•´åˆï¼ˆä¸å¼•å…¥å°æ ‡é¢˜ï¼‰
        combined_summaries = "\n\n".join([f"[Part {idx+1}]\n" + s for idx, s in enumerate(chunk_summaries)])

        logger.info("æ­£åœ¨æ•´åˆæœ€ç»ˆæ‘˜è¦...")
        if len(chunk_summaries) > 10:
            final_summary = await self._integrate_hierarchical_summaries(chunk_summaries, target_language)
        else:
            final_summary = await self._integrate_chunk_summaries(combined_summaries, target_language)

        # è½»åº¦æ¶¦è‰²åå†è¿”å›
        polished = await self._polish_summary(final_summary, target_language)
        return self._format_summary_with_meta(polished, target_language, video_title)

    def _smart_chunk_text(self, text: str, max_chars_per_chunk: int = 3500) -> list:
        """æ™ºèƒ½åˆ†å—ï¼ˆå…ˆæ®µè½åå¥å­ï¼‰ï¼ŒæŒ‰å­—ç¬¦ä¸Šé™åˆ‡åˆ†ã€‚"""
        chunks = []
        paragraphs = [p for p in text.split('\n\n') if p.strip()]
        cur = ""
        for p in paragraphs:
            candidate = (cur + "\n\n" + p).strip() if cur else p
            if len(candidate) > max_chars_per_chunk and cur:
                chunks.append(cur.strip())
                cur = p
            else:
                cur = candidate
        if cur.strip():
            chunks.append(cur.strip())

        # äºŒæ¬¡æŒ‰å¥å­åˆ‡åˆ†è¿‡é•¿å—
        import re
        final_chunks = []
        for c in chunks:
            if len(c) <= max_chars_per_chunk:
                final_chunks.append(c)
            else:
                sentences = [s.strip() for s in re.split(r"[ã€‚ï¼ï¼Ÿ\.!?]+", c) if s.strip()]
                scur = ""
                for s in sentences:
                    candidate = (scur + 'ã€‚' + s).strip() if scur else s
                    if len(candidate) > max_chars_per_chunk and scur:
                        final_chunks.append(scur.strip())
                        scur = s
                    else:
                        scur = candidate
                if scur.strip():
                    final_chunks.append(scur.strip())
        return final_chunks

    async def _integrate_chunk_summaries(self, combined_summaries: str, target_language: str) -> str:
        """
        æ•´åˆåˆ†å—æ‘˜è¦ä¸ºæœ€ç»ˆè¿è´¯æ‘˜è¦
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        
        try:
            system_prompt = f"""ä½ æ˜¯å†…å®¹æ•´åˆä¸“å®¶ã€‚è¯·æŠŠå¤šæ®µâ€œç¼–å·æ‘˜è¦ï¼ˆPart 1..Nï¼‰â€æ•´åˆä¸ºä¸€ç¯‡æµç•…è¿è´¯ä¸”è¦†ç›–å……åˆ†çš„{language_name}æ€»ç»“ã€‚

å¿…é¡»æŒ‰ç¼–å·é¡ºåºè¦†ç›–æ‰€æœ‰éƒ¨åˆ†ï¼Œä¸å¾—è·³è¿‡å‰åŠæ®µï¼›åˆå¹¶ç›¸è¿‘è¦ç‚¹ã€æ¶ˆé™¤é‡å¤ä¸å†²çªï¼›å…è®¸è‡ªç„¶æ®µæˆ–å°‘é‡æ¡ç›®ï¼Œä½†ä¸è¦ä½¿ç”¨å°æ ‡é¢˜ï¼›ç¯‡å¹…å»ºè®®800-1500å­—ï¼Œä¿è¯è®ºç‚¹ä¸è®ºæ®å……åˆ†ã€‚"""

            user_prompt = f"""å°†ä»¥ä¸‹åˆ†æ®µæ‘˜è¦æ•´åˆä¸ºä¸€ç¯‡å®Œæ•´æ€»ç»“ï¼ˆè‡ªç„¶æ®µä¸ºä¸»ï¼Œå¯å°‘é‡æ¡ç›®ï¼›ä¸ä½¿ç”¨å°æ ‡é¢˜ï¼›800-1500å­—ï¼‰ï¼š

{combined_summaries}

è¯·ç¡®ä¿è¦†ç›–æ‰€æœ‰Partçš„å…³é”®ä¿¡æ¯ï¼Œè¯­è¨€è‡ªç„¶ã€ä¿¡æ¯å¯†åº¦é«˜ï¼Œåªè¾“å‡ºæ€»ç»“æ­£æ–‡ã€‚"""

            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=2500,  # æ§åˆ¶è¾“å‡ºè§„æ¨¡ï¼Œå…¼é¡¾ä¸Šä¸‹æ–‡å®‰å…¨
                temperature=0.3
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"æ•´åˆæ‘˜è¦å¤±è´¥: {e}")
            # å¤±è´¥æ—¶ç›´æ¥åˆå¹¶
            return combined_summaries

    def _format_summary_with_meta(self, summary: str, target_language: str, video_title: str = None) -> str:
        """
        ä¸ºæ‘˜è¦æ·»åŠ æ ‡é¢˜å’Œå…ƒä¿¡æ¯
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        meta_labels = self._get_summary_labels(target_language)
        
        # ä¸åŠ ä»»ä½•å°æ ‡é¢˜/å…è´£å£°æ˜ï¼Œå¯ä¿ç•™è§†é¢‘æ ‡é¢˜ä½œä¸ºä¸€çº§æ ‡é¢˜
        if video_title:
            prefix = f"# {video_title}\n\n"
        else:
            prefix = ""
        return prefix + summary

    async def _polish_summary(self, text: str, target_language: str) -> str:
        """å¯¹å·²ç”Ÿæˆçš„æ€»ç»“åšè½»åº¦æ¶¦è‰²ï¼šå»é‡ã€é¡ºåºæ›´è‡ªç„¶ã€æªè¾æ›´ç®€æ´ï¼Œä¸æ–°å¢äº‹å®ã€‚"""
        if not self.client:
            return text
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        system_prompt = f"""ä½ æ˜¯èµ„æ·±ç¼–è¾‘ã€‚ç”¨{language_name}å¯¹ç»™å®šæ€»ç»“åšè½»åº¦æ¶¦è‰²ï¼š
- ä¸æ”¹å˜äº‹å®ä¸ç»“è®ºï¼Œä¸æ–°å¢å†…å®¹ï¼›
- å»é™¤é‡å¤ã€å£è¯­åŒ–ä¸å†—ä½™ï¼›
- é¡ºåºæ›´è‡ªç„¶ï¼Œæªè¾æ›´ç®€æ´ï¼›
- ä¸æ·»åŠ å°æ ‡é¢˜æˆ–è£…é¥°æ€§æ ¼å¼ã€‚"""
        user_prompt = f"""æ¶¦è‰²ä»¥ä¸‹æ€»ç»“ï¼Œä½¿å…¶æ›´æµç•…ã€ç´§å‡‘ï¼š

{text}

åªè¿”å›æ¶¦è‰²åçš„æ­£æ–‡ã€‚"""
        try:
            resp = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                max_tokens=800,
                temperature=0.2,
            )
            return resp.choices[0].message.content
        except Exception:
            return text
    
    def _generate_fallback_summary(self, transcript: str, target_language: str, video_title: str = None) -> str:
        """
        ç”Ÿæˆå¤‡ç”¨æ‘˜è¦ï¼ˆå½“OpenAI APIä¸å¯ç”¨æ—¶ï¼‰
        
        Args:
            transcript: è½¬å½•æ–‡æœ¬
            video_title: è§†é¢‘æ ‡é¢˜
            target_language: ç›®æ ‡è¯­è¨€ä»£ç 
            
        Returns:
            å¤‡ç”¨æ‘˜è¦æ–‡æœ¬
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰")
        
        # ç®€å•çš„æ–‡æœ¬å¤„ç†ï¼Œæå–å…³é”®ä¿¡æ¯
        lines = transcript.split('\n')
        content_lines = [line for line in lines if line.strip() and not line.startswith('#') and not line.startswith('**')]
        
        # è®¡ç®—å¤§æ¦‚çš„é•¿åº¦
        total_chars = sum(len(line) for line in content_lines)
        
        # ä½¿ç”¨ç›®æ ‡è¯­è¨€çš„æ ‡ç­¾
        meta_labels = self._get_summary_labels(target_language)
        fallback_labels = self._get_fallback_labels(target_language)
        
        # ç›´æ¥ä½¿ç”¨è§†é¢‘æ ‡é¢˜ä½œä¸ºä¸»æ ‡é¢˜  
        title = video_title if video_title else "Summary"
        
        summary = f"""# {title}

**{meta_labels['language_label']}:** {language_name}
**{fallback_labels['notice']}:** {fallback_labels['api_unavailable']}



## {fallback_labels['overview_title']}

**{fallback_labels['content_length']}:** {fallback_labels['about']} {total_chars} {fallback_labels['characters']}
**{fallback_labels['paragraph_count']}:** {len(content_lines)} {fallback_labels['paragraphs']}

## {fallback_labels['main_content']}

{fallback_labels['content_description']}

{fallback_labels['suggestions_intro']}

1. {fallback_labels['suggestion_1']}
2. {fallback_labels['suggestion_2']}
3. {fallback_labels['suggestion_3']}

## {fallback_labels['recommendations']}

- {fallback_labels['recommendation_1']}
- {fallback_labels['recommendation_2']}


<br/>

<p style="color: #888; font-style: italic; text-align: center; margin-top: 16px;"><em>{fallback_labels['fallback_disclaimer']}</em></p>"""
        
        return summary
    
    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def get_supported_languages(self) -> dict:
        """
        è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
        
        Returns:
            è¯­è¨€ä»£ç åˆ°è¯­è¨€åç§°çš„æ˜ å°„
        """
        return self.language_map.copy()
    
    def _detect_transcript_language(self, transcript: str) -> str:
        """
        æ£€æµ‹è½¬å½•æ–‡æœ¬çš„ä¸»è¦è¯­è¨€
        
        Args:
            transcript: è½¬å½•æ–‡æœ¬
            
        Returns:
            æ£€æµ‹åˆ°çš„è¯­è¨€ä»£ç 
        """
        # ç®€å•çš„è¯­è¨€æ£€æµ‹é€»è¾‘ï¼šæŸ¥æ‰¾è½¬å½•æ–‡æœ¬ä¸­çš„è¯­è¨€æ ‡è®°
        if "**æ£€æµ‹è¯­è¨€:**" in transcript:
            # ä»Whisperè½¬å½•ä¸­æå–æ£€æµ‹åˆ°çš„è¯­è¨€
            lines = transcript.split('\n')
            for line in lines:
                if "**æ£€æµ‹è¯­è¨€:**" in line:
                    # æå–è¯­è¨€ä»£ç ï¼Œä¾‹å¦‚: "**æ£€æµ‹è¯­è¨€:** en"
                    lang = line.split(":")[-1].strip()
                    return lang
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯­è¨€æ ‡è®°ï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦æ£€æµ‹
        # è®¡ç®—è‹±æ–‡å­—ç¬¦ã€ä¸­æ–‡å­—ç¬¦ç­‰çš„æ¯”ä¾‹
        total_chars = len(transcript)
        if total_chars == 0:
            return "en"  # é»˜è®¤è‹±æ–‡
            
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = sum(1 for char in transcript if '\u4e00' <= char <= '\u9fff')
        chinese_ratio = chinese_chars / total_chars
        
        # ç»Ÿè®¡è‹±æ–‡å­—æ¯
        english_chars = sum(1 for char in transcript if char.isascii() and char.isalpha())
        english_ratio = english_chars / total_chars
        
        # æ ¹æ®æ¯”ä¾‹åˆ¤æ–­
        if chinese_ratio > 0.3:
            return "zh"
        elif english_ratio > 0.3:
            return "en"
        else:
            return "en"  # é»˜è®¤è‹±æ–‡
    
    def _get_language_instruction(self, lang_code: str) -> str:
        """
        æ ¹æ®è¯­è¨€ä»£ç è·å–ä¼˜åŒ–æŒ‡ä»¤ä¸­ä½¿ç”¨çš„è¯­è¨€åç§°
        
        Args:
            lang_code: è¯­è¨€ä»£ç 
            
        Returns:
            è¯­è¨€åç§°
        """
        language_instructions = {
            "en": "English",
            "zh": "ä¸­æ–‡",
            "ja": "æ—¥æœ¬èª",
            "ko": "í•œêµ­ì–´",
            "es": "EspaÃ±ol",
            "fr": "FranÃ§ais",
            "de": "Deutsch",
            "it": "Italiano",
            "pt": "PortuguÃªs",
            "ru": "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
            "ar": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
        }
        return language_instructions.get(lang_code, "English")
    

    def _get_summary_labels(self, lang_code: str) -> dict:
        """
        è·å–æ‘˜è¦é¡µé¢çš„å¤šè¯­è¨€æ ‡ç­¾
        
        Args:
            lang_code: è¯­è¨€ä»£ç 
            
        Returns:
            æ ‡ç­¾å­—å…¸
        """
        labels = {
            "en": {
                "language_label": "Summary Language",
                "disclaimer": "This summary is automatically generated by AI for reference only"
            },
            "zh": {
                "language_label": "æ‘˜è¦è¯­è¨€",
                "disclaimer": "æœ¬æ‘˜è¦ç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ"
            },
            "ja": {
                "language_label": "è¦ç´„è¨€èª",
                "disclaimer": "ã“ã®è¦ç´„ã¯AIã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¦ãŠã‚Šã€å‚è€ƒç”¨ã§ã™"
            },
            "ko": {
                "language_label": "ìš”ì•½ ì–¸ì–´",
                "disclaimer": "ì´ ìš”ì•½ì€ AIì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìœ¼ë©° ì°¸ê³ ìš©ì…ë‹ˆë‹¤"
            },
            "es": {
                "language_label": "Idioma del Resumen",
                "disclaimer": "Este resumen es generado automÃ¡ticamente por IA, solo para referencia"
            },
            "fr": {
                "language_label": "Langue du RÃ©sumÃ©",
                "disclaimer": "Ce rÃ©sumÃ© est gÃ©nÃ©rÃ© automatiquement par IA, Ã  titre de rÃ©fÃ©rence uniquement"
            },
            "de": {
                "language_label": "Zusammenfassungssprache",
                "disclaimer": "Diese Zusammenfassung wird automatisch von KI generiert, nur zur Referenz"
            },
            "it": {
                "language_label": "Lingua del Riassunto",
                "disclaimer": "Questo riassunto Ã¨ generato automaticamente dall'IA, solo per riferimento"
            },
            "pt": {
                "language_label": "Idioma do Resumo",
                "disclaimer": "Este resumo Ã© gerado automaticamente por IA, apenas para referÃªncia"
            },
            "ru": {
                "language_label": "Ğ¯Ğ·Ñ‹Ğº Ñ€ĞµĞ·ÑĞ¼Ğµ",
                "disclaimer": "Ğ­Ñ‚Ğ¾ Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ˜Ğ˜, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ¸"
            },
            "ar": {
                "language_label": "Ù„ØºØ© Ø§Ù„Ù…Ù„Ø®Øµ",
                "disclaimer": "Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ø®Øµ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ù„Ù„Ù…Ø±Ø¬Ø¹ ÙÙ‚Ø·"
            }
        }
        return labels.get(lang_code, labels["en"])
    
    def _get_fallback_labels(self, lang_code: str) -> dict:
        """
        è·å–å¤‡ç”¨æ‘˜è¦çš„å¤šè¯­è¨€æ ‡ç­¾
        
        Args:
            lang_code: è¯­è¨€ä»£ç 
            
        Returns:
            æ ‡ç­¾å­—å…¸
        """
        labels = {
            "en": {
                "notice": "Notice",
                "api_unavailable": "OpenAI API is unavailable, this is a simplified summary",
                "overview_title": "Transcript Overview",
                "content_length": "Content Length",
                "about": "About",
                "characters": "characters",
                "paragraph_count": "Paragraph Count",
                "paragraphs": "paragraphs",
                "main_content": "Main Content",
                "content_description": "The transcript contains complete video speech content. Since AI summary cannot be generated currently, we recommend:",
                "suggestions_intro": "For detailed information, we suggest you:",
                "suggestion_1": "Review the complete transcript text for detailed information",
                "suggestion_2": "Focus on important paragraphs marked with timestamps",
                "suggestion_3": "Manually extract key points and takeaways",
                "recommendations": "Recommendations",
                "recommendation_1": "Configure OpenAI API key for better summary functionality",
                "recommendation_2": "Or use other AI services for text summarization",
                "fallback_disclaimer": "This is an automatically generated fallback summary"
            },
            "zh": {
                "notice": "æ³¨æ„",
                "api_unavailable": "ç”±äºOpenAI APIä¸å¯ç”¨ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ‘˜è¦",
                "overview_title": "è½¬å½•æ¦‚è§ˆ",
                "content_length": "å†…å®¹é•¿åº¦",
                "about": "çº¦",
                "characters": "å­—ç¬¦",
                "paragraph_count": "æ®µè½æ•°é‡",
                "paragraphs": "æ®µ",
                "main_content": "ä¸»è¦å†…å®¹",
                "content_description": "è½¬å½•æ–‡æœ¬åŒ…å«äº†å®Œæ•´çš„è§†é¢‘è¯­éŸ³å†…å®¹ã€‚ç”±äºå½“å‰æ— æ³•ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ï¼Œå»ºè®®æ‚¨ï¼š",
                "suggestions_intro": "ä¸ºè·å–è¯¦ç»†ä¿¡æ¯ï¼Œå»ºè®®æ‚¨ï¼š",
                "suggestion_1": "æŸ¥çœ‹å®Œæ•´çš„è½¬å½•æ–‡æœ¬ä»¥è·å–è¯¦ç»†ä¿¡æ¯",
                "suggestion_2": "å…³æ³¨æ—¶é—´æˆ³æ ‡è®°çš„é‡è¦æ®µè½",
                "suggestion_3": "æ‰‹åŠ¨æå–å…³é”®è§‚ç‚¹å’Œè¦ç‚¹",
                "recommendations": "å»ºè®®",
                "recommendation_1": "é…ç½®OpenAI APIå¯†é’¥ä»¥è·å¾—æ›´å¥½çš„æ‘˜è¦åŠŸèƒ½",
                "recommendation_2": "æˆ–è€…ä½¿ç”¨å…¶ä»–AIæœåŠ¡è¿›è¡Œæ–‡æœ¬æ€»ç»“",
                "fallback_disclaimer": "æœ¬æ‘˜è¦ä¸ºè‡ªåŠ¨ç”Ÿæˆçš„å¤‡ç”¨ç‰ˆæœ¬"
            }
        }
        return labels.get(lang_code, labels["en"])
    
    def is_available(self) -> bool:
        """
        æ£€æŸ¥æ‘˜è¦æœåŠ¡æ˜¯å¦å¯ç”¨
        
        Returns:
            True if OpenAI API is configured, False otherwise
        """
        return self.client is not None
