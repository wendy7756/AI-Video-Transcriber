# =============================================================================
# backend/summarizer.py - æ™ºæ…§æ–‡å­—æ‘˜è¦èˆ‡è½‰éŒ„æœ€ä½³åŒ–æ¨¡çµ„
# =============================================================================
# æœ¬æ¨¡çµ„æ˜¯ AI-Video-Transcriber å°ˆæ¡ˆçš„æ ¸å¿ƒæ–‡å­—è™•ç†å…ƒä»¶ï¼Œå°ˆé–€è™•ç†å½±ç‰‡è½‰éŒ„æ–‡æœ¬çš„æ™ºæ…§æœ€ä½³åŒ–å’Œæ‘˜è¦ç”¢ç”Ÿã€‚
#
# ä¸»è¦åŠŸèƒ½ï¼š
# 1. å¤šèªè¨€è½‰éŒ„æ–‡æœ¬æœ€ä½³åŒ–
#    - æ™ºæ…§ä¿®æ­£éŒ¯åˆ¥å­—å’Œèªæ³•
#    - é‡çµ„ä¸å®Œæ•´å¥å­
#    - æŒ‰èªæ„é‡æ–°åˆ†æ®µ
#
# 2. AIæ‘˜è¦ç”¢ç”Ÿ
#    - æ”¯æ´å¤šç¨®èªè¨€çš„æ™ºæ…§æ‘˜è¦
#    - é•·æ–‡æœ¬åˆ†å¡Šè™•ç†
#    - ä¿ç•™åŸæ–‡èªå¢ƒå’Œé—œéµè³‡è¨Š
#
# 3. è½‰éŒ„æ–‡æœ¬è™•ç†ç­–ç•¥
#    - æ”¯æ´ä¸åŒèªè¨€çš„æ–‡æœ¬è™•ç†
#    - æ™ºæ…§åˆ†å¡Šå’Œä¸Šä¸‹æ–‡ä¿ç•™
#    - è™•ç†å¤šèªè¨€è½‰éŒ„çš„è¤‡é›œæ€§
#
# ä¾è³´ï¼š
#    - OpenAI APIï¼ˆGPT-3.5/GPT-4oï¼‰
#    - æ”¯æ´å¤šç¨®èªè¨€çš„è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“
#
# è¨­è¨ˆåŸå‰‡ï¼š
#    - ä¿æŒåŸæ–‡æ„åœ–å’Œèªå¢ƒ
#    - æœ€å¤§ç¨‹åº¦ä¿ç•™èªªè©±è€…åŸå§‹è¡¨é”
#    - æ™ºæ…§ä¸”äººæ€§åŒ–çš„æ–‡æœ¬æœ€ä½³åŒ–
# =============================================================================

import os
import openai
import logging
from typing import Optional

logger = logging.getLogger(__name__)


class Summarizer:
    """æ–‡å­—æ‘˜è¦å™¨ï¼Œä½¿ç”¨OpenAI APIç”¢ç”Ÿå¤šèªè¨€æ‘˜è¦"""

    def __init__(self):
        """
        åˆå§‹åŒ–æ‘˜è¦å™¨ã€‚

        å¾ç’°å¢ƒè®Šæ•¸ç²å–OpenAI APIè¨­å®šï¼Œå¦‚æœæœªè¨­å®šAPIé‡‘é‘°å‰‡ç„¡æ³•ä½¿ç”¨æ‘˜è¦åŠŸèƒ½ã€‚
        """
        # å¾ç’°å¢ƒè®Šæ•¸ç²å–OpenAI APIè¨­å®š
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_BASE_URL")

        if not api_key:
            logger.warning("æœªè¨­å®šOPENAI_API_KEYç’°å¢ƒè®Šæ•¸ï¼Œå°‡ç„¡æ³•ä½¿ç”¨æ‘˜è¦åŠŸèƒ½")

        if api_key:
            if base_url:
                self.client = openai.OpenAI(api_key=api_key, base_url=base_url)
                logger.info(f"OpenAIå®¢æˆ¶ç«¯å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨è‡ªè¨‚ç«¯é»: {base_url}")
            else:
                self.client = openai.OpenAI(api_key=api_key)
                logger.info("OpenAIå®¢æˆ¶ç«¯å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨é è¨­ç«¯é»")
        else:
            self.client = None

        # å¾ç’°å¢ƒè®Šæ•¸è®€å–æ¨¡å‹ï¼Œé è¨­ç‚º gpt-4o-mini
        self.model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.optimization_model = os.getenv("OPENAI_OPTIMIZATION_MODEL", "gpt-3.5-turbo")
        logger.info(f"ä½¿ç”¨ OpenAI æ‘˜è¦æ¨¡å‹: {self.model}")
        logger.info(f"ä½¿ç”¨ OpenAI æœ€ä½³åŒ–æ¨¡å‹: {self.optimization_model}")

        # æ”¯æ´çš„èªè¨€å°æ‡‰
        self.language_map = {
            "en": "English",
            "zh": "ä¸­æ–‡ï¼ˆç¹é«”ï¼‰",
            "es": "EspaÃ±ol",
            "fr": "FranÃ§ais",
            "de": "Deutsch",
            "it": "Italiano",
            "pt": "PortuguÃªs",
            "ru": "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
            "ja": "æ—¥æœ¬èª",
            "ko": "í•œêµ­ì–´",
            "ar": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
        }

    async def optimize_transcript(self, raw_transcript: str) -> str:
        """
        æœ€ä½³åŒ–è½‰éŒ„æ–‡æœ¬ï¼šä¿®æ­£éŒ¯åˆ¥å­—ï¼ŒæŒ‰èªæ„åˆ†æ®µ
        æ”¯æ´é•·æ–‡æœ¬åˆ†å¡Šè™•ç†

        Args:
            raw_transcript: åŸå§‹è½‰éŒ„æ–‡æœ¬

        Returns:
            æœ€ä½³åŒ–å¾Œçš„è½‰éŒ„æ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        try:
            if not self.client:
                logger.warning("OpenAI APIä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹è½‰éŒ„")
                return raw_transcript

            # é è™•ç†ï¼šåƒ…ç§»é™¤æ™‚é–“æˆ³èˆ‡å…ƒè³‡è¨Šï¼Œä¿ç•™å…¨éƒ¨å£èª/é‡è¤‡å…§å®¹
            preprocessed = self._remove_timestamps_and_meta(raw_transcript)
            # ä½¿ç”¨JSç­–ç•¥ï¼šæŒ‰å­—å…ƒé•·åº¦åˆ†å¡Šï¼ˆæ›´è²¼è¿‘tokensä¸Šé™ï¼Œé¿å…ä¼°ç®—èª¤å·®ï¼‰
            detected_lang_code = self._detect_transcript_language(preprocessed)
            max_chars_per_chunk = 4000  # å°é½ŠJSï¼šæ¯å¡Šæœ€å¤§ç´„4000å­—å…ƒ

            if len(preprocessed) > max_chars_per_chunk:
                logger.info(f"æ–‡æœ¬è¼ƒé•·({len(preprocessed)} chars)ï¼Œå•Ÿç”¨åˆ†å¡Šæœ€ä½³åŒ–")
                return await self._format_long_transcript_in_chunks(preprocessed, detected_lang_code, max_chars_per_chunk)
            else:
                return await self._format_single_chunk(preprocessed, detected_lang_code)

        except Exception as e:
            logger.error(f"æœ€ä½³åŒ–è½‰éŒ„æ–‡æœ¬å¤±æ•—: {str(e)}")
            logger.info("è¿”å›åŸå§‹è½‰éŒ„æ–‡æœ¬")
            return raw_transcript

    def _estimate_tokens(self, text: str) -> int:
        """
        æ”¹é€²çš„tokenæ•¸é‡ä¼°ç®—æ¼”ç®—æ³•
        æ›´ä¿å®ˆçš„ä¼°ç®—ï¼Œè€ƒé‡ç³»çµ±promptå’Œæ ¼å¼åŒ–é–‹éŠ·
        """
        # æ›´ä¿å®ˆçš„ä¼°ç®—ï¼šè€ƒé‡å¯¦éš›ä½¿ç”¨ä¸­çš„tokenè†¨è„¹
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_words = len([word for word in text.split()
                            if word.isascii() and word.isalpha()])

        # è¨ˆç®—åŸºç¤tokens
        base_tokens = chinese_chars * 1.5 + english_words * 1.3

        # è€ƒé‡markdownæ ¼å¼ã€æ™‚é–“æˆ³ç­‰é–‹éŠ·ï¼ˆç´„30%é¡å¤–é–‹éŠ·ï¼‰
        format_overhead = len(text) * 0.15

        # è€ƒé‡ç³»çµ±prompté–‹éŠ·ï¼ˆç´„2000-3000 tokensï¼‰
        system_prompt_overhead = 2500

        total_estimated = int(
            base_tokens + format_overhead + system_prompt_overhead)

        return total_estimated

    async def _optimize_single_chunk(self, raw_transcript: str) -> str:
        """
        æœ€ä½³åŒ–å–®å€‹æ–‡æœ¬å¡Š
        """
        detected_lang = self._detect_transcript_language(raw_transcript)
        lang_instruction = self._get_language_instruction(detected_lang)

        system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡å­—ç·¨è¼¯å°ˆå®¶ã€‚è«‹å°æä¾›çš„å½±ç‰‡è½‰éŒ„æ–‡å­—é€²è¡Œæœ€ä½³åŒ–è™•ç†ã€‚

ç‰¹åˆ¥æ³¨æ„ï¼šé€™å¯èƒ½æ˜¯è¨ªè«‡ã€å°è©±æˆ–æ¼”è¬›ï¼Œå¦‚æœåŒ…å«å¤šå€‹èªªè©±è€…ï¼Œå¿…é ˆä¿æŒæ¯å€‹èªªè©±è€…çš„åŸå§‹è¦–è§’ã€‚

è¦æ±‚ï¼š
1. **åš´æ ¼ä¿æŒåŸå§‹èªè¨€({lang_instruction})ï¼Œçµ•å°ä¸è¦ç¿»è­¯æˆå…¶ä»–èªè¨€**
2. **å®Œå…¨ç§»é™¤æ‰€æœ‰æ™‚é–“æˆ³æ¨™è¨˜ï¼ˆå¦‚ [00:00 - 00:05]ï¼‰**
3. **æ™ºæ…§è­˜åˆ¥å’Œé‡çµ„è¢«æ™‚é–“æˆ³æ‹†åˆ†çš„å®Œæ•´å¥å­**ï¼Œèªæ³•ä¸Šä¸å®Œæ•´çš„å¥å­ç‰‡æ®µéœ€è¦èˆ‡ä¸Šä¸‹æ–‡åˆä½µ
4. ä¿®æ­£æ˜é¡¯çš„éŒ¯åˆ¥å­—å’Œèªæ³•éŒ¯èª¤
5. å°‡é‡çµ„å¾Œçš„å®Œæ•´å¥å­æŒ‰ç…§èªæ„å’Œé‚è¼¯å«æ„åˆ†æˆè‡ªç„¶çš„æ®µè½
6. æ®µè½ä¹‹é–“ç”¨ç©ºè¡Œåˆ†éš”
7. **åš´æ ¼ä¿æŒåŸæ„ä¸è®Šï¼Œä¸è¦æ–°å¢æˆ–åˆªé™¤å¯¦éš›å…§å®¹**
8. **çµ•å°ä¸è¦æ”¹è®Šäººç¨±ä»£è©ï¼ˆå¦‚I/æˆ‘ã€you/ä½ ã€he/ä»–ã€she/å¥¹ç­‰ï¼‰**
9. **ä¿æŒæ¯å€‹èªªè©±è€…çš„åŸå§‹è¦–è§’å’Œèªå¢ƒ**
10. **è­˜åˆ¥å°è©±çµæ§‹ï¼šè¨ªè«‡è€…ç”¨"you"ï¼Œå—è¨ªè€…ç”¨"I/we"ï¼Œçµ•ä¸æ··æ·†**
11. ç¢ºä¿æ¯å€‹å¥å­èªæ³•å®Œæ•´ï¼Œèªè¨€æµæš¢è‡ªç„¶

è™•ç†ç­–ç•¥ï¼š
- å„ªå…ˆè­˜åˆ¥ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µï¼ˆå¦‚ä»¥ä»‹è©ã€é€£æ¥è©ã€å½¢å®¹è©çµå°¾ï¼‰
- æª¢è¦–ç›¸é„°çš„æ–‡æœ¬ç‰‡æ®µï¼Œåˆä½µå½¢æˆå®Œæ•´å¥å­
- é‡æ–°æ–·å¥ï¼Œç¢ºä¿æ¯å¥è©±èªæ³•å®Œæ•´
- æŒ‰ä¸»é¡Œå’Œé‚è¼¯é‡æ–°åˆ†æ®µ

åˆ†æ®µè¦æ±‚ï¼š
- æŒ‰ä¸»é¡Œå’Œé‚è¼¯å«æ„åˆ†æ®µï¼Œæ¯æ®µåŒ…å«1-8å€‹ç›¸é—œå¥å­
- å–®æ®µé•·åº¦ä¸è¶…é400å­—å…ƒ
- é¿å…éå¤šçš„çŸ­æ®µè½ï¼Œåˆä½µç›¸é—œå…§å®¹
- ç•¶ä¸€å€‹å®Œæ•´æƒ³æ³•æˆ–è§€é»è¡¨é”å¾Œåˆ†æ®µ

è¼¸å‡ºæ ¼å¼ï¼š
- ç´”æ–‡æœ¬æ®µè½ï¼Œç„¡æ™‚é–“æˆ³æˆ–æ ¼å¼æ¨™è¨˜
- æ¯å€‹å¥å­çµæ§‹å®Œæ•´
- æ¯å€‹æ®µè½è¨è«–ä¸€å€‹ä¸»è¦è©±é¡Œ
- æ®µè½ä¹‹é–“ç”¨ç©ºè¡Œåˆ†éš”

é‡è¦æé†’ï¼šé€™æ˜¯{lang_instruction}å…§å®¹ï¼Œè«‹å®Œå…¨ç”¨{lang_instruction}é€²è¡Œæœ€ä½³åŒ–ï¼Œé‡é»è§£æ±ºå¥å­è¢«æ™‚é–“æˆ³æ‹†åˆ†å°è‡´çš„ä¸é€£è²«å•é¡Œï¼å‹™å¿…é€²è¡Œåˆç†çš„åˆ†æ®µï¼Œé¿å…å‡ºç¾è¶…é•·æ®µè½ï¼

**é—œéµè¦æ±‚ï¼šé€™å¯èƒ½æ˜¯è¨ªè«‡å°è©±ï¼Œçµ•å°ä¸è¦æ”¹è®Šä»»ä½•äººç¨±ä»£è©æˆ–èªªè©±è€…è¦–è§’ï¼è¨ªè«‡è€…ç”¨"you"ï¼Œå—è¨ªè€…ç”¨"I/we"ï¼Œå¿…é ˆåš´æ ¼ä¿æŒï¼**"""

        user_prompt = f"""è«‹å°‡ä»¥ä¸‹{lang_instruction}å½±ç‰‡è½‰éŒ„æ–‡æœ¬æœ€ä½³åŒ–ç‚ºæµæš¢çš„æ®µè½æ–‡æœ¬ï¼š

{raw_transcript}

é‡é»ä»»å‹™ï¼š
1. ç§»é™¤æ‰€æœ‰æ™‚é–“æˆ³æ¨™è¨˜
2. è­˜åˆ¥ä¸¦é‡çµ„è¢«æ‹†åˆ†çš„å®Œæ•´å¥å­
3. ç¢ºä¿æ¯å€‹å¥å­èªæ³•å®Œæ•´ã€èªæ„é€£è²«
4. æŒ‰èªæ„é‡æ–°åˆ†æ®µï¼Œæ®µè½é–“ç©ºè¡Œåˆ†éš”
5. ä¿æŒ{lang_instruction}èªè¨€ä¸è®Š

åˆ†æ®µæŒ‡å°ï¼š
- æŒ‰ä¸»é¡Œå’Œé‚è¼¯å«æ„åˆ†æ®µï¼Œæ¯æ®µåŒ…å«1-8å€‹ç›¸é—œå¥å­
- å–®æ®µé•·åº¦ä¸è¶…é400å­—å…ƒ
- é¿å…éå¤šçš„çŸ­æ®µè½ï¼Œåˆä½µç›¸é—œå…§å®¹
- ç¢ºä¿æ®µè½ä¹‹é–“æœ‰æ˜ç¢ºçš„ç©ºè¡Œ

è«‹ç‰¹åˆ¥æ³¨æ„ä¿®å¾©å› æ™‚é–“æˆ³åˆ†å‰²å°è‡´çš„å¥å­ä¸å®Œæ•´å•é¡Œï¼Œä¸¦é€²è¡Œåˆç†çš„æ®µè½åŠƒåˆ†ï¼"""

        response = self.client.chat.completions.create(
            model=self.optimization_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=4000,  # å°é½ŠJSï¼šæœ€ä½³åŒ–/æ ¼å¼åŒ–éšæ®µæœ€å¤§tokensâ‰ˆ4000
            temperature=0.1
        )

        return response.choices[0].message.content

    async def _optimize_with_chunks(self, raw_transcript: str, max_tokens: int) -> str:
        """
        åˆ†å¡Šæœ€ä½³åŒ–é•·æ–‡æœ¬
        """
        detected_lang = self._detect_transcript_language(raw_transcript)
        lang_instruction = self._get_language_instruction(detected_lang)

        # æŒ‰æ®µè½åˆ†å‰²åŸå§‹è½‰éŒ„ï¼ˆä¿ç•™æ™‚é–“æˆ³ä½œç‚ºåˆ†å‰²åƒè€ƒï¼‰
        chunks = self._split_into_chunks(raw_transcript, max_tokens)
        logger.info(f"åˆ†å‰²ç‚º {len(chunks)} å€‹å¡Šé€²è¡Œè™•ç†")

        optimized_chunks = []

        for i, chunk in enumerate(chunks):
            logger.info(f"æ­£åœ¨æœ€ä½³åŒ–ç¬¬ {i+1}/{len(chunks)} å¡Š...")

            system_prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„æ–‡æœ¬ç·¨è¼¯å°ˆå®¶ã€‚è«‹å°é€™æ®µè½‰éŒ„æ–‡æœ¬ç‰‡æ®µé€²è¡Œç°¡å–®æœ€ä½³åŒ–ã€‚

é€™æ˜¯å®Œæ•´è½‰éŒ„çš„ç¬¬{i+1}éƒ¨åˆ†ï¼Œå…±{len(chunks)}éƒ¨åˆ†ã€‚

ç°¡å–®æœ€ä½³åŒ–è¦æ±‚ï¼š
1. **åš´æ ¼ä¿æŒåŸå§‹èªè¨€({lang_instruction})**ï¼Œçµ•å°ä¸ç¿»è­¯
2. **åƒ…ä¿®æ­£æ˜é¡¯çš„éŒ¯åˆ¥å­—å’Œèªæ³•éŒ¯èª¤**
3. **ç¨å¾®èª¿æ•´å¥å­æµæš¢åº¦**ï¼Œä½†ä¸å¤§å¹…æ”¹å¯«
4. **ä¿æŒåŸæ–‡çµæ§‹å’Œé•·åº¦**ï¼Œä¸åšè¤‡é›œçš„æ®µè½é‡çµ„
5. **ä¿æŒåŸæ„100%ä¸è®Š**

æ³¨æ„ï¼šé€™åªæ˜¯åˆæ­¥æ¸…ç†ï¼Œä¸è¦åšè¤‡é›œçš„é‡å¯«æˆ–é‡æ–°çµ„ç¹”ã€‚"""

            user_prompt = f"""ç°¡å–®æœ€ä½³åŒ–ä»¥ä¸‹{lang_instruction}æ–‡æœ¬ç‰‡æ®µï¼ˆåƒ…ä¿®éŒ¯åˆ¥å­—å’Œèªæ³•ï¼‰ï¼š

{chunk}

è¼¸å‡ºæ¸…ç†å¾Œçš„æ–‡æœ¬ï¼Œä¿æŒåŸæ–‡çµæ§‹ã€‚"""

            try:
                response = self.client.chat.completions.create(
                    model=self.optimization_model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=1200,  # é©æ‡‰4000 tokensç¸½é™åˆ¶
                    temperature=0.1
                )

                optimized_chunk = response.choices[0].message.content
                optimized_chunks.append(optimized_chunk)

            except Exception as e:
                logger.error(f"æœ€ä½³åŒ–ç¬¬ {i+1} å¡Šå¤±æ•—: {e}")
                # å¤±æ•—æ™‚ä½¿ç”¨åŸºæœ¬æ¸…ç†
                cleaned_chunk = self._basic_transcript_cleanup(chunk)
                optimized_chunks.append(cleaned_chunk)

        # åˆä½µæ‰€æœ‰æœ€ä½³åŒ–å¾Œçš„å¡Š
        merged_text = "\n\n".join(optimized_chunks)

        # å°åˆä½µå¾Œçš„æ–‡æœ¬é€²è¡ŒäºŒæ¬¡æ®µè½æ•´ç†
        logger.info("æ­£åœ¨é€²è¡Œæœ€çµ‚æ®µè½æ•´ç†...")
        final_result = await self._final_paragraph_organization(merged_text, lang_instruction)

        logger.info("åˆ†å¡Šæœ€ä½³åŒ–å®Œæˆ")
        return final_result

    # ===== JS openaiService.js ç§»æ¤ï¼šåˆ†å¡Š/ä¸Šä¸‹æ–‡/å»é‡/æ ¼å¼åŒ– =====

    def _ensure_markdown_paragraphs(self, text: str) -> str:
        """ç¢ºä¿Markdownæ®µè½ç©ºè¡Œã€æ¨™é¡Œå¾Œç©ºè¡Œã€å£“ç¸®å¤šé¤˜ç©ºè¡Œã€‚"""
        if not text:
            return text
        formatted = text.replace("\r\n", "\n")
        import re
        # æ¨™é¡Œå¾ŒåŠ ç©ºè¡Œ
        formatted = re.sub(
            r"(^#{1,6}\s+.*)\n([^\n#])", r"\1\n\n\2", formatted, flags=re.M)
        # å£“ç¸®â‰¥3å€‹æ›è¡Œç‚º2å€‹
        formatted = re.sub(r"\n{3,}", "\n\n", formatted)
        # å»é¦–å°¾ç©ºè¡Œ
        formatted = re.sub(r"^\n+", "", formatted)
        formatted = re.sub(r"\n+$", "", formatted)
        return formatted

    async def _format_single_chunk(self, chunk_text: str, transcript_language: str = 'zh') -> str:
        """å–®å¡Šæœ€ä½³åŒ–ï¼ˆä¿®æ­£+æ ¼å¼åŒ–ï¼‰ï¼Œéµå¾ª4000 tokens é™åˆ¶ã€‚"""
        # å»ºæ§‹èˆ‡JSç‰ˆä¸€è‡´çš„ç³»çµ±/ä½¿ç”¨è€…æç¤º
        if transcript_language == 'zh':
            prompt = (
                "è«‹å°ä»¥ä¸‹éŸ³è¨Šè½‰éŒ„æ–‡æœ¬é€²è¡Œæ™ºæ…§æœ€ä½³åŒ–å’Œæ ¼å¼åŒ–ï¼Œè¦æ±‚ï¼š\n\n"
                "**å…§å®¹æœ€ä½³åŒ–ï¼ˆæ­£ç¢ºæ€§å„ªå…ˆï¼‰ï¼š**\n"
                "1. éŒ¯èª¤ä¿®æ­£ï¼ˆè½‰éŒ„éŒ¯èª¤/éŒ¯åˆ¥å­—/åŒéŸ³å­—/å°ˆæœ‰åè©ï¼‰\n"
                "2. é©åº¦æ”¹å–„èªæ³•ï¼Œè£œå…¨ä¸å®Œæ•´å¥å­ï¼Œä¿æŒåŸæ„å’Œèªè¨€ä¸è®Š\n"
                "3. å£èªè™•ç†ï¼šä¿ç•™è‡ªç„¶å£èªèˆ‡é‡è¤‡è¡¨é”ï¼Œä¸è¦åˆªæ¸›å…§å®¹ï¼Œåƒ…æ–°å¢å¿…è¦æ¨™é»\n"
                "4. **çµ•å°ä¸è¦æ”¹è®Šäººç¨±ä»£è©ï¼ˆI/æˆ‘ã€you/ä½ ç­‰ï¼‰å’Œèªªè©±è€…è¦–è§’**\n\n"
                "**åˆ†æ®µè¦å‰‡ï¼š**\n"
                "- æŒ‰ä¸»é¡Œå’Œé‚è¼¯å«æ„åˆ†æ®µï¼Œæ¯æ®µåŒ…å«1-8å€‹ç›¸é—œå¥å­\n"
                "- å–®æ®µé•·åº¦ä¸è¶…é400å­—å…ƒ\n"
                "- é¿å…éå¤šçš„çŸ­æ®µè½ï¼Œåˆä½µç›¸é—œå…§å®¹\n\n"
                "**æ ¼å¼è¦æ±‚ï¼š**Markdown æ®µè½ï¼Œæ®µè½é–“ç©ºè¡Œ\n\n"
                f"åŸå§‹è½‰éŒ„æ–‡æœ¬ï¼š\n{chunk_text}"
            )
            system_prompt = (
                "ä½ æ˜¯å°ˆæ¥­çš„éŸ³è¨Šè½‰éŒ„æ–‡æœ¬æœ€ä½³åŒ–åŠ©æ‰‹ï¼Œä¿®æ­£éŒ¯èª¤ã€æ”¹å–„é€šé †åº¦å’Œæ’ç‰ˆæ ¼å¼ï¼Œ"
                "å¿…é ˆä¿æŒåŸæ„ï¼Œä¸å¾—åˆªæ¸›å£èª/é‡è¤‡/ç´°ç¯€ï¼›åƒ…ç§»é™¤æ™‚é–“æˆ³æˆ–å…ƒè³‡è¨Šã€‚"
                "çµ•å°ä¸è¦æ”¹è®Šäººç¨±ä»£è©æˆ–èªªè©±è€…è¦–è§’ã€‚é€™å¯èƒ½æ˜¯è¨ªè«‡å°è©±ï¼Œè¨ªè«‡è€…ç”¨'you'ï¼Œå—è¨ªè€…ç”¨'I/we'ã€‚"
            )
        else:
            prompt = (
                "Please intelligently optimize and format the following audio transcript text:\n\n"
                "Content Optimization (Accuracy First):\n"
                "1. Error Correction (typos, homophones, proper nouns)\n"
                "2. Moderate grammar improvement, complete incomplete sentences, keep original language/meaning\n"
                "3. Speech processing: keep natural fillers and repetitions, do NOT remove content; only add punctuation if needed\n"
                "4. **NEVER change pronouns (I, you, he, she, etc.) or speaker perspective**\n\n"
                "Segmentation Rules: Group 1-8 related sentences per paragraph by topic/logic; paragraph length NOT exceed 400 characters; avoid too many short paragraphs\n\n"
                "Format: Markdown paragraphs with blank lines between paragraphs\n\n"
                f"Original transcript text:\n{chunk_text}"
            )
            system_prompt = (
                "You are a professional transcript formatting assistant. Fix errors and improve fluency "
                "without changing meaning or removing any content; only timestamps/meta may be removed; keep Markdown paragraphs with blank lines. "
                "NEVER change pronouns or speaker perspective. This may be an interview: interviewer uses 'you', interviewee uses 'I/we'."
            )

        try:
            response = self.client.chat.completions.create(
                model=self.optimization_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=4000,  # å°é½ŠJSï¼šæœ€ä½³åŒ–/æ ¼å¼åŒ–éšæ®µæœ€å¤§tokensâ‰ˆ4000
                temperature=0.1
            )
            optimized_text = response.choices[0].message.content or ""
            # ç§»é™¤è«¸å¦‚ "# Transcript" / "## Transcript" ç­‰æ¨™é¡Œ
            optimized_text = self._remove_transcript_heading(optimized_text)
            enforced = self._enforce_paragraph_max_chars(
                optimized_text.strip(), max_chars=400)
            return self._ensure_markdown_paragraphs(enforced)
        except Exception as e:
            logger.error(f"å–®å¡Šæ–‡æœ¬æœ€ä½³åŒ–å¤±æ•—: {e}")
            return self._apply_basic_formatting(chunk_text)

    def _smart_split_long_chunk(self, text: str, max_chars_per_chunk: int) -> list:
        """åœ¨å¥å­/ç©ºæ ¼é‚Šç•Œè™•å®‰å…¨åˆ‡åˆ†è¶…é•·æ–‡æœ¬ã€‚"""
        chunks = []
        pos = 0
        while pos < len(text):
            end = min(pos + max_chars_per_chunk, len(text))
            if end < len(text):
                # å„ªå…ˆå¥å­é‚Šç•Œ
                sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
                best = -1
                for ch in sentence_endings:
                    idx = text.rfind(ch, pos, end)
                    if idx > best:
                        best = idx
                if best > pos + int(max_chars_per_chunk * 0.7):
                    end = best + 1
                else:
                    # æ¬¡é¸ï¼šç©ºæ ¼é‚Šç•Œ
                    space_idx = text.rfind(' ', pos, end)
                    if space_idx > pos + int(max_chars_per_chunk * 0.8):
                        end = space_idx
            chunks.append(text[pos:end].strip())
            pos = end
        return [c for c in chunks if c]

    def _find_safe_cut_point(self, text: str) -> int:
        """æ‰¾åˆ°å®‰å…¨çš„åˆ‡å‰²é»ï¼ˆæ®µè½>å¥å­>ç‰‡èªï¼‰ã€‚"""
        import re
        # æ®µè½
        p = text.rfind("\n\n")
        if p > 0:
            return p + 2
        # å¥å­
        last_sentence_end = -1
        for m in re.finditer(r"[ã€‚ï¼ï¼Ÿ\.!?]\s*", text):
            last_sentence_end = m.end()
        if last_sentence_end > 20:
            return last_sentence_end
        # ç‰‡èª
        last_phrase_end = -1
        for m in re.finditer(r"[ï¼Œï¼›,;]\s*", text):
            last_phrase_end = m.end()
        if last_phrase_end > 20:
            return last_phrase_end
        return len(text)

    def _find_overlap_between_texts(self, text1: str, text2: str) -> str:
        """åµæ¸¬ç›¸é„°å…©æ®µçš„é‡ç–Šå…§å®¹ï¼Œç”¨æ–¼å»é‡ã€‚"""
        max_len = min(len(text1), len(text2))
        # é€æ­¥å¾é•·åˆ°çŸ­å˜—è©¦
        for length in range(max_len, 19, -1):
            suffix = text1[-length:]
            prefix = text2[:length]
            if suffix == prefix:
                cut = self._find_safe_cut_point(prefix)
                if cut > 20:
                    return prefix[:cut]
                return suffix
        return ""

    def _apply_basic_formatting(self, text: str) -> str:
        """ç•¶AIå¤±æ•—æ™‚çš„å›é€€ï¼šæŒ‰å¥å­æ‹¼æ®µï¼Œæ®µè½â‰¤250å­—å…ƒï¼Œé›™æ›è¡Œåˆ†éš”ã€‚"""
        if not text or not text.strip():
            return text
        import re
        parts = re.split(r"([ã€‚ï¼ï¼Ÿ\.!?]+\s*)", text)
        sentences = []
        current = ""
        for i, part in enumerate(parts):
            if i % 2 == 0:
                current += part
            else:
                current += part
                if current.strip():
                    sentences.append(current.strip())
                    current = ""
        if current.strip():
            sentences.append(current.strip())
        paras = []
        cur = ""
        sentence_count = 0
        for s in sentences:
            candidate = (cur + " " + s).strip() if cur else s
            sentence_count += 1
            # æ”¹é€²çš„åˆ†æ®µé‚è¼¯ï¼šè€ƒé‡å¥å­æ•¸é‡å’Œé•·åº¦
            should_break = False
            if len(candidate) > 400 and cur:  # æ®µè½éé•·
                should_break = True
            elif len(candidate) > 200 and sentence_count >= 3:  # ä¸­ç­‰é•·åº¦ä¸”å¥å­æ•¸è¶³å¤ 
                should_break = True
            elif sentence_count >= 6:  # å¥å­æ•¸éå¤š
                should_break = True

            if should_break:
                paras.append(cur.strip())
                cur = s
                sentence_count = 1
            else:
                cur = candidate
        if cur.strip():
            paras.append(cur.strip())
        return self._ensure_markdown_paragraphs("\n\n".join(paras))

    async def _format_long_transcript_in_chunks(self, raw_transcript: str, transcript_language: str, max_chars_per_chunk: int) -> str:
        """æ™ºæ…§åˆ†å¡Š+ä¸Šä¸‹æ–‡+å»é‡ åˆæˆæœ€ä½³åŒ–æ–‡æœ¬ï¼ˆJSç­–ç•¥ç§»æ¤ï¼‰ã€‚"""
        import re
        # å…ˆæŒ‰å¥å­åˆ‡åˆ†ï¼Œçµ„è£ä¸è¶…émax_chars_per_chunkçš„å¡Š
        parts = re.split(r"([ã€‚ï¼ï¼Ÿ\.!?]+\s*)", raw_transcript)
        sentences = []
        buf = ""
        for i, part in enumerate(parts):
            if i % 2 == 0:
                buf += part
            else:
                buf += part
                if buf.strip():
                    sentences.append(buf.strip())
                    buf = ""
        if buf.strip():
            sentences.append(buf.strip())

        chunks = []
        cur = ""
        for s in sentences:
            candidate = (cur + " " + s).strip() if cur else s
            if len(candidate) > max_chars_per_chunk and cur:
                chunks.append(cur.strip())
                cur = s
            else:
                cur = candidate
        if cur.strip():
            chunks.append(cur.strip())

        # å°ä»ç„¶éé•·çš„å¡ŠäºŒæ¬¡å®‰å…¨åˆ‡åˆ†
        final_chunks = []
        for c in chunks:
            if len(c) <= max_chars_per_chunk:
                final_chunks.append(c)
            else:
                final_chunks.extend(
                    self._smart_split_long_chunk(c, max_chars_per_chunk))

        logger.info(f"æ–‡æœ¬åˆ†ç‚º {len(final_chunks)} å¡Šè™•ç†")

        optimized = []
        for i, c in enumerate(final_chunks):
            chunk_with_context = c
            if i > 0:
                prev_tail = final_chunks[i - 1][-100:]
                marker = f"[ä¸Šæ–‡çºŒï¼š{prev_tail}]" if transcript_language == 'zh' else f"[Context continued: {prev_tail}]"
                chunk_with_context = marker + "\n\n" + c
            try:
                oc = await self._format_single_chunk(chunk_with_context, transcript_language)
                # ç§»é™¤ä¸Šä¸‹æ–‡æ¨™è¨˜
                oc = re.sub(
                    r"^[(ä¸Šæ–‡çºŒ|Context continued)ï¼š?:?.*?]\s*", "", oc, flags=re.S)
                optimized.append(oc)
            except Exception as e:
                logger.warning(f"ç¬¬ {i+1} å¡Šæœ€ä½³åŒ–å¤±æ•—ï¼Œä½¿ç”¨åŸºç¤æ ¼å¼åŒ–: {e}")
                optimized.append(self._apply_basic_formatting(c))

        # é„°æ¥å¡Šå»é‡
        deduped = []
        for i, c in enumerate(optimized):
            cur_txt = c
            if i > 0 and deduped:
                prev = deduped[-1]
                overlap = self._find_overlap_between_texts(
                    prev[-200:], cur_txt[:200])
                if overlap:
                    cur_txt = cur_txt[len(overlap):].lstrip()
                    if not cur_txt:
                        continue
            if cur_txt.strip():
                deduped.append(cur_txt)

        merged = "\n\n".join(deduped)
        merged = self._remove_transcript_heading(merged)
        enforced = self._enforce_paragraph_max_chars(merged, max_chars=400)
        return self._ensure_markdown_paragraphs(enforced)

    def _remove_timestamps_and_meta(self, text: str) -> str:
        """åƒ…ç§»é™¤æ™‚é–“æˆ³è¡Œèˆ‡æ˜é¡¯å…ƒè³‡è¨Šï¼ˆæ¨™é¡Œã€åµæ¸¬èªè¨€ç­‰ï¼‰ï¼Œä¿ç•™åŸæ–‡å£èª/é‡è¤‡ã€‚"""
        lines = text.split('\n')
        kept = []
        for line in lines:
            s = line.strip()
            # è·³éæ™‚é–“æˆ³èˆ‡å…ƒè³‡è¨Š
            if (s.startswith('**[') and s.endswith(']**')):
                continue
            if s.startswith('# '):
                # è·³éé ‚ç´šæ¨™é¡Œï¼ˆé€šå¸¸æ˜¯å½±ç‰‡æ¨™é¡Œï¼Œå¯åœ¨æœ€çµ‚åŠ å›ï¼‰
                continue
            if s.startswith('**åµæ¸¬èªè¨€:**') or s.startswith('**èªè¨€æ©Ÿç‡:**'):
                continue
            kept.append(line)
        # è¦ç¯„ç©ºè¡Œ
        cleaned = '\n'.join(kept)
        return cleaned

    def _enforce_paragraph_max_chars(self, text: str, max_chars: int = 400) -> str:
        """æŒ‰æ®µè½æ‹†åˆ†ä¸¦ç¢ºä¿æ¯æ®µä¸è¶…émax_charsï¼Œå¿…è¦æ™‚æŒ‰å¥å­é‚Šç•Œæ‹†ç‚ºå¤šæ®µã€‚"""
        if not text:
            return text
        import re
        paragraphs = [p for p in re.split(r"\n\s*\n", text) if p is not None]
        new_paragraphs = []
        for para in paragraphs:
            para = para.strip()
            if len(para) <= max_chars:
                new_paragraphs.append(para)
                continue
            # å¥å­åˆ‡åˆ†
            parts = re.split(r"([ã€‚ï¼ï¼Ÿ\.!?]+\s*)", para)
            sentences = []
            buf = ""
            for i, part in enumerate(parts):
                if i % 2 == 0:
                    buf += part
                else:
                    buf += part
                    if buf.strip():
                        sentences.append(buf.strip())
                        buf = ""
            if buf.strip():
                sentences.append(buf.strip())
            cur = ""
            for s in sentences:
                candidate = (cur + (" " if cur else "") + s).strip()
                if len(candidate) > max_chars and cur:
                    new_paragraphs.append(cur)
                    cur = s
                else:
                    cur = candidate
            if cur:
                new_paragraphs.append(cur)
        return "\n\n".join([p.strip() for p in new_paragraphs if p is not None])

    def _remove_transcript_heading(self, text: str) -> str:
        """ç§»é™¤é–‹é ­æˆ–æ®µè½ä¸­çš„ä»¥ Transcript ç‚ºæ¨™é¡Œçš„è¡Œï¼ˆä»»æ„ç´šåˆ¥#ï¼‰ï¼Œä¸æ”¹è®Šæ­£æ–‡ã€‚"""
        if not text:
            return text
        import re
        # ç§»é™¤å½¢å¦‚ '## Transcript'ã€'# Transcript Text'ã€'### transcript' çš„æ¨™é¡Œè¡Œ
        lines = text.split('\n')
        filtered = []
        for line in lines:
            stripped = line.strip()
            if re.match(r"^#{1,6}\s*transcript(\s+text)?\s*$", stripped, flags=re.I):
                continue
            filtered.append(line)
        return '\n'.join(filtered)

    def _split_into_chunks(self, text: str, max_tokens: int) -> list:
        """
        å°‡åŸå§‹è½‰éŒ„æ–‡æœ¬æ™ºæ…§åˆ†å‰²æˆåˆé©å¤§å°çš„å¡Š
        ç­–ç•¥ï¼šå…ˆæå–ç´”æ–‡æœ¬ï¼ŒæŒ‰å¥å­å’Œæ®µè½è‡ªç„¶åˆ†å‰²
        """
        import re

        # 1. å…ˆæå–ç´”æ–‡æœ¬å…§å®¹ï¼ˆç§»é™¤æ™‚é–“æˆ³ã€æ¨™é¡Œç­‰ï¼‰
        pure_text = self._extract_pure_text(text)

        # 2. æŒ‰å¥å­åˆ†å‰²ï¼Œä¿æŒå¥å­å®Œæ•´æ€§
        sentences = self._split_into_sentences(pure_text)

        # 3. æŒ‰tokené™åˆ¶çµ„è£æˆå¡Š
        chunks = []
        current_chunk = []
        current_tokens = 0

        for sentence in sentences:
            sentence_tokens = self._estimate_tokens(sentence)

            # æª¢æŸ¥æ˜¯å¦èƒ½åŠ å…¥ç›®å‰å¡Š
            if current_tokens + sentence_tokens > max_tokens and current_chunk:
                # ç›®å‰å¡Šå·²æ»¿ï¼Œå„²å­˜ä¸¦é–‹å§‹æ–°å¡Š
                chunks.append(self._join_sentences(current_chunk))
                current_chunk = [sentence]
                current_tokens = sentence_tokens
            else:
                # æ–°å¢åˆ°ç›®å‰å¡Š
                current_chunk.append(sentence)
                current_tokens += sentence_tokens

        # æ–°å¢æœ€å¾Œä¸€å¡Š
        if current_chunk:
            chunks.append(self._join_sentences(current_chunk))

        return chunks

    def _extract_pure_text(self, raw_transcript: str) -> str:
        """
        å¾åŸå§‹è½‰éŒ„ä¸­æå–ç´”æ–‡æœ¬ï¼Œç§»é™¤æ™‚é–“æˆ³å’Œå…ƒæ•¸æ“š
        """
        lines = raw_transcript.split('\n')
        text_lines = []

        for line in lines:
            line = line.strip()
            # è·³éæ™‚é–“æˆ³ã€æ¨™é¡Œã€å…ƒæ•¸æ“š
            if (line.startswith('**[') and line.endswith(']**') or
                line.startswith('#') or
                line.startswith('**åµæ¸¬èªè¨€:**') or
                line.startswith('**èªè¨€æ©Ÿç‡:**') or
                    not line):
                continue
            text_lines.append(line)

        return ' '.join(text_lines)

    def _split_into_sentences(self, text: str) -> list:
        """
        æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬ï¼Œè€ƒé‡ä¸­è‹±æ–‡å·®ç•°
        """
        import re

        # ä¸­è‹±æ–‡å¥å­çµæŸç¬¦
        sentence_endings = r'[.!?ã€‚ï¼ï¼Ÿ;ï¼›]+'

        # åˆ†å‰²å¥å­ï¼Œä¿ç•™å¥è™Ÿ
        parts = re.split(f'({sentence_endings})', text)

        sentences = []
        current = ""

        for i, part in enumerate(parts):
            if re.match(sentence_endings, part):
                # é€™æ˜¯å¥å­çµæŸç¬¦ï¼ŒåŠ åˆ°ç›®å‰å¥å­
                current += part
                if current.strip():
                    sentences.append(current.strip())
                current = ""
            else:
                # é€™æ˜¯å¥å­å…§å®¹
                current += part

        # è™•ç†æœ€å¾Œæ²’æœ‰å¥è™Ÿçš„éƒ¨åˆ†
        if current.strip():
            sentences.append(current.strip())

        return [s for s in sentences if s.strip()]

    def _join_sentences(self, sentences: list) -> str:
        """
        é‡æ–°çµ„åˆå¥å­ç‚ºæ®µè½
        """
        return ' '.join(sentences)

    def _basic_transcript_cleanup(self, raw_transcript: str) -> str:
        """
        åŸºæœ¬çš„è½‰éŒ„æ–‡æœ¬æ¸…ç†ï¼šç§»é™¤æ™‚é–“æˆ³å’Œæ¨™é¡Œè³‡è¨Š
        ç•¶GPTæœ€ä½³åŒ–å¤±æ•—æ™‚çš„å¾Œå‚™æ–¹æ¡ˆ
        """
        lines = raw_transcript.split('\n')
        cleaned_lines = []

        for line in lines:
            # è·³éæ™‚é–“æˆ³è¡Œ
            if line.strip().startswith('**[') and line.strip().endswith(']**'):
                continue
            # è·³éæ¨™é¡Œè¡Œ
            if line.strip().startswith('# ') or line.strip().startswith('## '):
                continue
            # è·³éåµæ¸¬èªè¨€ç­‰å…ƒè³‡è¨Šè¡Œ
            if line.strip().startswith('**åµæ¸¬èªè¨€:**') or line.strip().startswith('**èªè¨€æ©Ÿç‡:**'):
                continue
            # ä¿ç•™éç©ºæ–‡æœ¬è¡Œ
            if line.strip():
                cleaned_lines.append(line.strip())

        # å°‡å¥å­é‡æ–°çµ„åˆä¸¦æ™ºæ…§åˆ†æ®µ
        text = ' '.join(cleaned_lines)

        # æ›´æ™ºæ…§çš„åˆ†å¥è™•ç†ï¼Œè€ƒé‡ä¸­è‹±æ–‡å·®ç•°
        import re

        # æŒ‰å¥è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿåˆ†å¥
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        paragraphs = []
        current_paragraph = []

        for i, sentence in enumerate(sentences):
            if sentence:
                current_paragraph.append(sentence)

                # æ™ºæ…§åˆ†æ®µæ¢ä»¶ï¼š
                # 1. æ¯3å€‹å¥å­ä¸€æ®µï¼ˆåŸºæœ¬è¦å‰‡ï¼‰
                # 2. é‡åˆ°è©±é¡Œè½‰æ›è©å½™æ™‚å¼·åˆ¶åˆ†æ®µ
                # 3. é¿å…è¶…é•·æ®µè½
                topic_change_keywords = [
                    'é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å¾Œ', 'æ¥ä¸‹ä¾†', 'å¦å¤–', 'æ­¤å¤–', 'æœ€å¾Œ', 'ç¸½ä¹‹',
                    'first', 'second', 'third', 'next', 'also', 'however', 'finally',
                    'ç¾åœ¨', 'é‚£éº¼', 'æ‰€ä»¥', 'å› æ­¤', 'ä½†æ˜¯', 'ç„¶è€Œ',
                    'now', 'so', 'therefore', 'but', 'however'
                ]

                should_break = False

                # æª¢æŸ¥æ˜¯å¦éœ€è¦åˆ†æ®µ
                if len(current_paragraph) >= 3:  # åŸºæœ¬é•·åº¦æ¢ä»¶
                    should_break = True
                elif len(current_paragraph) >= 2:  # è¼ƒçŸ­ä½†é‡åˆ°è©±é¡Œè½‰æ›
                    for keyword in topic_change_keywords:
                        if sentence.lower().startswith(keyword.lower()):
                            should_break = True
                            break

                if should_break or len(current_paragraph) >= 4:  # æœ€å¤§é•·åº¦é™åˆ¶
                    # çµ„åˆç›®å‰æ®µè½
                    paragraph_text = '. '.join(current_paragraph)
                    if not paragraph_text.endswith('.'):
                        paragraph_text += '.'
                    paragraphs.append(paragraph_text)
                    current_paragraph = []

        # æ–°å¢å‰©é¤˜çš„å¥å­
        if current_paragraph:
            paragraph_text = '. '.join(current_paragraph)
            if not paragraph_text.endswith('.'):
                paragraph_text += '.'
            paragraphs.append(paragraph_text)

        return '\n\n'.join(paragraphs)

    async def _final_paragraph_organization(self, text: str, lang_instruction: str) -> str:
        """
        å°åˆä½µå¾Œçš„æ–‡æœ¬é€²è¡Œæœ€çµ‚çš„æ®µè½æ•´ç†
        ä½¿ç”¨æ”¹é€²çš„promptå’Œå·¥ç¨‹é©—è­‰
        """
        try:
            # ä¼°ç®—æ–‡æœ¬é•·åº¦ï¼Œå¦‚æœå¤ªé•·å‰‡åˆ†å¡Šè™•ç†
            estimated_tokens = self._estimate_tokens(text)
            if estimated_tokens > 3000:  # å°æ–¼å¾ˆé•·çš„æ–‡æœ¬ï¼Œåˆ†å¡Šè™•ç†
                return await self._organize_long_text_paragraphs(text, lang_instruction)

            system_prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„{lang_instruction}æ–‡æœ¬æ®µè½æ•´ç†å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯æŒ‰ç…§èªæ„å’Œé‚è¼¯é‡æ–°çµ„ç¹”æ®µè½ã€‚

ğŸ¯ **æ ¸å¿ƒåŸå‰‡**ï¼š
1. **åš´æ ¼ä¿æŒåŸå§‹èªè¨€({lang_instruction})**ï¼Œçµ•ä¸ç¿»è­¯
2. **ä¿æŒæ‰€æœ‰å…§å®¹å®Œæ•´**ï¼Œä¸åˆªé™¤ä¸æ–°å¢ä»»ä½•è³‡è¨Š
3. **æŒ‰èªæ„é‚è¼¯åˆ†æ®µ**ï¼šæ¯æ®µåœç¹ä¸€å€‹å®Œæ•´çš„æ€æƒ³æˆ–è©±é¡Œ
4. **åš´æ ¼æ§åˆ¶æ®µè½é•·åº¦**ï¼šæ¯æ®µçµ•ä¸è¶…é250è©
5. **ä¿æŒè‡ªç„¶æµæš¢**ï¼šæ®µè½é–“æ‡‰æœ‰é‚è¼¯é€£æ¥

ğŸ“ **åˆ†æ®µæ¨™æº–**ï¼š
- **èªæ„å®Œæ•´æ€§**ï¼šæ¯æ®µè¬›è¿°ä¸€å€‹å®Œæ•´æ¦‚å¿µæˆ–äº‹ä»¶
- **é©ä¸­é•·åº¦**ï¼š3-7å€‹å¥å­ï¼Œæ¯æ®µçµ•ä¸è¶…é250è©
- **é‚è¼¯é‚Šç•Œ**ï¼šåœ¨è©±é¡Œè½‰æ›ã€æ™‚é–“è½‰æ›ã€è§€é»è½‰æ›è™•åˆ†æ®µ
- **è‡ªç„¶æ–·é»**ï¼šéµå¾ªèªªè©±è€…çš„è‡ªç„¶åœé “å’Œé‚è¼¯

âš ï¸ **åš´ç¦**ï¼š
- å‰µé€ è¶…é250è©çš„å·¨å‹æ®µè½
- å¼·è¡Œåˆä½µä¸ç›¸é—œçš„å…§å®¹
- æ‰“æ–·å®Œæ•´çš„æ•…äº‹æˆ–è«–è¿°

è¼¸å‡ºæ ¼å¼ï¼šæ®µè½é–“ç”¨ç©ºè¡Œåˆ†éš”ã€‚"""

            user_prompt = f"""è«‹é‡æ–°æ•´ç†ä»¥ä¸‹{lang_instruction}æ–‡æœ¬çš„æ®µè½çµæ§‹ã€‚åš´æ ¼æŒ‰ç…§èªæ„å’Œé‚è¼¯é€²è¡Œåˆ†æ®µï¼Œç¢ºä¿æ¯æ®µä¸è¶…é200è©ï¼š

{text}

é‡æ–°åˆ†æ®µå¾Œçš„æ–‡æœ¬ï¼š"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=4000,  # å°é½ŠJSï¼šæ®µè½æ•´ç†éšæ®µæœ€å¤§tokensâ‰ˆ4000
                temperature=0.05  # é™ä½æº«åº¦ï¼Œæé«˜ä¸€è‡´æ€§
            )

            organized_text = response.choices[0].message.content

            # å·¥ç¨‹é©—è­‰ï¼šæª¢æŸ¥æ®µè½é•·åº¦
            validated_text = self._validate_paragraph_lengths(organized_text)

            return validated_text

        except Exception as e:
            logger.error(f"æœ€çµ‚æ®µè½æ•´ç†å¤±æ•—: {e}")
            # å¤±æ•—æ™‚ä½¿ç”¨åŸºç¤åˆ†æ®µè™•ç†
            return self._basic_paragraph_fallback(text)

    async def _organize_long_text_paragraphs(self, text: str, lang_instruction: str) -> str:
        """
        å°æ–¼å¾ˆé•·çš„æ–‡æœ¬ï¼Œåˆ†å¡Šé€²è¡Œæ®µè½æ•´ç†
        """
        try:
            # æŒ‰ç¾æœ‰æ®µè½åˆ†å‰²
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            organized_chunks = []

            current_chunk = []
            current_tokens = 0
            max_chunk_tokens = 2500  # é©æ‡‰4000 tokensç¸½é™åˆ¶çš„chunkå¤§å°

            for para in paragraphs:
                para_tokens = self._estimate_tokens(para)

                if current_tokens + para_tokens > max_chunk_tokens and current_chunk:
                    # è™•ç†ç›®å‰chunk
                    chunk_text = '\n\n'.join(current_chunk)
                    organized_chunk = await self._organize_single_chunk(chunk_text, lang_instruction)
                    organized_chunks.append(organized_chunk)

                    current_chunk = [para]
                    current_tokens = para_tokens
                else:
                    current_chunk.append(para)
                    current_tokens += para_tokens

            # è™•ç†æœ€å¾Œä¸€å€‹chunk
            if current_chunk:
                chunk_text = '\n\n'.join(current_chunk)
                organized_chunk = await self._organize_single_chunk(chunk_text, lang_instruction)
                organized_chunks.append(organized_chunk)

            return '\n\n'.join(organized_chunks)

        except Exception as e:
            logger.error(f"é•·æ–‡æœ¬æ®µè½æ•´ç†å¤±æ•—: {e}")
            return self._basic_paragraph_fallback(text)

    async def _organize_single_chunk(self, text: str, lang_instruction: str) -> str:
        """
        æ•´ç†å–®å€‹æ–‡æœ¬å¡Šçš„æ®µè½
        """
        system_prompt = f"""You are a {lang_instruction} paragraph organization expert. Reorganize paragraphs by semantics, ensuring each paragraph does not exceed 200 words.

Core requirements:
1. Strictly maintain the original {lang_instruction} language
2. Organize by semantic logic, one theme per paragraph
3. Each paragraph must not exceed 250 words
4. Separate paragraphs with blank lines
5. Keep content complete, do not reduce information"""

        user_prompt = f"""Re-paragraph the following text in {lang_instruction}, ensuring each paragraph does not exceed 200 words:

{text}"""

        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=1200,  # é©æ‡‰4000 tokensç¸½é™åˆ¶
            temperature=0.05
        )

        return response.choices[0].message.content

    def _validate_paragraph_lengths(self, text: str) -> str:
        """
        é©—è­‰æ®µè½é•·åº¦ï¼Œå¦‚æœæœ‰è¶…é•·æ®µè½å‰‡å˜—è©¦åˆ†å‰²
        """
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        validated_paragraphs = []

        for para in paragraphs:
            word_count = len(para.split())

            if word_count > 300:  # å¦‚æœæ®µè½è¶…é300è©
                logger.warning(f"åµæ¸¬åˆ°è¶…é•·æ®µè½({word_count}è©)ï¼Œå˜—è©¦åˆ†å‰²")
                # å˜—è©¦æŒ‰å¥å­åˆ†å‰²é•·æ®µè½
                split_paras = self._split_long_paragraph(para)
                validated_paragraphs.extend(split_paras)
            else:
                validated_paragraphs.append(para)

        return '\n\n'.join(validated_paragraphs)

    def _split_long_paragraph(self, paragraph: str) -> list:
        """
        åˆ†å‰²éé•·çš„æ®µè½
        """
        import re

        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', paragraph)
        sentences = [s.strip() + '.' for s in sentences if s.strip()]

        split_paragraphs = []
        current_para = []
        current_words = 0

        for sentence in sentences:
            sentence_words = len(sentence.split())

            if current_words + sentence_words > 200 and current_para:
                # ç›®å‰æ®µè½é”åˆ°é•·åº¦é™åˆ¶
                split_paragraphs.append(' '.join(current_para))
                current_para = [sentence]
                current_words = sentence_words
            else:
                current_para.append(sentence)
                current_words += sentence_words

        # æ–°å¢æœ€å¾Œä¸€æ®µ
        if current_para:
            split_paragraphs.append(' '.join(current_para))

        return split_paragraphs

    def _basic_paragraph_fallback(self, text: str) -> str:
        """
        åŸºç¤åˆ†æ®µfallbackæ©Ÿåˆ¶
        ç•¶GPTæ•´ç†å¤±æ•—æ™‚ï¼Œä½¿ç”¨ç°¡å–®çš„è¦å‰‡åˆ†æ®µ
        """
        import re

        # ç§»é™¤å¤šé¤˜çš„ç©ºè¡Œ
        text = re.sub(r"\n\s*\n\s*\n+", "\n\n", text)

        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        basic_paragraphs = []

        for para in paragraphs:
            word_count = len(para.split())

            if word_count > 250:
                # é•·æ®µè½æŒ‰å¥å­åˆ†å‰²
                split_paras = self._split_long_paragraph(para)
                basic_paragraphs.extend(split_paras)
            elif word_count < 30 and basic_paragraphs:
                # çŸ­æ®µè½èˆ‡ä¸Šä¸€æ®µåˆä½µï¼ˆå¦‚æœåˆä½µå¾Œä¸è¶…é200è©ï¼‰
                last_para = basic_paragraphs[-1]
                combined_words = len(last_para.split()) + word_count

                if combined_words <= 200:
                    basic_paragraphs[-1] = last_para + ' ' + para
                else:
                    basic_paragraphs.append(para)
            else:
                basic_paragraphs.append(para)

        return '\n\n'.join(basic_paragraphs)

    async def summarize(self, transcript: str, target_language: str = "zh", video_title: str = None) -> str:
        """
        ç”¢ç”Ÿå½±ç‰‡è½‰éŒ„çš„æ‘˜è¦

        Args:
            transcript: è½‰éŒ„æ–‡æœ¬
            target_language: ç›®æ¨™èªè¨€ä»£ç¢¼
            video_title: å½±ç‰‡æ¨™é¡Œ

        Returns:
            æ‘˜è¦æ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        try:
            if not self.client:
                logger.warning("OpenAI APIä¸å¯ç”¨ï¼Œç”¢ç”Ÿå‚™ç”¨æ‘˜è¦")
                return self._generate_fallback_summary(transcript, target_language, video_title)

            # ä¼°ç®—è½‰éŒ„æ–‡æœ¬é•·åº¦ï¼Œæ±ºå®šæ˜¯å¦éœ€è¦åˆ†å¡Šæ‘˜è¦
            estimated_tokens = self._estimate_tokens(transcript)
            max_summarize_tokens = 4000  # æé«˜é™åˆ¶ï¼Œå„ªå…ˆä½¿ç”¨å–®æ–‡æœ¬è™•ç†ä»¥ç²å¾—æ›´å¥½çš„ç¸½çµå“è³ª

            if estimated_tokens <= max_summarize_tokens:
                # çŸ­æ–‡æœ¬ç›´æ¥æ‘˜è¦
                return await self._summarize_single_text(transcript, target_language, video_title)
            else:
                # é•·æ–‡æœ¬åˆ†å¡Šæ‘˜è¦
                logger.info(f"æ–‡æœ¬è¼ƒé•·({estimated_tokens} tokens)ï¼Œå•Ÿç”¨åˆ†å¡Šæ‘˜è¦")
                return await self._summarize_with_chunks(transcript, target_language, video_title, max_summarize_tokens)

        except Exception as e:
            logger.error(f"ç”¢ç”Ÿæ‘˜è¦å¤±æ•—: {str(e)}")
            return self._generate_fallback_summary(transcript, target_language, video_title)

    async def _summarize_single_text(self, transcript: str, target_language: str, video_title: str = None) -> str:
        """
        å°å–®å€‹æ–‡æœ¬é€²è¡Œæ‘˜è¦

        Args:
            transcript: è½‰éŒ„æ–‡æœ¬
            target_language: ç›®æ¨™èªè¨€
            video_title: å½±ç‰‡æ¨™é¡Œ
        """
        # ç²å–ç›®æ¨™èªè¨€åç¨±
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç¹é«”ï¼‰")

        # å»ºæ§‹è‹±æ–‡æç¤ºè©ï¼Œé©ç”¨æ–¼æ‰€æœ‰ç›®æ¨™èªè¨€
        system_prompt = f"""You are a professional content analyst. Please generate a well-structured summary in {language_name} for the following text.

Summary Requirements:
1. Extract the main topics and core viewpoints from the text
2. Maintain clear logical structure, highlighting the core arguments
3. Include important discussions, viewpoints, and conclusions
4. Use concise and clear language
5. Appropriately preserve the speaker's expression style and key opinions

Paragraph Organization Requirements (Core):
1. **Organize by semantic and logical themes** - Start a new paragraph whenever the topic shifts, discussion focus changes, or when transitioning from one viewpoint to another
2. **Each paragraph should focus on one main point or theme**
3. **Paragraphs must be separated by blank lines (double line breaks \n\n)**
4. **Consider the logical flow of content and reasonably divide paragraph boundaries**

Format Requirements:
1. Use Markdown format with double line breaks between paragraphs
2. Each paragraph should be a complete logical unit
3. Write entirely in {language_name}"""

        user_prompt = f"""Based on the following content, write a well-structured summary in {language_name}:

{transcript}

Requirements:
- Focus on natural paragraphs, avoiding decorative headings
- Cover all key ideas and arguments, prioritize the most important content
- Ensure balanced coverage of both early and later content
- Use clear and concise language
- Organize content logically with proper paragraph breaks"""

        logger.info(f"æ­£åœ¨ç”¢ç”Ÿ{language_name}æ‘˜è¦...")

        # å‘¼å«OpenAI API
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=3500,
            temperature=0.3
        )

        summary = response.choices[0].message.content

        return self._format_summary_with_meta(summary, target_language, video_title)

    async def _summarize_with_chunks(self, transcript: str, target_language: str, video_title: str, max_tokens: int) -> str:
        """
        åˆ†å¡Šæ‘˜è¦é•·æ–‡æœ¬

        Args:
            transcript: è½‰éŒ„æ–‡æœ¬
            target_language: ç›®æ¨™èªè¨€
            video_title: å½±ç‰‡æ¨™é¡Œ
            max_tokens: åˆ†å¡Šè™•ç†çš„æœ€å¤§tokenæ•¸
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç¹é«”ï¼‰")

        # ä½¿ç”¨JSç­–ç•¥ï¼šæŒ‰å­—å…ƒé€²è¡Œæ™ºæ…§åˆ†å¡Šï¼ˆæ®µè½>å¥å­ï¼‰
        chunks = self._smart_chunk_text(transcript, max_chars_per_chunk=4000)
        logger.info(f"åˆ†å‰²ç‚º {len(chunks)} å€‹å¡Šé€²è¡Œæ‘˜è¦")

        chunk_summaries = []

        # æ¯å¡Šç”¢ç”Ÿå±€éƒ¨æ‘˜è¦
        for i, chunk in enumerate(chunks):
            logger.info(f"æ­£åœ¨æ‘˜è¦ç¬¬ {i+1}/{len(chunks)} å¡Š...")

            system_prompt = f"""You are a summarization expert. Please write a high-density summary for this text chunk in {language_name}.

This is part {i+1} of {len(chunks)} of the complete content (Part {i+1}/{len(chunks)}).

Output preferences: Focus on natural paragraphs, use minimal bullet points if necessary; highlight new information and its relationship to the main narrative; avoid vague repetition and formatted headings; moderate length (suggested 120-220 words)."""

            user_prompt = f"""[Part {i+1}/{len(chunks)}] Summarize the key points of the following text in {language_name} (natural paragraphs preferred, minimal bullet points, 120-220 words):

{chunk}

Avoid using any subheadings or decorative separators, output content only."""

            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=1000,  # æå‡åˆ†å¡Šæ‘˜è¦å®¹é‡ä»¥æ¶µè“‹æ›´å¤šç´°ç¯€
                    temperature=0.3
                )

                chunk_summary = response.choices[0].message.content
                chunk_summaries.append(chunk_summary)

            except Exception as e:
                logger.error(f"æ‘˜è¦ç¬¬ {i+1} å¡Šå¤±æ•—: {e}")
                # å¤±æ•—æ™‚ç”¢ç”Ÿç°¡å–®æ‘˜è¦
                simple_summary = f"ç¬¬{i+1}éƒ¨åˆ†å…§å®¹æ¦‚è¿°ï¼š" + chunk[:200] + "..."
                chunk_summaries.append(simple_summary)

        # åˆä½µæ‰€æœ‰å±€éƒ¨æ‘˜è¦ï¼ˆå¸¶ç·¨è™Ÿï¼‰ï¼Œå¦‚åˆ†å¡Šè¼ƒå¤šå‰‡åˆ†å±¤æ•´åˆï¼ˆä¸å¼•å…¥å°æ¨™é¡Œï¼‰
        combined_summaries = "\n\n".join(
            [f"[Part {idx+1}]\n" + s for idx, s in enumerate(chunk_summaries)])

        logger.info("æ­£åœ¨æ•´åˆæœ€çµ‚æ‘˜è¦...")
        if len(chunk_summaries) > 10:
            final_summary = await self._integrate_hierarchical_summaries(chunk_summaries, target_language)
        else:
            final_summary = await self._integrate_chunk_summaries(combined_summaries, target_language)

        return self._format_summary_with_meta(final_summary, target_language, video_title)

    def _smart_chunk_text(self, text: str, max_chars_per_chunk: int = 3500) -> list:
        """æ™ºæ…§åˆ†å¡Šï¼ˆå…ˆæ®µè½å¾Œå¥å­ï¼‰ï¼ŒæŒ‰å­—å…ƒä¸Šé™åˆ‡åˆ†ã€‚"""
        chunks = []
        paragraphs = [p for p in text.split('\n\n') if p.strip()]
        cur = ""
        for p in paragraphs:
            candidate = (cur + "\n\n" + p).strip() if cur else p
            if len(candidate) > max_chars_per_chunk and cur:
                chunks.append(cur.strip())
                cur = p
            else:
                cur = candidate
        if cur.strip():
            chunks.append(cur.strip())

        # äºŒæ¬¡æŒ‰å¥å­åˆ‡åˆ†éé•·å¡Š
        import re
        final_chunks = []
        for c in chunks:
            if len(c) <= max_chars_per_chunk:
                final_chunks.append(c)
            else:
                sentences = [s.strip()
                             for s in re.split(r"[ã€‚ï¼ï¼Ÿ\.!?]+", c) if s.strip()]
                scur = ""
                for s in sentences:
                    candidate = (scur + 'ã€‚' + s).strip() if scur else s
                    if len(candidate) > max_chars_per_chunk and scur:
                        final_chunks.append(scur.strip())
                        scur = s
                    else:
                        scur = candidate
                if scur.strip():
                    final_chunks.append(scur.strip())
        return final_chunks

    async def _integrate_chunk_summaries(self, combined_summaries: str, target_language: str) -> str:
        """
        æ•´åˆåˆ†å¡Šæ‘˜è¦ç‚ºæœ€çµ‚é€£è²«æ‘˜è¦
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç¹é«”ï¼‰")

        try:
            system_prompt = f"""You are a content integration expert. Please integrate multiple segmented summaries into a complete, coherent summary in {language_name}.

Integration Requirements:
1. Remove duplicate content and maintain clear logic
2. Reorganize content by themes or chronological order
3. Each paragraph must be separated by double line breaks
4. Ensure output is in Markdown format with double line breaks between paragraphs
5. Use concise and clear language
6. Form a complete content summary
7. Cover all parts comprehensively without omission"""

            user_prompt = f"""Please integrate the following segmented summaries into a complete, coherent summary in {language_name}:

{combined_summaries}

Requirements:
- Remove duplicate content and maintain clear logic
- Reorganize content by themes or chronological order
- Each paragraph must be separated by double line breaks
- Ensure output is in Markdown format with double line breaks between paragraphs
- Use concise and clear language
- Form a complete content summary"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=2500,  # æ§åˆ¶è¼¸å‡ºè¦æ¨¡ï¼Œå…¼é¡§ä¸Šä¸‹æ–‡å®‰å…¨
                temperature=0.3
            )

            return response.choices[0].message.content

        except Exception as e:
            logger.error(f"æ•´åˆæ‘˜è¦å¤±æ•—: {e}")
            # å¤±æ•—æ™‚ç›´æ¥åˆä½µ
            return combined_summaries

    def _format_summary_with_meta(self, summary: str, target_language: str, video_title: str = None) -> str:
        """
        ç‚ºæ‘˜è¦åŠ å…¥æ¨™é¡Œå’Œå…ƒè³‡è¨Š
        """
        # ä¸åŠ ä»»ä½•å°æ¨™é¡Œ/å…è²¬è²æ˜ï¼Œå¯ä¿ç•™å½±ç‰‡æ¨™é¡Œä½œç‚ºä¸€ç´šæ¨™é¡Œ
        if video_title:
            prefix = f"# {video_title}\n\n"
        else:
            prefix = ""
        return prefix + summary

    def _generate_fallback_summary(self, transcript: str, target_language: str, video_title: str = None) -> str:
        """
        ç”¢ç”Ÿå‚™ç”¨æ‘˜è¦ï¼ˆç•¶OpenAI APIä¸å¯ç”¨æ™‚ï¼‰

        Args:
            transcript: è½‰éŒ„æ–‡æœ¬
            video_title: å½±ç‰‡æ¨™é¡Œ
            target_language: ç›®æ¨™èªè¨€ä»£ç¢¼

        Returns:
            å‚™ç”¨æ‘˜è¦æ–‡æœ¬
        """
        language_name = self.language_map.get(target_language, "ä¸­æ–‡ï¼ˆç¹é«”ï¼‰")

        # ç°¡å–®çš„æ–‡æœ¬è™•ç†ï¼Œæå–é—œéµè³‡è¨Š
        lines = transcript.split('\n')
        content_lines = [line for line in lines if line.strip(
        ) and not line.startswith('#') and not line.startswith('**')]

        # è¨ˆç®—å¤§æ¦‚çš„é•·åº¦
        total_chars = sum(len(line) for line in content_lines)

        # ä½¿ç”¨ç›®æ¨™èªè¨€çš„æ¨™ç±¤
        meta_labels = self._get_summary_labels(target_language)
        fallback_labels = self._get_fallback_labels(target_language)

        # ç›´æ¥ä½¿ç”¨å½±ç‰‡æ¨™é¡Œä½œç‚ºä¸»æ¨™é¡Œ
        title = video_title if video_title else "Summary"

        summary = f"""# {title}

**{meta_labels['language_label']}:** {language_name}
**{fallback_labels['notice']}:** {fallback_labels['api_unavailable']}



## {fallback_labels['overview_title']}

**{fallback_labels['content_length']}:** {fallback_labels['about']} {total_chars} {fallback_labels['characters']}
**{fallback_labels['paragraph_count']}:** {len(content_lines)} {fallback_labels['paragraphs']}

## {fallback_labels['main_content']}

{fallback_labels['content_description']}

{fallback_labels['suggestions_intro']}

1. {fallback_labels['suggestion_1']}
2. {fallback_labels['suggestion_2']}
3. {fallback_labels['suggestion_3']}

## {fallback_labels['recommendations']}

- {fallback_labels['recommendation_1']}
- {fallback_labels['recommendation_2']}


<br/>

<p style=\"color: #888; font-style: italic; text-align: center; margin-top: 16px;"><em>{fallback_labels['fallback_disclaimer']}</em></p>"""

        return summary

    def _get_current_time(self) -> str:
        """ç²å–ç›®å‰æ™‚é–“å­—ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def get_supported_languages(self) -> dict:
        """
        ç²å–æ”¯æ´çš„èªè¨€åˆ—è¡¨

        Returns:
            èªè¨€ä»£ç¢¼åˆ°èªè¨€åç¨±çš„å°æ‡‰
        """
        return self.language_map.copy()

    def _detect_transcript_language(self, transcript: str) -> str:
        """
        åµæ¸¬è½‰éŒ„æ–‡æœ¬çš„ä¸»è¦èªè¨€

        Args:
            transcript: è½‰éŒ„æ–‡æœ¬

        Returns:
            åµæ¸¬åˆ°çš„èªè¨€ä»£ç¢¼
        """
        # ç°¡å–®çš„èªè¨€åµæ¸¬é‚è¼¯ï¼šå°‹æ‰¾è½‰éŒ„æ–‡æœ¬ä¸­çš„èªè¨€æ¨™è¨˜
        if "**åµæ¸¬èªè¨€:**" in transcript:
            # å¾Whisperè½‰éŒ„ä¸­æå–åµæ¸¬åˆ°çš„èªè¨€
            lines = transcript.split('\n')
            for line in lines:
                if "**åµæ¸¬èªè¨€:**" in line:
                    # æå–èªè¨€ä»£ç¢¼ï¼Œä¾‹å¦‚: "**åµæ¸¬èªè¨€:** en"
                    lang = line.split(":")[-1].strip()
                    return lang

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°èªè¨€æ¨™è¨˜ï¼Œä½¿ç”¨ç°¡å–®çš„å­—å…ƒåµæ¸¬
        # è¨ˆç®—è‹±æ–‡å­—å…ƒã€ä¸­æ–‡å­—å…ƒç­‰çš„æ¯”ä¾‹
        total_chars = len(transcript)
        if total_chars == 0:
            return "en"  # é è¨­è‹±æ–‡

        # çµ±è¨ˆä¸­æ–‡å­—å…ƒ
        chinese_chars = sum(
            1 for char in transcript if '\u4e00' <= char <= '\u9fff')
        chinese_ratio = chinese_chars / total_chars

        # çµ±è¨ˆè‹±æ–‡å­—æ¯
        english_chars = sum(
            1 for char in transcript if char.isascii() and char.isalpha())
        english_ratio = english_chars / total_chars

        # æ ¹æ“šæ¯”ä¾‹åˆ¤æ–·
        if chinese_ratio > 0.3:
            return "zh"
        elif english_ratio > 0.3:
            return "en"
        else:
            return "en"  # é è¨­è‹±æ–‡

    def _get_language_instruction(self, lang_code: str) -> str:
        """
        æ ¹æ“šèªè¨€ä»£ç¢¼ç²å–æœ€ä½³åŒ–æŒ‡ä»¤ä¸­ä½¿ç”¨çš„èªè¨€åç¨±

        Args:
            lang_code: èªè¨€ä»£ç¢¼

        Returns:
            èªè¨€åç¨±
        """
        language_instructions = {
            "en": "English",
            "zh": "ä¸­æ–‡",
            "ja": "æ—¥æœ¬èª",
            "ko": "í•œêµ­ì–´",
            "es": "EspaÃ±ol",
            "fr": "FranÃ§ais",
            "de": "Deutsch",
            "it": "Italiano",
            "pt": "PortuguÃªs",
            "ru": "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
            "ar": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
        }
        return language_instructions.get(lang_code, "English")

    def _get_summary_labels(self, lang_code: str) -> dict:
        """
        ç²å–æ‘˜è¦é é¢çš„å¤šèªè¨€æ¨™ç±¤

        Args:
            lang_code: èªè¨€ä»£ç¢¼

        Returns:
            æ¨™ç±¤å­—å…¸
        """
        labels = {
            "en": {
                "language_label": "Summary Language",
                "disclaimer": "This summary is automatically generated by AI for reference only"
            },
            "zh": {
                "language_label": "æ‘˜è¦èªè¨€",
                "disclaimer": "æœ¬æ‘˜è¦ç”±AIè‡ªå‹•ç”¢ç”Ÿï¼Œåƒ…ä¾›åƒè€ƒ"
            },
            "ja": {
                "language_label": "è¦ç´„è¨€èª",
                "disclaimer": "ã“ã®è¦ç´„ã¯AIã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¦ãŠã‚Šã€å‚è€ƒç”¨ã§ã™"
            },
            "ko": {
                "language_label": "ìš”ì•½ ì–¸ì–´",
                "disclaimer": "ì´ ìš”ì•½ì€ AIì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìœ¼ë©° ì°¸ê³ ìš©ì…ë‹ˆë‹¤"
            },
            "es": {
                "language_label": "Idioma del Resumen",
                "disclaimer": "Este resumen es generado automÃ¡ticamente por IA, solo para referencia"
            },
            "fr": {
                "language_label": "Langue du RÃ©sumÃ©",
                "disclaimer": "Ce rÃ©sumÃ© est gÃ©nÃ©rÃ© automatiquement par IA, Ã  titre de rÃ©fÃ©rence uniquement"
            },
            "de": {
                "language_label": "Zusammenfassungssprache",
                "disclaimer": "Diese Zusammenfassung wird automatisch von KI generiert, nur zur Referenz"
            },
            "it": {
                "language_label": "Lingua del Riassunto",
                "disclaimer": "Questo riassunto Ã¨ generato automaticamente dall'IA, solo per riferimento"
            },
            "pt": {
                "language_label": "Idioma do Resumo",
                "disclaimer": "Este resumo Ã© gerado automaticamente por IA, apenas para referÃªncia"
            },
            "ru": {
                "language_label": "Ğ¯Ğ·Ñ‹Ğº Ñ€ĞµĞ·ÑĞ¼Ğµ",
                "disclaimer": "Ğ­Ñ‚Ğ¾ Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ˜Ğ˜, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ¸"
            },
            "ar": {
                "language_label": "Ù„ØºØ© Ø§Ù„Ù…Ù„Ø®Øµ",
                "disclaimer": "Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ø®Øµ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ù„Ù„Ù…Ø±Ø¬Ø¹ ÙÙ‚Ø·"
            }
        }
        return labels.get(lang_code, labels["en"])

    def _get_fallback_labels(self, lang_code: str) -> dict:
        """
        ç²å–å‚™ç”¨æ‘˜è¦çš„å¤šèªè¨€æ¨™ç±¤

        Args:
            lang_code: èªè¨€ä»£ç¢¼

        Returns:
            æ¨™ç±¤å­—å…¸
        """
        labels = {
            "en": {
                "notice": "Notice",
                "api_unavailable": "OpenAI API is unavailable, this is a simplified summary",
                "overview_title": "Transcript Overview",
                "content_length": "Content Length",
                "about": "About",
                "characters": "characters",
                "paragraph_count": "Paragraph Count",
                "paragraphs": "paragraphs",
                "main_content": "Main Content",
                "content_description": "The transcript contains complete video speech content. Since AI summary cannot be generated currently, we recommend:",
                "suggestions_intro": "For detailed information, we suggest you:",
                "suggestion_1": "Review the complete transcript text for detailed information",
                "suggestion_2": "Focus on important paragraphs marked with timestamps",
                "suggestion_3": "Manually extract key points and takeaways",
                "recommendations": "Recommendations",
                "recommendation_1": "Configure OpenAI API key for better summary functionality",
                "recommendation_2": "Or use other AI services for text summarization",
                "fallback_disclaimer": "This is an automatically generated fallback summary"
            },
            "zh": {
                "notice": "æ³¨æ„",
                "api_unavailable": "ç”±æ–¼OpenAI APIä¸å¯ç”¨ï¼Œé€™æ˜¯ä¸€å€‹ç°¡åŒ–çš„æ‘˜è¦",
                "overview_title": "è½‰éŒ„æ¦‚è¦½",
                "content_length": "å…§å®¹é•·åº¦",
                "about": "ç´„",
                "characters": "å­—å…ƒ",
                "paragraph_count": "æ®µè½æ•¸é‡",
                "paragraphs": "æ®µ",
                "main_content": "ä¸»è¦å…§å®¹",
                "content_description": "è½‰éŒ„æ–‡æœ¬åŒ…å«äº†å®Œæ•´çš„å½±ç‰‡èªéŸ³å…§å®¹ã€‚ç”±æ–¼ç›®å‰ç„¡æ³•ç”¢ç”Ÿæ™ºæ…§æ‘˜è¦ï¼Œå»ºè­°æ‚¨ï¼š",
                "suggestions_intro": "ç‚ºç²å–è©³ç´°è³‡è¨Šï¼Œå»ºè­°æ‚¨ï¼š",
                "suggestion_1": "æª¢è¦–å®Œæ•´çš„è½‰éŒ„æ–‡æœ¬ä»¥ç²å–è©³ç´°è³‡è¨Š",
                "suggestion_2": "é—œæ³¨æ™‚é–“æˆ³æ¨™è¨˜çš„é‡è¦æ®µè½",
                "suggestion_3": "æ‰‹å‹•æå–é—œéµè§€é»å’Œè¦é»",
                "recommendations": "å»ºè­°",
                "recommendation_1": "è¨­å®šOpenAI APIé‡‘é‘°ä»¥ç²å¾—æ›´å¥½çš„æ‘˜è¦åŠŸèƒ½",
                "recommendation_2": "æˆ–è€…ä½¿ç”¨å…¶ä»–AIæœå‹™é€²è¡Œæ–‡æœ¬ç¸½çµ",
                "fallback_disclaimer": "æœ¬æ‘˜è¦ç‚ºè‡ªå‹•ç”¢ç”Ÿçš„å‚™ç”¨ç‰ˆæœ¬"
            }
        }
        return labels.get(lang_code, labels["en"])

    def is_available(self) -> bool:
        """
        æª¢æŸ¥æ‘˜è¦æœå‹™æ˜¯å¦å¯ç”¨

        Returns:
            å¦‚æœå·²è¨­å®š OpenAI API å‰‡ç‚º Trueï¼Œå¦å‰‡ç‚º False
        """
        return self.client is not None
